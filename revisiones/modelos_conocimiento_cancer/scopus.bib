Scopus
EXPORT DATE: 16 October 2024

@ARTICLE{Xu2022215,
	author = {Xu, Jianmin and Yao, Yueping and Xu, Binghua and Li, Yipeng and Su, Zhijian},
	title = {Unsupervised learning of cross-modal mappings in multi-omics data for survival stratification of gastric cancer},
	year = {2022},
	journal = {Future Oncology},
	volume = {18},
	number = {2},
	pages = {215 – 230},
	doi = {10.2217/fon-2021-1059},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121652847&doi=10.2217%2ffon-2021-1059&partnerID=40&md5=d5da280fb71cdb62ded6c072a4c49a7e},
	abstract = {Aims: This study presents a survival stratification model based on multi-omics integration using bidirectional deep neural networks (BiDNNs) in gastric cancer. Methods: Based on the survival-related representation features yielded by BiDNNs through integrating transcriptomics and epigenomics data, K-means clustering analysis was performed to cluster tumor samples into different survival subgroups. The BiDNNs-based model was validated using tenfold cross-validation and in two independent confirmation cohorts. Results: Using the BiDNNs-based survival stratification model, patients were grouped into two survival subgroups with log-rank p-value = 9.05E-05. The subgroups classification was robustly validated in tenfold cross-validation (C-index = 0.65 ± 0.02) and in two confirmation cohorts (E-GEOD-26253, C-index = 0.609; E-GEOD-62254, C-index = 0.706). Conclusion: We propose and validate a robust and stable BiDNN-based survival stratification model in gastric cancer. © 2021 Future Medicine Ltd.},
	publication_stage = {Final}
}

@ARTICLE{Singh20232079,
	author = {Singh, Surabhi and Singh, Shiwangi and Koohang, Alex and Sharma, Anuj and Dhir, Sanjay},
	title = {Soft computing in business: exploring current research and outlining future research directions},
	year = {2023},
	journal = {Industrial Management and Data Systems},
	volume = {123},
	number = {8},
	pages = {2079 – 2127},
	doi = {10.1108/IMDS-02-2023-0126},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164482400&doi=10.1108%2fIMDS-02-2023-0126&partnerID=40&md5=4ef43b847f1efec6cb31620e1d596cf7},
	abstract = {Purpose: The primary aim of this study is to detail the use of soft computing techniques in business and management research. Its objectives are as follows: to conduct a comprehensive scientometric analysis of publications in the field of soft computing, to explore the evolution of keywords, to identify key research themes and latent topics and to map the intellectual structure of soft computing in the business literature. Design/methodology/approach: This research offers a comprehensive overview of the field by synthesising 43 years (1980–2022) of soft computing research from the Scopus database. It employs descriptive analysis, topic modelling (TM) and scientometric analysis. Findings: This study's co-citation analysis identifies three primary categories of research in the field: the components, the techniques and the benefits of soft computing. Additionally, this study identifies 16 key study themes in the soft computing literature using TM, including decision-making under uncertainty, multi-criteria decision-making (MCDM), the application of deep learning in object detection and fault diagnosis, circular economy and sustainable development and a few others. Practical implications: This analysis offers a valuable understanding of soft computing for researchers and industry experts and highlights potential areas for future research. Originality/value: This study uses scientific mapping and performance indicators to analyse a large corpus of 4,512 articles in the field of soft computing. It makes significant contributions to the intellectual and conceptual framework of soft computing research by providing a comprehensive overview of the literature on soft computing literature covering a period of four decades and identifying significant trends and topics to direct future research. © 2023, Emerald Publishing Limited.},
	publication_stage = {Final}
}

@ARTICLE{Sefcikova20221545,
	author = {Sefcikova, Viktoria and Sporrer, Juliana K. and Juvekar, Parikshit and Golby, Alexandra and Samandouras, George},
	title = {Converting sounds to meaning with ventral semantic language networks: integration of interdisciplinary data on brain connectivity, direct electrical stimulation and clinical disconnection syndromes},
	year = {2022},
	journal = {Brain Structure and Function},
	volume = {227},
	number = {5},
	pages = {1545 – 1564},
	doi = {10.1007/s00429-021-02438-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126024992&doi=10.1007%2fs00429-021-02438-x&partnerID=40&md5=f3641bf9620f9938911e4c81dc0776a9},
	abstract = {Numerous traditional linguistic theories propose that semantic language pathways convert sounds to meaningful concepts, generating interpretations ranging from simple object descriptions to communicating complex, analytical thinking. Although the dual-stream model of Hickok and Poeppel is widely employed, proposing a dorsal stream, mapping speech sounds to articulatory/phonological networks, and a ventral stream, mapping speech sounds to semantic representations, other language models have been proposed. Indeed, despite seemingly congruent models of semantic language pathways, research outputs from varied specialisms contain only partially congruent data, secondary to the diversity of applied disciplines, ranging from fibre dissection, tract tracing, and functional neuroimaging to neuropsychiatry, stroke neurology, and intraoperative direct electrical stimulation. The current review presents a comprehensive, interdisciplinary synthesis of the ventral, semantic connectivity pathways consisting of the uncinate, middle longitudinal, inferior longitudinal, and inferior fronto-occipital fasciculi, with special reference to areas of controversies or consensus. This is achieved by describing, for each tract, historical concept evolution, terminations, lateralisation, and segmentation models. Clinical implications are presented in three forms: (a) functional considerations derived from normal subject investigations, (b) outputs of direct electrical stimulation during awake brain surgery, and (c) results of disconnection syndromes following disease-related lesioning. The current review unifies interpretation of related specialisms and serves as a framework/thinking model for additional research on language data acquisition and integration. © 2022, The Author(s).},
	publication_stage = {Final}
}

@BOOK{Neustein20221,
	author = {Neustein, Amy and Christen, Nathaniel},
	title = {Innovative Data Integration and Conceptual Space Modeling for COVID, Cancer, and Cardiac Care},
	year = {2022},
	journal = {Innovative Data Integration and Conceptual Space Modeling for COVID, Cancer, and Cardiac Care},
	pages = {1 – 278},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126156238&partnerID=40&md5=05dbb4b3b65714882168141cec7e0b55},
	abstract = {In recent years, scientific research and translation medicine have placed increased emphasis on computational methodology and data curation across many disciplines, both to advance underlying science and to instantiate precision-medicine protocols in the lab and in clinical practice. The nexus of concerns related to oncology, cardiology, and virology (SARS-CoV-2) presents a fortuitous context within which to examine the theory and practice of biomedical data curation. Innovative Data Integration and Conceptual Space Modeling for COVID, Cancer, and Cardiac Care argues that a well-rounded approach to data modeling should optimally embrace multiple perspectives inasmuch as data-modeling is neither a purely formal nor a purely conceptual discipline, but rather a hybrid of both. On the one hand, data models are designed for use by computer software components, and are, consequently, constrained by the mechanistic demands of software environments; data modeling strategies must accept the formal rigors imposed by unambiguous data-sharing and query-evaluation logic. In particular, data models are not well-suited for software-level deployment if such models do not translate seamlessly to clear strategies for querying data and ensuring data integrity as information is moved across multiple points. On the other hand, data modeling is, likewise, constrained by human conceptual tendencies, because the information which is managed by databases and data networks is ultimately intended to be visualized/utilized by humans as the end-user. Thus, at the intersection of both formal and humanistic methodology, data modeling takes on elements of both logico-mathematical frameworks (e.g., type systems and graph theory) and conceptual/philosophical paradigms (e.g., linguistics and cognitive science). The authors embrace this two-sided aspect of data models by seeking non-reductionistic points of convergence between formal and humanistic/conceptual viewpoints, and by leveraging biomedical contexts (viz., COVID, Cancer, and Cardiac Care) so as to provide motivating examples and case-studies in this volume. © 2022 Elsevier Inc. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Derbal2024,
	author = {Derbal, Youcef},
	title = {Adaptive Cancer Therapy in the Age of Generative Artificial Intelligence},
	year = {2024},
	journal = {Cancer Control},
	volume = {31},
	doi = {10.1177/10732748241264704},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196437105&doi=10.1177%2f10732748241264704&partnerID=40&md5=56d5bfbc9ddf091d1e35130a216a3855},
	abstract = {Therapeutic resistance is a major challenge facing the design of effective cancer treatments. Adaptive cancer therapy is in principle the most viable approach to manage cancer’s adaptive dynamics through drug combinations with dose timing and modulation. However, there are numerous open issues facing the clinical success of adaptive therapy. Chief among these issues is the feasibility of real-time predictions of treatment response which represent a bedrock requirement of adaptive therapy. Generative artificial intelligence has the potential to learn prediction models of treatment response from clinical, molecular, and radiomics data about patients and their treatments. The article explores this potential through a proposed integration model of Generative Pre-Trained Transformers (GPTs) in a closed loop with adaptive treatments to predict the trajectories of disease progression. The conceptual model and the challenges facing its realization are discussed in the broader context of artificial intelligence integration in oncology. © The Author(s) 2024.},
	publication_stage = {Final}
}

@ARTICLE{Arian2024109,
	author = {Arian, Mahdieh and Hajiabadi, Fatemeh and Amini, Zakiyeh and Oghazian, Mohammad Bagher and Valinejadi, Ali and Sahebkar, Amirhossein},
	title = {Introduction of Various Models of Palliative Oncology Care: A Systematic Review},
	year = {2024},
	journal = {Reviews on Recent Clinical Trials},
	volume = {19},
	number = {2},
	pages = {109 – 126},
	doi = {10.2174/0115748871272511231215053624},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194390775&doi=10.2174%2f0115748871272511231215053624&partnerID=40&md5=249894d13767f82108b503823db1fc9e},
	abstract = {Background: The aim of this study is to synthesize the existing evidence on various palliative care (PC) models for cancer patients. This effort seeks to discern which facets of PC models are suitable for various patient cohorts, elucidate their mechanisms, and clarify the circumstances in which these models operate. Methods: A comprehensive search was performed using MeSH terms related to PC and cancer across various databases. The Preferred Reporting Items for Systematic Reviews and a comprehensive evidence map were also applied. Results: Thirty-three reviews were published between 2009 and 2023. The conceptual PC models can be classified broadly into time-based, provider-based, disease-based, nurse-based, issue-based, system-based, team-based, non-hospice-based, hospital-based, community-based, telehealth-based, and setting-based models. The study argues that the outcomes of PC encompass timely symptom management, longitudinal psychosocial support, enhanced communication, and decision-making. Referral methods to specialized PC services include oncologist-initiated referral based on clinical judgment alone, via referral criteria, automatic referral at the diagnosis of advanced cancer, or referral based on symptoms or other triggers. Conclusion: The gold standard for selecting a PC model in the context of oncology is a model that ensures broad availability of early PC for all patients and provides well-timed, scheduled, and specialized care for patients with the greatest requirement. © 2024 Bentham Science Publishers.},
	publication_stage = {Final}
}

@ARTICLE{Rejeb2024523,
	author = {Rejeb, Abderahman and Rejeb, Karim and Abdollahi, Alireza and Kayikci, Yasanur and Appolloni, Andrea},
	title = {Mapping the scholarly research on restaurants: a bibliometric analysis},
	year = {2024},
	journal = {Journal of Foodservice Business Research},
	volume = {27},
	number = {5},
	pages = {523 – 572},
	doi = {10.1080/15378020.2022.2136477},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141158122&doi=10.1080%2f15378020.2022.2136477&partnerID=40&md5=3522fe07aec47aa50dbd615ec48e41ec},
	abstract = {Given the recent surge in restaurant research, there is a need for timely reviews employing quantitative methods to portray the intellectual structure of the field. This paper aims to address this gap by conducting a comprehensive bibliometric analysis of restaurant research on the Web of Science database. The research investigates the dynamic evolution of the restaurant literature during three critical stages between 1995 and 2021. Based on 1146 journal articles published by 1849 authors, the paper analyzes different bibliometric networks, including co-citation, keyword co-occurrence, and collaboration networks. The study additionally highlights the most influential scholars and publications in the restaurant field. Results indicate that restaurant research has grown exponentially over the last five years. Findings also show that consumer behavior, consumer satisfaction, consumer-brand relationships, corporate social responsibility, and green restaurants represent the contemporary hotspots in restaurant research. Finally, the study provides practical implications and some opportunities for future research. © 2022 Taylor & Francis.},
	publication_stage = {Final}
}

@ARTICLE{Pang202380,
	author = {Pang, Beibei and Gou, Juanqiong and Afsarmanesh, Hamideh and Mu, Wenxin and Zhang, Zuopeng},
	title = {Methodology and mechanisms for federation of heterogeneous metadata sources and ontology development in emerging collaborative environment},
	year = {2023},
	journal = {VINE Journal of Information and Knowledge Management Systems},
	volume = {53},
	number = {1},
	pages = {80 – 99},
	doi = {10.1108/VJIKMS-09-2020-0159},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107336445&doi=10.1108%2fVJIKMS-09-2020-0159&partnerID=40&md5=da74b4759f3668a0b49d526d237667c7},
	abstract = {Purpose: Leading-edge information and communication technology provides the base to facilitate obtaining, interoperating and federating shared metadata knowledge in collaborative networks from multiple heterogeneous data sources. The purpose of this study is to develop a methodology and a set of mechanisms to support this task in the collaborative environment. Design/methodology/approach: In this paper, the authors first identify and capture four main typical sources to find or generate metadata knowledge for shared data in emerging networked environments, including existing well-designed metadata, the typical ones are relational schemas of existing databases in the environment; fragmented metadata sources, i.e. metadata that can be realized from existing mission statements and example application scenarios in the environment, usually characterized by their fragmented, lightweight and behavior-intensive features; extracting metadata for simple labeled unstructured data, e.g. textual communications among its stakeholders; and semantic constraints on metadata, e.g. the temporal data behavior could be generated from governance policies in the environment. Second, the authors introduce their systematic methodology to the unification of the resulted metadata consisting of four semiautomated unification steps that gradually develops and enhances a unified ontology for the environment, formalized in web ontology language. Findings: The methodology steps and their corresponding mechanisms are described and exemplified in detail in this paper. Furthermore, this paper presents the outcome of applying the authors’ methodology to an example emerging case through the generation of a unified ontology for that environment. Originality/value: The addressed example application area is a real case in the field of higher education in China and therefore serves as a proof of concept and verification of the effectiveness of the authors’ proposed approach. © 2020, Emerald Publishing Limited.},
	publication_stage = {Final}
}

@ARTICLE{Lobentanzer20231056,
	author = {Lobentanzer, Sebastian and Aloy, Patrick and Baumbach, Jan and Bohar, Balazs and Carey, Vincent J. and Charoentong, Pornpimol and Danhauser, Katharina and Doğan, Tunca and Dreo, Johann and Dunham, Ian and Farr, Elias and Fernandez-Torras, Adrià and Gyori, Benjamin M. and Hartung, Michael and Hoyt, Charles Tapley and Klein, Christoph and Korcsmaros, Tamas and Maier, Andreas and Mann, Matthias and Ochoa, David and Pareja-Lorente, Elena and Popp, Ferdinand and Preusse, Martin and Probul, Niklas and Schwikowski, Benno and Sen, Bünyamin and Strauss, Maximilian T. and Turei, Denes and Ulusoy, Erva and Waltemath, Dagmar and Wodke, Judith A. H. and Saez-Rodriguez, Julio},
	title = {Democratizing knowledge representation with BioCypher},
	year = {2023},
	journal = {Nature Biotechnology},
	volume = {41},
	number = {8},
	pages = {1056 – 1059},
	doi = {10.1038/s41587-023-01848-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162101723&doi=10.1038%2fs41587-023-01848-y&partnerID=40&md5=e387659e7ace978b12ad6eba33ee4c3c},
	publication_stage = {Final}
}

@ARTICLE{Muscolino2022,
	author = {Muscolino, Alessandro and Di Maria, Antonio and Rapicavoli, Rosaria Valentina and Alaimo, Salvatore and Bellomo, Lorenzo and Billeci, Fabrizio and Borzì, Stefano and Ferragina, Paolo and Ferro, Alfredo and Pulvirenti, Alfredo},
	title = {NETME: on-the-fly knowledge network construction from biomedical literature},
	year = {2022},
	journal = {Applied Network Science},
	volume = {7},
	number = {1},
	doi = {10.1007/s41109-021-00435-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122539967&doi=10.1007%2fs41109-021-00435-x&partnerID=40&md5=bb4206ea2b4b616ceae72063ba1b7739},
	abstract = {Background: The rapidly increasing biological literature is a key resource to automatically extract and gain knowledge concerning biological elements and their relations. Knowledge Networks are helpful tools in the context of biological knowledge discovery and modeling. Results: We introduce a novel system called NETME, which, starting from a set of full-texts obtained from PubMed, through an easy-to-use web interface, interactively extracts biological elements from ontological databases and then synthesizes a network inferring relations among such elements. The results clearly show that our tool is capable of inferring comprehensive and reliable biological networks. © 2021, The Author(s).},
	publication_stage = {Final}
}

@BOOK{Ni2022157,
	author = {Ni, Lisa and Phuong, Christina and Hong, Julian},
	title = {Natural Language Processing for Radiation Oncology},
	year = {2022},
	journal = {Artificial Intelligence in Radiation Oncology},
	pages = {157 – 183},
	doi = {10.1142/9789811263545_0008},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153425888&doi=10.1142%2f9789811263545_0008&partnerID=40&md5=ef75804b120b5afdcf1551cbc04785f1},
	abstract = {The bulk of clinical information in electronic medical records (EMR) is in narrative form. Unlike structured data, while free text is effective and convenient for communication and documentation, it is not easily translatable for research, quality improvement, or clinical decision support. Recently, there has been increasing interest in the use of natural language processing (NLP) to extract the valuable clinical information from free-text narratives available within EMRs. This chapter aims to provide an overview of NLP technologies, applications in medicine and oncology in particular, and future directions that will facilitate advances in the field of radiation oncology. © 2023 by World Scientific Publishing Co. Pte. Ltd.},
	publication_stage = {Final}
}

@ARTICLE{Lou2022,
	author = {Lou, Peiliang and Wang, Chunbao and Guo, Ruifeng and Yao, Lixia and Zhang, Guanjun and Yang, Jun and Yuan, Yong and Dong, Yuxin and Gao, Zeyu and Gong, Tieliang and Li, Chen},
	title = {HistoML, a markup language for representation and exchange of histopathological features in pathology images},
	year = {2022},
	journal = {Scientific Data},
	volume = {9},
	number = {1},
	doi = {10.1038/s41597-022-01505-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133702787&doi=10.1038%2fs41597-022-01505-0&partnerID=40&md5=e8d8ca0221519fca0986cd4e96c6323a},
	abstract = {The study of histopathological phenotypes is vital for cancer research and medicine as it links molecular mechanisms to disease prognosis. It typically involves integration of heterogenous histopathological features in whole-slide images (WSI) to objectively characterize a histopathological phenotype. However, the large-scale implementation of phenotype characterization has been hindered by the fragmentation of histopathological features, resulting from the lack of a standardized format and a controlled vocabulary for structured and unambiguous representation of semantics in WSIs. To fill this gap, we propose the Histopathology Markup Language (HistoML), a representation language along with a controlled vocabulary (Histopathology Ontology) based on Semantic Web technologies. Multiscale features within a WSI, from single-cell features to mesoscopic features, could be represented using HistoML which is a crucial step towards the goal of making WSIs findable, accessible, interoperable and reusable (FAIR). We pilot HistoML in representing WSIs of kidney cancer as well as thyroid carcinoma and exemplify the uses of HistoML representations in semantic queries to demonstrate the potential of HistoML-powered applications for phenotype characterization. © 2022, The Author(s).},
	publication_stage = {Final}
}

@ARTICLE{Guan20214623,
	author = {Guan, Ting and Qan’ir, Yousef and Song, Lixin},
	title = {Systematic review of illness uncertainty management interventions for cancer patients and their family caregivers},
	year = {2021},
	journal = {Supportive Care in Cancer},
	volume = {29},
	number = {8},
	pages = {4623 – 4640},
	doi = {10.1007/s00520-020-05931-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099853653&doi=10.1007%2fs00520-020-05931-x&partnerID=40&md5=819430a60c6918028717a239112e0597},
	abstract = {Purpose: Illness uncertainty pervades individuals’ experiences of cancer across the illness trajectory and is associated with poor psychological adjustment. This review systematically examined the characteristics and outcomes of interventions promoting illness uncertainty management among cancer patients and/or their family caregivers. Methods: PubMed, Scopus, PsycINFO, Cumulative Index to Nursing and Allied Health Literature (CINAHL), Embase, and Cochrane Database of Systematic Reviews were systematically searched for relevant literature. We included randomized controlled trials (RCTs) and quasi-experimental studies focusing on interventions for uncertainty management in cancer patients and/or their family caregivers. Results: Our database searches yielded 26 studies. Twenty interventions were only offered to cancer patients, who were mostly elder, female, and White. All interventions included informational support. Other intervention components included emotional support, appraisal support, and instrumental support. Most interventions were delivered in person and via telephone (n = 8) or exclusively in person (n = 7). Overall, 18 studies identified positive intervention effects on illness uncertainty outcomes. Conclusion: This systematic review foregrounds the promising potential of several interventions—and especially multi-component interventions—to promote uncertainty management among cancer patients and their family caregivers. To further improve these interventions’ effectiveness and expand their potential impact, future uncertainty management interventions should be tested among more diverse populations using rigorous methodologies. © 2021, The Author(s).},
	publication_stage = {Final}
}

@BOOK{Upton20241,
	author = {Upton, Dominic and Thirlaway, Katie},
	title = {Promoting Healthy Behaviour: A Practical Guide to Physical Health and Mental Wellbeing: Third Edition},
	year = {2024},
	journal = {Promoting Healthy Behaviour: A Practical Guide to Physical Health and Mental Wellbeing: Third Edition},
	pages = {1 – 380},
	doi = {10.4324/9781003471233},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191509267&doi=10.4324%2f9781003471233&partnerID=40&md5=fc54508ec84e6b0196bd576ed50db98d},
	abstract = {The new edition of this leading text is an essential guide to promoting healthy behaviour in a multi-cultural society, providing a holistic stance that integrates both physical and mental health and wellbeing. With a comprehensive overview of the interplay between social class, gender, ethnicity and individual health differences, the book also looks at key lifestyle issues such as eating well, smoking, drinking alcohol and safe sex, as well as the mechanisms for behavioural change. Each chapter features engaging case studies, points for discussion and student activities. Updated since the COVID-19 pandemic, the new edition also discusses the effects of lockdowns on healthy behaviours. An accessible and engaging text, the third edition of Promoting Healthy Behaviour will continue to be essential reading for both students and practitioners across nursing, public health and allied health professions. © 2024 Dominic Upton and Katie Thirlaway.},
	publication_stage = {Final}
}

@BOOK{Parahoo20241,
	author = {Parahoo, Kader},
	title = {How to Design Studies and Write Research Proposals: A Guide for Nursing, Allied Health and Social Care Students},
	year = {2024},
	journal = {How to Design Studies and Write Research Proposals: A Guide for Nursing, Allied Health and Social Care Students},
	pages = {1 – 276},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205208360&partnerID=40&md5=a17e73dd3bfbe3d92765fee008ad30bc},
	abstract = {If you’re writing a research proposal for the first time, this is the book for you. It’s the only text on the market that guides you through the entire process, from designing a research study to submitting a successful proposal. It covers everything from formulating the research question to selecting the research methodology, collecting data, and navigating ethics, all supported with plenty of practical tips, real life examples and checklists for honing your proposal. How to Design Studies and Write Research Proposals is written by nursing research expert Professor Kader Parahoo, whose work is loved by students for its accessible writing style and practical approach. © 2024 Elsevier Inc. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Bizerea-Moga2024,
	author = {Bizerea-Moga, Teofana Otilia and Pitulice, Laura and Bizerea-Spiridon, Otilia and Moga, Tudor Voicu},
	title = {Exploring the Link between Oxidative Stress, Selenium Levels, and Obesity in Youth},
	year = {2024},
	journal = {International Journal of Molecular Sciences},
	volume = {25},
	number = {13},
	doi = {10.3390/ijms25137276},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198407814&doi=10.3390%2fijms25137276&partnerID=40&md5=b520e5423170275720f2cecba2534bc7},
	abstract = {Obesity is a worldwide increasing concern. Although in adults this is easily estimated with the body mass index, in children, who are constantly growing and whose bodies are changing, the reference points to assess weight status are age and gender, and need corroboration with complementary data, making their quantification highly difficult. The present review explores the interaction spectrum of oxidative stress, selenium status, and obesity in children and adolescents. Any factor related to oxidative stress that triggers obesity and, conversely, obesity that induces oxidative stress are part of a vicious circle, a complex chain of mechanisms that derive from each other and reinforce each other with serious health consequences. Selenium and its compounds exhibit key antioxidant activity and also have a significant role in the nutritional evaluation of obese children. The balance of selenium intake, retention, and metabolism emerges as a vital aspect of health, reflecting the complex interactions between diet, oxidative stress, and obesity. Understanding whether selenium status is a contributor to or a consequence of obesity could inform nutritional interventions and public health strategies aimed at preventing and managing obesity from an early age. © 2024 by the authors.},
	publication_stage = {Final}
}

@ARTICLE{Rahman2024,
	author = {Rahman, Md. Mijanur and Khatun, Fatema and Jahan, Ismat and Devnath, Ramprosad and Bhuiyan, Md. Al-Amin},
	title = {Cobotics: The Evolving Roles and Prospects of Next-Generation Collaborative Robots in Industry 5.0},
	year = {2024},
	journal = {Journal of Robotics},
	volume = {2024},
	doi = {10.1155/2024/2918089},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203544289&doi=10.1155%2f2024%2f2918089&partnerID=40&md5=83eb48ca8aa5d18b57f44e147b98d3ed},
	abstract = {The emergence of Industry 5.0 marks a transformative era in manufacturing industry that is driven by the integration of advanced technologies. It is a renewed focus on human-robot collaboration. At the forefront of this industrial revolution stands Cobotics, a field that emphasizes the cooperative interaction between humans and robots in shared workspaces. This research paper delves into the critical role and implications of Cobotics in the context of Industry 5.0. The study addresses the research problem of effectively integrating Cobots into industrial processes, considering technical, economic, and social challenges. The paper's objectives include surveying existing literature, analyzing developments and trends, showcasing real-world examples, and proposing future research directions. A comprehensive literature review explores Cobotics' foundations and its alignment with Industry 5.0 principles. The research presents real-world examples of successful Cobot integration across various industries, demonstrating benefits such as increased productivity, improved quality, and enhanced worker safety. The paper identifies gaps in existing knowledge and contributions to the field, emphasizing the need for research in simplified programming, cost-benefit analysis, and ethical frameworks. This research paper illuminates the central role of Cobotics in Industry 5.0, where human-robot collaboration, innovation, and inclusivity are paramount. It anticipates a future where Cobotics will continue to evolve, shaping industries, empowering the workforce, and driving sustainable, data-driven industrial transformations.  © 2024 Md. Mijanur Rahman et al.},
	publication_stage = {Final}
}

@ARTICLE{Raboudi2022,
	author = {Raboudi, Amel and Allanic, Marianne and Balvay, Daniel and Hervé, Pierre-Yves and Viel, Thomas and Yoganathan, Thulaciga and Certain, Anais and Hilbey, Jacques and Charlet, Jean and Durupt, Alexandre and Boutinaud, Philippe and Eynard, Benoît and Tavitian, Bertrand},
	title = {The BMS-LM ontology for biomedical data reporting throughout the lifecycle of a research study: From data model to ontology},
	year = {2022},
	journal = {Journal of Biomedical Informatics},
	volume = {127},
	doi = {10.1016/j.jbi.2022.104007},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124697234&doi=10.1016%2fj.jbi.2022.104007&partnerID=40&md5=13152f1080870eb3ad10b581ec8dd914},
	abstract = {Biomedical research data reuse and sharing is essential for fostering research progress. To this aim, data producers need to master data management and reporting through standard and rich metadata, as encouraged by open data initiatives such as the FAIR (Findable, Accessible, Interoperable, Reusable) guidelines. This helps data re-users to understand and reuse the shared data with confidence. Therefore, dedicated frameworks are required. The provenance reporting throughout a biomedical study lifecycle has been proposed as a way to increase confidence in data while reusing it. The Biomedical Study - Lifecycle Management (BMS-LM) data model has implemented provenance and lifecycle traceability for several multimodal-imaging techniques but this is not enough for data understanding while reusing it. Actually, in the large scope of biomedical research, a multitude of metadata sources, also called Knowledge Organization Systems (KOSs), are available for data annotation. In addition, data producers uses local terminologies or KOSs, containing vernacular terms for data reporting. The result is a set of heterogeneous KOSs (local and published) with different formats and levels of granularity. To manage the inherent heterogeneity, semantic interoperability is encouraged by the Research Data Management (RDM) community. Ontologies, and more specifically top ontologies such as BFO and DOLCE, make explicit the metadata semantics and enhance semantic interoperability. Based on the BMS-LM data model and the BFO top ontology, the BioMedical Study - Lifecycle Management (BMS-LM) core ontology is proposed together with an associated framework for semantic interoperability between heterogeneous KOSs. It is made of four ontological levels: top/core/domain/local and aims to build bridges between local and published KOSs. In this paper, the conversion of the BMS-LM data model to a core ontology is detailed. The implementation of its semantic interoperability in a specific domain context is explained and illustrated with examples from small animal preclinical research. © 2022 Elsevier Inc.},
	publication_stage = {Final}
}

@ARTICLE{Shetty2022423,
	author = {Shetty, Shashank and Ananthanarayana, V.S. and Mahale, Ajit},
	title = {Comprehensive Review of Multimodal Medical Data Analysis: Open Issues and Future Research Directions},
	year = {2022},
	journal = {Acta Informatica Pragensia},
	volume = {11},
	number = {3},
	pages = {423 – 457},
	doi = {10.18267/j.aip.202},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146527274&doi=10.18267%2fj.aip.202&partnerID=40&md5=7b0fc88c0a36b84399a9f738a73f0c03},
	abstract = {Over the past few decades, the enormous expansion of medical data has led to searching for ways of data analysis in smart healthcare systems. Acquisition of data from pictures, archives, communication systems, electronic health records, online documents, radiology reports and clinical records of different styles with specific numerical information has given rise to the concept of multimodality and the need for machine learning and deep learning techniques in the analysis of the healthcare system. Medical data play a vital role in medical education and diagnosis; determining dependency between distinct modalities is essential. This paper gives a gist of current radiology medical data analysis techniques and their various approaches and frameworks for representation and classification. A brief outline of the existing medical multimodal data processing work is presented. The main objective of this study is to spot gaps in the surveyed area and list future tasks and challenges in radiology. The Preferred Reporting Items for Systematic Reviews and Meta-Analysis (or PRISMA) guidelines were incorporated in this study for effective article search and to investigate several relevant scientific publications. The systematic review was carried out on multimodal medical data analysis and highlighted advantages, limitations and strategies. The inherent benefit of multimodality in the medical domain powered with artificial intelligence has a significant impact on the performance of the disease diagnosis frameworks. © 2022 by the author(s).},
	publication_stage = {Final}
}

@ARTICLE{Demelo2021,
	author = {Demelo, Jonathan and Sedig, Kamran},
	title = {Design of generalized search interfaces for health informatics},
	year = {2021},
	journal = {Information (Switzerland)},
	volume = {12},
	number = {8},
	doi = {10.3390/info12080317},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112684251&doi=10.3390%2finfo12080317&partnerID=40&md5=68ec45a8e513e16acdddfa4f2e668a57},
	abstract = {In this paper, we investigate ontology-supported interfaces for health informatics search tasks involving large document sets. We begin by providing background on health informatics, machine learning, and ontologies. We review leading research on health informatics search tasks to help formulate high-level design criteria. We use these criteria to examine traditional design strategies for search interfaces. To demonstrate the utility of the criteria, we apply them to the design of ONTology-supported Search Interface (ONTSI), a demonstrative, prototype system. ONTSI allows users to plug-and-play document sets and expert-defined domain ontologies through a generalized search interface. ONTSI’s goal is to help align users’ common vocabulary with the domain-specific vocabulary of the plug-and-play document set. We describe the functioning and utility of ONTSI in health informatics search tasks through a workflow and a scenario. We conclude with a summary of ongoing evaluations, limitations, and future research. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	publication_stage = {Final}
}

@BOOK{Labory2024201,
	author = {Labory, Justine and Bottini, Silvia},
	title = {The multiomics revolution in the era of deep learning: Allies or enemies?},
	year = {2024},
	journal = {Artificial Intelligence for Medicine: An Applied Reference for Methods and Applications},
	pages = {201 – 216},
	doi = {10.1016/B978-0-443-13671-9.00017-X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200531018&doi=10.1016%2fB978-0-443-13671-9.00017-X&partnerID=40&md5=727ff57016234edda24538f2d24b8503},
	abstract = {The advent of high-throughput omics technologies has led to the generation of a large volume of omics data to be analyzed. Individual analyses of omics layers provide only a one-dimensional view of a complex system and are no longer sufficient. Multi-omics data provide a global view of a patient sample, and their integration has become essential to better understand biological systems and disease mechanisms. Thanks to machine learning methods, new omics data integration techniques are possible to analyze the complex interactions between the omics data, allowing the discovery of new biomarkers for complex diseases. Here, we first provide an overview of single-omic data modalities; then, we discuss the open challenges and how different learning models can be employed for multiomics data integration methods. We highlight that there is no golden standard model but the choice of which model to use depends on the biomedical application and the omics data available. © 2024 Elsevier Inc. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Maier-Hein2022,
	author = {Maier-Hein, Lena and Eisenmann, Matthias and Sarikaya, Duygu and März, Keno and Collins, Toby and Malpani, Anand and Fallert, Johannes and Feussner, Hubertus and Giannarou, Stamatia and Mascagni, Pietro and Nakawala, Hirenkumar and Park, Adrian and Pugh, Carla and Stoyanov, Danail and Vedula, Swaroop S. and Cleary, Kevin and Fichtinger, Gabor and Forestier, Germain and Gibaud, Bernard and Grantcharov, Teodor and Hashizume, Makoto and Heckmann-Nötzel, Doreen and Kenngott, Hannes G. and Kikinis, Ron and Mündermann, Lars and Navab, Nassir and Onogur, Sinan and Roß, Tobias and Sznitman, Raphael and Taylor, Russell H. and Tizabi, Minu D. and Wagner, Martin and Hager, Gregory D. and Neumuth, Thomas and Padoy, Nicolas and Collins, Justin and Gockel, Ines and Goedeke, Jan and Hashimoto, Daniel A. and Joyeux, Luc and Lam, Kyle and Leff, Daniel R. and Madani, Amin and Marcus, Hani J. and Meireles, Ozanan and Seitel, Alexander and Teber, Dogu and Ückert, Frank and Müller-Stich, Beat P. and Jannin, Pierre and Speidel, Stefanie},
	title = {Surgical data science – from concepts toward clinical translation},
	year = {2022},
	journal = {Medical Image Analysis},
	volume = {76},
	doi = {10.1016/j.media.2021.102306},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120910690&doi=10.1016%2fj.media.2021.102306&partnerID=40&md5=aeceb686e1b4d7e36515143adf389695},
	abstract = {Recent developments in data science in general and machine learning in particular have transformed the way experts envision the future of surgery. Surgical Data Science (SDS) is a new research field that aims to improve the quality of interventional healthcare through the capture, organization, analysis and modeling of data. While an increasing number of data-driven approaches and clinical applications have been studied in the fields of radiological and clinical data science, translational success stories are still lacking in surgery. In this publication, we shed light on the underlying reasons and provide a roadmap for future advances in the field. Based on an international workshop involving leading researchers in the field of SDS, we review current practice, key achievements and initiatives as well as available standards and tools for a number of topics relevant to the field, namely (1) infrastructure for data acquisition, storage and access in the presence of regulatory constraints, (2) data annotation and sharing and (3) data analytics. We further complement this technical perspective with (4) a review of currently available SDS products and the translational progress from academia and (5) a roadmap for faster clinical translation and exploitation of the full potential of SDS, based on an international multi-round Delphi process. © 2021},
	publication_stage = {Final}
}

@ARTICLE{Caufield2023,
	author = {Caufield, J. Harry and Putman, Tim and Schaper, Kevin and Unni, Deepak R and Hegde, Harshad and Callahan, Tiffany J and Cappelletti, Luca and Moxon, Sierra A. T and Ravanmehr, Vida and Carbon, Seth and Chan, Lauren E and Cortes, Katherina and Shefchek, Kent A and Elsarboukh, Glass and Balhoff, Jim and Fontana, Tommaso and Matentzoglu, Nicolas and Bruskiewich, Richard M and Thessen, Anne E and Harris, Nomi L and Munoz-Torres, Monica C and Haendel, Melissa A and Robinson, Peter N and Joachimiak, Marcin P and Mungall, Christopher J and Reese, Justin T},
	title = {KG-Hub - building and exchanging biological knowledge graphs},
	year = {2023},
	journal = {Bioinformatics},
	volume = {39},
	number = {7},
	doi = {10.1093/bioinformatics/btad418},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164291000&doi=10.1093%2fbioinformatics%2fbtad418&partnerID=40&md5=eef1cbbffd30604bd7bfe83d657a96f8},
	abstract = {Motivation: Knowledge graphs (KGs) are a powerful approach for integrating heterogeneous data and making inferences in biology and many other domains, but a coherent solution for constructing, exchanging, and facilitating the downstream use of KGs is lacking. Results: Here we present KG-Hub, a platform that enables standardized construction, exchange, and reuse of KGs. Features include a simple, modular extract-transform-load pattern for producing graphs compliant with Biolink Model (a high-level data model for standardizing biological data), easy integration of any OBO (Open Biological and Biomedical Ontologies) ontology, cached downloads of upstream data sources, versioned and automatically updated builds with stable URLs, web-browsable storage of KG artifacts on cloud infrastructure, and easy reuse of transformed subgraphs across projects. Current KG-Hub projects span use cases including COVID-19 research, drug repurposing, microbial-environmental interactions, and rare disease research. KG-Hub is equipped with tooling to easily analyze and manipulate KGs. KG-Hub is also tightly integrated with graph machine learning (ML) tools which allow automated graph ML, including node embeddings and training of models for link prediction and node classification.  © 2023 The Author(s). Published by Oxford University Press.},
	publication_stage = {Final}
}

@ARTICLE{Gruendner2022,
	author = {Gruendner, Julian and Deppenwiese, Noemi and Folz, Michael and Köhler, Thomas and Kroll, Björn and Prokosch, Hans-Ulrich and Rosenau, Lorenz and Rühle, Mathias and Scheidl, Marc-Anton and Schüttler, Christina and Sedlmayr, Brita and Twrdik, Alexander and Kiel, Alexander and Majeed, Raphael W.},
	title = {The Architecture of a Feasibility Query Portal for Distributed COVID-19 Fast Healthcare Interoperability Resources (FHIR) Patient Data Repositories: Design and Implementation Study},
	year = {2022},
	journal = {JMIR Medical Informatics},
	volume = {10},
	number = {5},
	doi = {10.2196/36709},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129664464&doi=10.2196%2f36709&partnerID=40&md5=6c343b04a51d6f07fd07c2dee41eb526},
	abstract = {Background: An essential step in any medical research project after identifying the research question is to determine if there are sufficient patients available for a study and where to find them. Pursuing digital feasibility queries on available patient data registries has proven to be an excellent way of reusing existing real-world data sources. To support multicentric research, these feasibility queries should be designed and implemented to run across multiple sites and securely access local data. Working across hospitals usually involves working with different data formats and vocabularies. Recently, the Fast Healthcare Interoperability Resources (FHIR) standard was developed by Health Level Seven to address this concern and describe patient data in a standardized format. The Medical Informatics Initiative in Germany has committed to this standard and created data integration centers, which convert existing data into the FHIR format at each hospital. This partially solves the interoperability problem; however, a distributed feasibility query platform for the FHIR standard is still missing. Objective: This study described the design and implementation of the components involved in creating a cross-hospital feasibility query platform for researchers based on FHIR resources. This effort was part of a large COVID-19 data exchange platform and was designed to be scalable for a broad range of patient data. Methods: We analyzed and designed the abstract components necessary for a distributed feasibility query. This included a user interface for creating the query, backend with an ontology and terminology service, middleware for query distribution, and FHIR feasibility query execution service. Results: We implemented the components described in the Methods section. The resulting solution was distributed to 33 German university hospitals. The functionality of the comprehensive network infrastructure was demonstrated using a test data set based on the German Corona Consensus Data Set. A performance test using specifically created synthetic data revealed the applicability of our solution to data sets containing millions of FHIR resources. The solution can be easily deployed across hospitals and supports feasibility queries, combining multiple inclusion and exclusion criteria using standard Health Level Seven query languages such as Clinical Quality Language and FHIR Search. Developing a platform based on multiple microservices allowed us to create an extendable platform and support multiple Health Level Seven query languages and middleware components to allow integration with future directions of the Medical Informatics Initiative. Conclusions: We designed and implemented a feasibility platform for distributed feasibility queries, which works directly on FHIR-formatted data and distributed it across 33 university hospitals in Germany. We showed that developing a feasibility platform directly on the FHIR standard is feasible. ©Julian Gruendner, Noemi Deppenwiese, Michael Folz, Thomas Köhler, Björn Kroll, Hans-Ulrich Prokosch, Lorenz Rosenau, Mathias Rühle, Marc-Anton Scheidl, Christina Schüttler, Brita Sedlmayr, Alexander Twrdik, Alexander Kiel, Raphael W Majeed.},
	publication_stage = {Final}
}

@ARTICLE{Sheng2024328,
	author = {Sheng, Nan and Xie, Xuping and Wang, Yan and Huang, Lan and Zhang, Shuangquan and Gao, Ling and Wang, Hao},
	title = {A Survey of Deep Learning for Detecting miRNA- Disease Associations: Databases, Computational Methods, Challenges, and Future Directions},
	year = {2024},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	volume = {21},
	number = {3},
	pages = {328 – 347},
	doi = {10.1109/TCBB.2024.3351752},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182364126&doi=10.1109%2fTCBB.2024.3351752&partnerID=40&md5=8cf291e310df7f1cc38d81d20b98af6f},
	abstract = {MicroRNAs (miRNAs) are an important class of non-coding RNAs that play an essential role in the occurrence and development of various diseases. Identifying the potential miRNA-disease associations (MDAs) can be beneficial in understanding disease pathogenesis. Traditional laboratory experiments are expensive and time-consuming. Computational models have enabled systematic large-scale prediction of potential MDAs, greatly improving the research efficiency. With recent advances in deep learning, it has become an attractive and powerful technique for uncovering novel MDAs. Consequently, numerous MDA prediction methods based on deep learning have emerged. In this review, we first summarize publicly available databases related to miRNAs and diseases for MDA prediction. Next, we outline commonly used miRNA and disease similarity calculation and integration methods. Then, we comprehensively review the 48 existing deep learning-based MDA computation methods, categorizing them into classical deep learning and graph neural network-based techniques. Subsequently, we investigate the evaluation methods and metrics that are frequently used to assess MDA prediction performance. Finally, we discuss the performance trends of different computational methods, point out some problems in current research, and propose 9 potential future research directions. Data resources and recent advances in MDA prediction methods are summarized in the GitHub repository https://github.com/sheng-n/DL-miRNA-disease-association-methods.  © 2004-2012 IEEE.},
	publication_stage = {Final}
}

@ARTICLE{Penn2023,
	author = {Penn, Steve and Lomax, Jane and Karlsson, Anneli and Antonucci, Vincent and Zachmann, Carl-Dieter and Kanza, Samantha and Schurer, Stephan and Turner, John},
	title = {An extension of the BioAssay Ontology to include pharmacokinetic/pharmacodynamic terminology for the enrichment of scientific workflows},
	year = {2023},
	journal = {Journal of Biomedical Semantics},
	volume = {14},
	number = {1},
	doi = {10.1186/s13326-023-00288-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167757756&doi=10.1186%2fs13326-023-00288-6&partnerID=40&md5=8c5649c1f2c3866fd7f210e1b39f3dbb},
	abstract = {With the capacity to produce and record data electronically, Scientific research and the data associated with it have grown at an unprecedented rate. However, despite a decent amount of data now existing in an electronic form, it is still common for scientific research to be recorded in an unstructured text format with inconsistent context (vocabularies) which vastly reduces the potential for direct intelligent analysis. Research has demonstrated that the use of semantic technologies such as ontologies to structure and enrich scientific data can greatly improve this potential. However, whilst there are many ontologies that can be used for this purpose, there is still a vast quantity of scientific terminology that does not have adequate semantic representation. A key area for expansion identified by the authors was the pharmacokinetic/pharmacodynamic (PK/PD) domain due to its high usage across many areas of Pharma. As such we have produced a set of these terms and other bioassay related terms to be incorporated into the BioAssay Ontology (BAO), which was identified as the most relevant ontology for this work. A number of use cases developed by experts in the field were used to demonstrate how these new ontology terms can be used, and to set the scene for the continuation of this work with a look to expanding this work out into further relevant domains. The work done in this paper was part of Phase 1 of the SEED project (Semantically Enriching electronic laboratory notebook (eLN) Data). © 2023, BioMed Central Ltd., part of Springer Nature.},
	publication_stage = {Final}
}

@CONFERENCE{Lazic2022942,
	author = {Lazic, Ivan and Jakovljevic, Niksa and Boban, Jasmina and Nosek, Igor and Loncar-Turukalo, Tatjana},
	title = {Information extraction from clinical records: an example for breast cancer},
	year = {2022},
	journal = {MELECON 2022 - IEEE Mediterranean Electrotechnical Conference, Proceedings},
	pages = {942 – 947},
	doi = {10.1109/MELECON53508.2022.9842995},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136410314&doi=10.1109%2fMELECON53508.2022.9842995&partnerID=40&md5=febfcfdae17100ff253b24824acda9f6},
	abstract = {The extraction of relevant information from electronic health records (EHR) can facilitate large scale clinical studies related to certain diseases to uncover diversity of their biological and clinical signatures, and patterns of treatment and prognosis. Variety of EHR formats and use of clinical narrative present significant challenges to this task. In this work we describe a process of an automated information extraction from an oncology hospital clinical reports related to 2966 subjects with suspected or confirmed breast cancer. The lack of open medical term dictionaries for the Serbian language and the variety of clinical data types required, imply the use of rule-based approaches with exact matches, regular expressions, hierarchical rules and customized mini dictionaries to analyze clinical text. The accuracy of the applied approach has been validated on manually extracted clinical data of 50 breast cancer patients. The accuracy varied, field dependent, between 71.3% to 100%, indicating that certain relevant fields can be successfully captured, yet implying the need for sophisticated natural language processing tools for accurate extraction of more descriptive features.  © 2022 IEEE.},
	publication_stage = {Final}
}

@ARTICLE{Davis20211,
	author = {Davis, Leona F. and Ramírez-Andreotta, Mónica D.},
	title = {Participatory research for environmental justice: a critical interpretive synthesis},
	year = {2021},
	journal = {Environmental Health Perspectives},
	volume = {129},
	number = {2},
	pages = {1 – 20},
	doi = {10.1289/EHP6274},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101431519&doi=10.1289%2fEHP6274&partnerID=40&md5=c7e49828a90d95ccd2eb10f372964822},
	abstract = {BACKGROUND: Environmental health risks are disproportionately colocated with communities in poverty and communities of color. In some cases, participatory research projects have effectively addressed structural causes of health risk in environmental justice (EJ) communities. However, many such projects fail to catalyze change at a structural level. OBJECTIVES: This review employs Critical Interpretive Synthesis (CIS) to theorize specific elements of participatory research for environmental health that effectively prompt structural change in EJ communities. METHODS: Academic database search was used to identify peer-reviewed literature describing participatory research with EJ communities to address environmental health. Synthetic constructs were developed iteratively related to study characteristics, design elements, and outcomes; and data were extracted for included records. Statistical analyses were performed to assess correlations between study design elements and structural change outcomes. Through critical, comparative, and contextual analyses of the “structural change” case study group and “non-structural change” group, informed by relevant theoretical literature, a synthesizing argument was generated. RESULTS: From 505 total records identified, eligibility screening produced 232 case study articles, representing 154 case studies, and 55 theoretical articles for synthesis. Twenty-six case studies resulted in a structural change outcome. The synthesizing argument states that participatory research with EJ communities may be more likely to result in structural change when a) community members hold formal leadership roles; b) project design includes decision-makers and policy goals; and c) long term partnerships are sustained through multiple funding mechanisms. The assumption of EJ community benefit through research participation is critically examined. DISCUSSION: Recommended future directions include establishing structural change as a goal of participatory research, employing participatory assessment of community benefit, and increased hiring of faculty of color at research institutions. The power, privilege, and political influence that academic institutions are able to leverage in partnership with EJ communities may be as valuable as the research itself. © 2021, Public Health Services, US Dept of Health and Human Services. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Faria2024,
	author = {Faria, Daniel and Eugénio, Patrícia and Silva, Marta Contreiras and Balbi, Laura and Bedran, Georges and Kallor, Ashwin Adrian and Nunes, Susana and Palkowski, Aleksander and Waleron, Michal and Alfaro, Javier A. and Pesquita, Catia},
	title = {The Immunopeptidomics Ontology (ImPO)},
	year = {2024},
	journal = {Database},
	volume = {2024},
	doi = {10.1093/database/baae014},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195623282&doi=10.1093%2fdatabase%2fbaae014&partnerID=40&md5=83277c087c13424537429ae8d7d85d85},
	abstract = {The adaptive immune response plays a vital role in eliminating infected and aberrant cells from the body.This process hinges on the presentation of short peptides by major histocompatibility complex Class I molecules on the cell surface. Immunopeptidomics, the study of peptides displayed on cells, delves into the wide variety of these peptides. Understanding the mechanisms behind antigen processing and presentation is crucial for effectively evaluating cancer immunotherapies. As an emerging domain, immunopeptidomics currently lacks standardization—there is neither an established terminology nor formally defined semantics—a critical concern considering the complexity, heterogeneity, and growing volume of data involved in immunopeptidomics studies. Additionally, there is a disconnection between how the proteomics community delivers the information about antigen presentation and its uptake by the clinical genomics community. Considering the significant relevance of immunopeptidomics in cancer, this shortcoming must be addressed to bridge the gap between research and clinical practice. In this work, we detail the development of the ImmunoPeptidomics Ontology, ImPO, the first effort at standardizing the terminology and semantics in the domain. ImPO aims to encapsulate and systematize data generated by immunopeptidomics experimental processes and bioinformatics analysis. ImPO establishes cross-references to 24 relevant ontologies, including the National Cancer Institute Thesaurus, Mondo Disease Ontology, Logical Observation Identifier Names and Codes and Experimental Factor Ontology. Although ImPO was developed using expert knowledge to characterize a large and representative data collection, it may be readily used to encode other datasets within the domain. Ultimately, ImPO facilitates data integration and analysis, enabling querying, inference and knowledge generation and importantly bridging the gap between the clinical proteomics and genomics communities. As the field of immunogenomics uses protein-level immunopeptidomics data, we expect ImPO to play a key role in supporting a rich and standardized description of the large-scale data that emerging high-throughput technologies are expected to bring in the near future. © The Author(s) 2024.},
	publication_stage = {Final}
}

@ARTICLE{Raghavendra2023,
	author = {Raghavendra, U. and Gudigar, Anjan and Paul, Aritra and Goutham, T.S. and Inamdar, Mahesh Anil and Hegde, Ajay and Devi, Aruna and Ooi, Chui Ping and Deo, Ravinesh C. and Barua, Prabal Datta and Molinari, Filippo and Ciaccio, Edward J. and Acharya, U. Rajendra},
	title = {Brain tumor detection and screening using artificial intelligence techniques: Current trends and future perspectives},
	year = {2023},
	journal = {Computers in Biology and Medicine},
	volume = {163},
	doi = {10.1016/j.compbiomed.2023.107063},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161957423&doi=10.1016%2fj.compbiomed.2023.107063&partnerID=40&md5=9a263ecd97df5636911abe8b167df64a},
	abstract = {A brain tumor is an abnormal mass of tissue located inside the skull. In addition to putting pressure on the healthy parts of the brain, it can lead to significant health problems. Depending on the region of the brain tumor, it can cause a wide range of health issues. As malignant brain tumors grow rapidly, the mortality rate of individuals with this cancer can increase substantially with each passing week. Hence it is vital to detect these tumors early so that preventive measures can be taken at the initial stages. Computer-aided diagnostic (CAD) systems, in coordination with artificial intelligence (AI) techniques, have a vital role in the early detection of this disorder. In this review, we studied 124 research articles published from 2000 to 2022. Here, the challenges faced by CAD systems based on different modalities are highlighted along with the current requirements of this domain and future prospects in this area of research. © 2023 Elsevier Ltd},
	publication_stage = {Final}
}

@ARTICLE{Gogate2021,
	author = {Gogate, Nikhita and Lyman, Daniel and Bell, Amanda and Cauley, Edmund and Crandall, Keith A and Joseph, Ashia and Kahsay, Robel and Natale, Darren A and Schriml, Lynn M and Sen, Sabyasach and Mazumder, Raja},
	title = {COVID-19 biomarkers and their overlap with comorbidities in a disease biomarker data model},
	year = {2021},
	journal = {Briefings in Bioinformatics},
	volume = {22},
	number = {6},
	doi = {10.1093/bib/bbab191},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121952652&doi=10.1093%2fbib%2fbbab191&partnerID=40&md5=956f61717e76afeb52a10fb4c76d7495},
	abstract = {In response to the COVID-19 outbreak, scientists and medical researchers are capturing a wide range of host responses, symptoms and lingering postrecovery problems within the human population. These variable clinical manifestations suggest differences in influential factors, such as innate and adaptive host immunity, existing or underlying health conditions, comorbidities, genetics and other factors - compounding the complexity of COVID-19 pathobiology and potential biomarkers associated with the disease, as they become available. The heterogeneous data pose challenges for efficient extrapolation of information into clinical applications. We have curated 145 COVID-19 biomarkers by developing a novel cross-cutting disease biomarker data model that allows integration and evaluation of biomarkers in patients with comorbidities. Most biomarkers are related to the immune (SAA, TNF-∝ and IP-10) or coagulation (D-dimer, antithrombin and VWF) cascades, suggesting complex vascular pathobiology of the disease. Furthermore, we observe commonality with established cancer biomarkers (ACE2, IL-6, IL-4 and IL-2) as well as biomarkers for metabolic syndrome and diabetes (CRP, NLR and LDL). We explore these trends as we put forth a COVID-19 biomarker resource (https://data.oncomx.org/covid19) that will help researchers and diagnosticians alike.  © 2021 The Author(s). Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com.},
	publication_stage = {Final}
}

@ARTICLE{Waters2022,
	author = {Waters, Austin R. and Bybee, Sara and Warner, Echo L. and Kaddas, Heydon K. and Kent, Erin E. and Kirchhoff, Anne C.},
	title = {Financial Burden and Mental Health Among LGBTQIA+ Adolescent and Young Adult Cancer Survivors During the COVID-19 Pandemic},
	year = {2022},
	journal = {Frontiers in Oncology},
	volume = {12},
	doi = {10.3389/fonc.2022.832635},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133534188&doi=10.3389%2ffonc.2022.832635&partnerID=40&md5=4e4f37580f671f593bf5b43c5507c8d7},
	abstract = {Background: In the United States, the cost of cancer treatment can lead to severe financial burden for cancer survivors. The economic impacts of the COVID-19 pandemic compound cancer survivors’ financial challenges. Financial burden may be particularly challenging for lesbian, gay, bisexual, transgender, queer, intersex, asexual and other sexual and gender minority (LGBTQIA+) survivors. LGBTQIA+ survivors who are adolescent and young adults (AYA) may face elevated financial burden due to multiple, intersecting identities. Methods: An explanatory sequential mixed methods design was applied, beginning with a survey of AYA cancer survivors in the Mountain West region of the United States. Survey measures included demographics, COVID-19 impacts, the COmprehensive Score for financial Toxicity (COST), Perceived Stress Scale-4 (PSS-4), and PROMIS anxiety and depression scales. Two-way t-tests were used to analyze differences in outcomes between LGBTQIA+ and non-LGBTQIA+ AYAs. All LGBTQIA+ survey participants were invited to complete an interview, and those who agreed participated in descriptive interviews about financial burden due to cancer, COVID-19, and LGBTQIA+ identity. Interviews were audio recorded, transcribed, and analyzed using Dedoose. Results: Survey participants (N=325) were LGBTQIA+ (n=29, 8.9%), primarily female (n= 197, 60.6%), non-Hispanic White (n= 267, 82.2%), and received treatment during COVID-19 (n= 174, 54.0%). LGBTQIA+ interview participants (n=9, 100%) identified as a sexual minority and (n=2, 22.2%) identified as a gender minority. Most were non-Hispanic White (n=6, 66.7%) and had received treatment during COVID-19 (n=7, 77.8%). Statistical analyses revealed that LGBTQIA+ AYAs reported significantly worse COST scores than non-LGBTQIA+ AYAs (p=0.002). LGBTQIA+ AYAs also reported significantly higher PSS-4 (p=0.001), PROMIS anxiety (p=0.002) and depression scores (p<0.001) than non-LGBTQIA+ AYAs, reflecting worse mental health outcomes. High costs of cancer treatment and employment disruptions due to COVID-19 contributed to substantial financial stress, which exacerbated existing mental health challenges and introduced new ones. Conclusions: LGBTQIA+ AYA survivors reported substantial financial burden and psychological distress exacerbated by cancer, the COVID-19 pandemic, and LGBTQIA+ stigma. Given their multiple intersecting identities and potential for marginalization, LGBTQIA+ AYA survivors deserve prioritization in research to reduce financial burden and poor mental health. Copyright © 2022 Waters, Bybee, Warner, Kaddas, Kent and Kirchhoff.},
	publication_stage = {Final}
}

@ARTICLE{Sivarajkumar2024313,
	author = {Sivarajkumar, Sonish and Mohammad, Haneef Ahamed and Oniani, David and Roberts, Kirk and Hersh, William and Liu, Hongfang and He, Daqing and Visweswaran, Shyam and Wang, Yanshan},
	title = {Clinical Information Retrieval: A Literature Review},
	year = {2024},
	journal = {Journal of Healthcare Informatics Research},
	volume = {8},
	number = {2},
	pages = {313 – 352},
	doi = {10.1007/s41666-024-00159-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183373523&doi=10.1007%2fs41666-024-00159-4&partnerID=40&md5=29da88f609a1fa2f97dbda604d7bf1ef},
	abstract = {Clinical information retrieval (IR) plays a vital role in modern healthcare by facilitating efficient access and analysis of medical literature for clinicians and researchers. This scoping review aims to offer a comprehensive overview of the current state of clinical IR research and identify gaps and potential opportunities for future studies in this field. The main objective was to assess and analyze the existing literature on clinical IR, focusing on the methods, techniques, and tools employed for effective retrieval and analysis of medical information. Adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, we conducted an extensive search across databases such as Ovid Embase, Ovid Medline, Scopus, ACM Digital Library, IEEE Xplore, and Web of Science, covering publications from January 1, 2010, to January 4, 2023. The rigorous screening process led to the inclusion of 184 papers in our review. Our findings provide a detailed analysis of the clinical IR research landscape, covering aspects like publication trends, data sources, methodologies, evaluation metrics, and applications. The review identifies key research gaps in clinical IR methods such as indexing, ranking, and query expansion, offering insights and opportunities for future studies in clinical IR, thus serving as a guiding framework for upcoming research efforts in this rapidly evolving field. The study also underscores an imperative for innovative research on advanced clinical IR systems capable of fast semantic vector search and adoption of neural IR techniques for effective retrieval of information from unstructured electronic health records (EHRs). © The Author(s), under exclusive licence to Springer Nature Switzerland AG 2024.},
	publication_stage = {Final}
}

@ARTICLE{Jena2024,
	author = {Jena, Siddhartha G. and Verma, Archit and Engelhardt, Barbara E.},
	title = {Answering open questions in biology using spatial genomics and structured methods},
	year = {2024},
	journal = {BMC Bioinformatics},
	volume = {25},
	number = {1},
	doi = {10.1186/s12859-024-05912-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203071956&doi=10.1186%2fs12859-024-05912-5&partnerID=40&md5=3c3704fe54a0bbb191bd146ada0923ae},
	abstract = {Genomics methods have uncovered patterns in a range of biological systems, but obscure important aspects of cell behavior: the shapes, relative locations, movement, and interactions of cells in space. Spatial technologies that collect genomic or epigenomic data while preserving spatial information have begun to overcome these limitations. These new data promise a deeper understanding of the factors that affect cellular behavior, and in particular the ability to directly test existing theories about cell state and variation in the context of morphology, location, motility, and signaling that could not be tested before. Rapid advancements in resolution, ease-of-use, and scale of spatial genomics technologies to address these questions also require an updated toolkit of statistical methods with which to interrogate these data. We present a framework to respond to this new avenue of research: four open biological questions that can now be answered using spatial genomics data paired with methods for analysis. We outline spatial data modalities for each open question that may yield specific insights, discuss how conflicting theories may be tested by comparing the data to conceptual models of biological behavior, and highlight statistical and machine learning-based tools that may prove particularly helpful to recover biological understanding. © The Author(s) 2024.},
	publication_stage = {Final}
}

@CONFERENCE{Yang20241250,
	author = {Yang, Hang and Xiao, Liang and Zhu, Rujun and Liu, Ziji and Chen, Jianxia},
	title = {Interlinking Clinical Guidelines via Mining Medical Literature Knowledge for Multi-Morbidity Decision-Making},
	year = {2024},
	journal = {Proceedings - 2024 IEEE 48th Annual Computers, Software, and Applications Conference, COMPSAC 2024},
	pages = {1250 – 1255},
	doi = {10.1109/COMPSAC61105.2024.00165},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204063764&doi=10.1109%2fCOMPSAC61105.2024.00165&partnerID=40&md5=6827cf68f762f1d6baa5971b783670cb},
	abstract = {Independently developed clinical guidelines present a systematic challenge in managing patients with multi-morbidity in a consistent and integrated manner. Existing approaches mainly focus on combining multiple guidelines and lack approaches that combine with additional medical resources. The correlations and conflicts between treatment plans in the management of multi-morbidity are well-documented in medical literature but are less explored in the Clinical Decision Support line of research. In this paper, we propose a literature-based guideline interlinking method to address these challenges through the integration of clinical guidelines and the harmonization of conflicting recommendations, thereby providing a more holistic and efficient way to manage patients with multi-morbidity conditions. This method employs an ontology model and knowledge graph technology to represent and analyze the complexity and interrelations of diseases, with the aim of transcending the limitations of traditional single disease guidelines and providing a holistic and integrated framework for multi-morbidity management. The objective is to construct a multi-morbidity knowledge graph by correlating medical literature with clinical guidelines and to provide optimal decision support for patients with multi-morbidity complications in a clinical decision support system (CDSS).  © 2024 IEEE.},
	publication_stage = {Final}
}

@ARTICLE{Penberthy2022287,
	author = {Penberthy, Lynne T. and Rivera, Donna R. and Lund, Jennifer L. and Bruno, Melissa A. and Meyer, Anne-Marie},
	title = {An overview of real-world data sources for oncology and considerations for research},
	year = {2022},
	journal = {CA Cancer Journal for Clinicians},
	volume = {72},
	number = {3},
	pages = {287 – 300},
	doi = {10.3322/caac.21714},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122108934&doi=10.3322%2fcaac.21714&partnerID=40&md5=603f8c453e12848900515292d4a5918c},
	abstract = {Generating evidence on the use, effectiveness, and safety of new cancer therapies is a priority for researchers, health care providers, payers, and regulators given the rapid pace of change in cancer diagnosis and treatments. The use of real-world data (RWD) is integral to understanding the utilization patterns and outcomes of these new treatments among patients with cancer who are treated in clinical practice and community settings. An initial step in the use of RWD is careful study design to assess the suitability of an RWD source. This pivotal process can be guided by using a conceptual model that encourages predesign conceptualization. The primary types of RWD included are electronic health records, administrative claims data, cancer registries, and specialty data providers and networks. Careful consideration of each data type is necessary because they are collected for a specific purpose, capturing a set of data elements within a certain population for that purpose, and they vary by population coverage and longitudinality. In this review, the authors provide a high-level assessment of the strengths and limitations of each data category to inform data source selection appropriate to the study question. Overall, the development and accessibility of RWD sources for cancer research are rapidly increasing, and the use of these data requires careful consideration of composition and utility to assess important questions in understanding the use and effectiveness of new therapies. © 2021 The Authors. CA: A Cancer Journal for Clinicians published by Wiley Periodicals LLC on behalf of American Cancer Society. This article has been contributed to by US Government employees and their work is in the public domain in the USA.},
	publication_stage = {Final}
}

@BOOK{Fernandes2023157,
	author = {Fernandes, Juliana and Oliveira, Lucas and Graciano Neto, Valdemar Vicente and dos Santos, Rodrigo Pereira and Angarita, Rafael and Guehis, Sonia and Cardinale, Yudith},
	title = {PIS: Interoperability and Decision-Making Process-A Review},
	year = {2023},
	journal = {The Evolution of Pervasive Information Systems},
	pages = {157 – 190},
	doi = {10.1007/978-3-031-18176-4_7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160132801&doi=10.1007%2f978-3-031-18176-4_7&partnerID=40&md5=013a4e93a7a8454916f4551cbdc9300c},
	abstract = {Pervasive Information Systems (PIS) can be seen as Information Systems (IS) deployed everywhere, going beyond the traditional frontiers of organizations. In this context, they can be considered as Systems-of-Information Systems (SoIS), which are an emerging classification of arrangements of managerial and operationally independent IS. Despite the evident importance and recurrent need for interoperability among IS, the management of interoperability links and their adjustment at a suitable level is still challenging, particularly considering the independence of IS. Given that context, we aim to bring the IS community the discussion about the importance of technical, human, and organizational factors beyond just integration among systems, around interoperability in the domain of PIS, seen as SoIS, to support their decision-making processes. We present these factors as potential issues to explain how practices around interoperability need a synergy of efforts beyond technical decisions and propose some guidelines for the design of interoperability links in PIS, seen as SoIS. We report results of a deep study about factors that potentially influence the establishment of interoperability links among IS to form PIS, seen as SoIS, and support their decision-making processes. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2023, corrected publication 2023.},
	publication_stage = {Final}
}

@ARTICLE{Epizitone20234015,
	author = {Epizitone, Ayogeboh and Moyane, Smangele Pretty and Agbehadji, Israel Edem},
	title = {A Data-Driven Paradigm for a Resilient and Sustainable Integrated Health Information Systems for Health Care Applications},
	year = {2023},
	journal = {Journal of Multidisciplinary Healthcare},
	volume = {16},
	pages = {4015 – 4025},
	doi = {10.2147/JMDH.S433299},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179329550&doi=10.2147%2fJMDH.S433299&partnerID=40&md5=07e85262b32bcc1c110a96370d7576c6},
	abstract = {Introduction: Many transformations and uncertainties, such as the fourth industrial revolution and pandemics, have propelled healthcare acceptance and deployment of health information systems (HIS). External and internal determinants aligning with the global course influence their deployments. At the epic is digitalization, which generates endless data that has permeated healthcare. The continuous proliferation of complex and dynamic healthcare data is the digitalization frontier in healthcare that necessitates attention. Objective: This study explores the existing body of information on HIS for healthcare through the data lens to present a data-driven paradigm for healthcare augmentation paramount to attaining a sustainable and resilient HIS. Method: Preferred Reporting Items for Systematic Reviews and Meta-Analyses: PRISMA-compliant in-depth literature review was conducted systematically to synthesize and analyze the literature content to ascertain the value disposition of HIS data in healthcare delivery. Results: This study details the aspects of a data-driven paradigm for robust and sustainable HIS for health care applications. Data source, data action and decisions, data sciences techniques, serialization of data sciences techniques in the HIS, and data insight implementation and application are data-driven features expounded. These are essential data-driven paradigm building blocks that need iteration to succeed. Discussions: Existing literature considers insurgent data in healthcare challenging, disruptive, and potentially revolutionary. This view echoes the current healthcare quandary of good and bad data availability. Thus, data-driven insights are essential for building a resilient and sustainable HIS. People, technology, and tasks dominated prior HIS frameworks, with few data-centric facets. Improving healthcare and the HIS requires identifying and integrating crucial data elements. Conclusion: The paper presented a data-driven paradigm for a resilient and sustainable HIS. The findings show that data-driven track and components are essential to improve healthcare using data analytics insights. It provides an integrated footing for data analytics to support and effectively assist health care delivery. © 2023 Epizitone et al.},
	publication_stage = {Final}
}

@ARTICLE{Qiu20241533,
	author = {Qiu, Rui and Tu, Yamei and Wang, Yu-Shuen and Yen, Po-Yin and Shen, Han-Wei},
	title = {DocFlow: A Visual Analytics System for Question-Based Document Retrieval and Categorization},
	year = {2024},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	volume = {30},
	number = {2},
	pages = {1533 – 1548},
	doi = {10.1109/TVCG.2022.3219762},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141635500&doi=10.1109%2fTVCG.2022.3219762&partnerID=40&md5=0206cebc5f4832c313a8c166c724ff81},
	abstract = {A systematic review (SR) is essential with up-to-date research evidence to support clinical decisions and practices. However, the growing literature volume makes it challenging for SR reviewers and clinicians to discover useful information efficiently. Many human-in-the-loop information retrieval approaches (HIR) have been proposed to rank documents semantically similar to users' queries and provide interactive visualizations to facilitate document retrieval. Given that the queries are mainly composed of keywords and keyphrases retrieving documents that are semantically similar to a query does not necessarily respond to the clinician's need. Clinicians still have to review many documents to find the solution. The problem motivates us to develop a visual analytics system, DocFlow, to facilitate information-seeking. One of the features of our DocFlow is accepting natural language questions. The detailed description enables retrieving documents that can answer users' questions. Additionally, clinicians often categorize documents based on their backgrounds and with different purposes (e.g., populations, treatments). Since the criteria are unknown and cannot be pre-defined in advance, existing methods can only achieve categorization by considering the entire information in documents. In contrast, by locating answers in each document, our DocFlow can intelligently categorize documents based on users' questions. The second feature of our DocFlow is a flexible interface where users can arrange a sequence of questions to customize their rules for document retrieval and categorization. The two features of this visual analytics system support a flexible information-seeking process. The case studies and the feedback from domain experts demonstrate the usefulness and effectiveness of our DocFlow. © 1995-2012 IEEE.},
	publication_stage = {Final}
}

@ARTICLE{Zhao2021,
	author = {Zhao, Sendong and Su, Chang and Lu, Zhiyong and Wang, Fei},
	title = {Recent advances in biomedical literature mining},
	year = {2021},
	journal = {Briefings in Bioinformatics},
	volume = {22},
	number = {3},
	doi = {10.1093/bib/bbaa057},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107088750&doi=10.1093%2fbib%2fbbaa057&partnerID=40&md5=5642ad986ee2846d9cf9654b4ef3a212},
	abstract = {The recent years have witnessed a rapid increase in the number of scientific articles in biomedical domain. These literature are mostly available and readily accessible in electronic format. The domain knowledge hidden in them is critical for biomedical research and applications, which makes biomedical literature mining (BLM) techniques highly demanding. Numerous efforts have been made on this topic from both biomedical informatics (BMI) and computer science (CS) communities. The BMI community focuses more on the concrete application problems and thus prefer more interpretable and descriptive methods, while the CS community chases more on superior performance and generalization ability, thus more sophisticated and universal models are developed. The goal of this paper is to provide a review of the recent advances in BLM from both communities and inspire new research directions.  © 2020 The Author(s) 2020. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oup.com.},
	publication_stage = {Final}
}

@ARTICLE{Zhang20211184,
	author = {Zhang, Chihao and Zhang, Shihua},
	title = {Bayesian Joint Matrix Decomposition for Data Integration with Heterogeneous Noise},
	year = {2021},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume = {43},
	number = {4},
	pages = {1184 – 1196},
	doi = {10.1109/TPAMI.2019.2946370},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073512245&doi=10.1109%2fTPAMI.2019.2946370&partnerID=40&md5=26417578f40843fbdfb3652d5feea13d},
	abstract = {Matrix decomposition is a popular and fundamental approach in machine learning and data mining. It has been successfully applied into various fields. Most matrix decomposition methods focus on decomposing a data matrix from one single source. However, it is common that data are from different sources with heterogeneous noise. A few of the matrix decomposition methods have been extended for such multi-view data integration and pattern discovery while only a few methods were designed to consider the heterogeneity of noise in such multi-view data for data integration explicitly. To this end, in this article, we propose a joint matrix decomposition framework (BJMD), which models the heterogeneity of noise by the Gaussian distribution in a Bayesian framework. We develop two algorithms to solve this model: one is a variational Bayesian inference algorithm, which makes full use of the posterior distribution; and another is a maximum a posterior algorithm, which is more scalable and can be easily paralleled. Extensive experiments on synthetic and real-world datasets demonstrate that BJMD is superior or competitive to the state-of-The-Art methods. © 1979-2012 IEEE.},
	publication_stage = {Final}
}

@ARTICLE{Hu2021,
	author = {Hu, Jiang and Liu, Boji and Xie, Weihao and Zhu, Jinhan and Yu, Xiaoli and Gu, Huikuan and Wang, Mingli and Wang, Yixuan and Qi, ZhenYu},
	title = {Quantitative Comparison of Knowledge-Based and Manual Intensity Modulated Radiation Therapy Planning for Nasopharyngeal Carcinoma},
	year = {2021},
	journal = {Frontiers in Oncology},
	volume = {10},
	doi = {10.3389/fonc.2020.551763},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099726996&doi=10.3389%2ffonc.2020.551763&partnerID=40&md5=8a0bd7e597756149d3a490db43921b84},
	abstract = {Background and purpose: To validate the feasibility and efficiency of a fully automatic knowledge-based planning (KBP) method for nasopharyngeal carcinoma (NPC) cases, with special attention to the possible way that the success rate of auto-planning can be improved. Methods and materials: A knowledge-based dose volume histogram (DVH) prediction model was developed based on 99 formerly treated NPC patients, by means of which the optimization objectives and the corresponding priorities for intensity modulation radiation therapy (IMRT) planning were automatically generated for each head and neck organ at risk (OAR). The automatic KBP method was thus evaluated in 17 new NPC cases with comparison to manual plans (MP) and expert plans (EXP) in terms of target dose coverage, conformity index (CI), homogeneity index (HI), and normal tissue protection. To quantify the plan quality, a metric was applied for plan evaluation. The variation in the plan quality and time consumption among planners was also investigated. Results: With comparable target dose distributions, the KBP method achieved a significant dose reduction in critical organs such as the optic chiasm (p<0.001), optic nerve (p=0.021), and temporal lobe (p<0.001), but failed to spare the spinal cord (p<0.001) compared with MPs and EXPs. The overall plan quality evaluation gave mean scores of 144.59±11.48, 142.71±15.18, and 144.82±15.17, respectively, for KBPs, MPs, and EXPs (p=0.259). A total of 15 out of 17 KBPs (i.e., 88.24%) were approved by our physician as clinically acceptable. Conclusion: The automatic KBP method using the DVH prediction model provided a possible way to generate clinically acceptable plans in a short time for NPC patients. © Copyright © 2021 Hu, Liu, Xie, Zhu, Yu, Gu, Wang, Wang and Qi.},
	publication_stage = {Final}
}

@ARTICLE{Blanchard20222796,
	author = {Blanchard, Andrew E. and Gao, Shang and Yoon, Hong-Jun and Christian, J. Blair and Durbin, Eric B. and Wu, Xiao-Cheng and Stroup, Antoinette and Doherty, Jennifer and Schwartz, Stephen M. and Wiggins, Charles and Coyle, Linda and Penberthy, Lynne and Tourassi, Georgia D.},
	title = {A Keyword-Enhanced Approach to Handle Class Imbalance in Clinical Text Classification},
	year = {2022},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	volume = {26},
	number = {6},
	pages = {2796 – 2803},
	doi = {10.1109/JBHI.2022.3141976},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123347177&doi=10.1109%2fJBHI.2022.3141976&partnerID=40&md5=5c9747fc41914657eb30d031b8a6e5cd},
	abstract = {Recent applications ofdeep learning have shown promising results for classifying unstructured text in the healthcare domain. However, the reliability of models in production settings has been hindered by imbalanced data sets in which a small subset of the classes dominate. In the absence of adequate training data, rare classes necessitate additional model constraints for robust performance. Here, we present a strategy for incorporating short sequences of text (i.e. keywords) into training to boost model accuracy on rare classes. In our approach, we assemble a set of keywords, including short phrases, associated with each class. The keywords are then used as additional data during each batch of model training, resulting in a training loss that has contributions from both raw data and keywords. We evaluate our approach on classification of cancer pathology reports, which shows a substantial increase in model performance for rare classes. Furthermore, we analyze the impact of keywords on model output probabilities for bigrams, providing a straightforward method to identify model difficulties for limited training data.  © 2013 IEEE.},
	publication_stage = {Final}
}

@BOOK{Cannataro20221,
	author = {Cannataro, Mario and Guzzi, Pietro Hiram and Agapito, Giuseppe and Zucco, Chiara and Milano, Marianna},
	title = {Artificial Intelligence in Bioinformatics: From Omics Analysis to Deep Learning and Network Mining},
	year = {2022},
	journal = {Artificial Intelligence in Bioinformatics: From Omics Analysis to Deep Learning and Network Mining},
	pages = {1 – 247},
	doi = {10.1016/C2019-0-04920-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138931915&doi=10.1016%2fC2019-0-04920-5&partnerID=40&md5=3a9ab174ca80376cb09eac142808cfae},
	abstract = {Artificial Intelligence in Bioinformatics: From Omics Analysis to Deep Learning and Network Mining reviews the main applications of the topic, from omics analysis to deep learning and network mining. The book includes a rigorous introduction on bioinformatics, also reviewing how methods are incorporated in tasks and processes. In addition, it presents methods and theory, including content for emergent fields such as Sentiment Analysis and Network Alignment.  Other sections survey how Artificial Intelligence is exploited in bioinformatics applications, including sequence analysis, structure analysis, functional analysis, protein classification, omics analysis, biomarker discovery, integrative bioinformatics, protein interaction analysis, metabolic networks analysis, and much more. © 2022 Elsevier Inc. All rights reserved.},
	publication_stage = {Final}
}

@BOOK{Kaplan20221,
	author = {Kaplan, Robert M. and Beatty, Alexandra S.},
	title = {Ontologies in the behavioral sciences},
	year = {2022},
	journal = {Ontologies in the Behavioral Sciences},
	pages = {1 – 149},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138897210&partnerID=40&md5=d1fbb4765bc1dc29746069b598e3616c},
	abstract = {New research in psychology, neuroscience, cognitive science, and other fields is published every day, but the gap between what is known and the capacity to act on that knowledge has never been larger. Scholars and nonscholars alike face the problem of how to organize knowledge and to integrate new observations with what is already known. Ontologies - formal, explicit specifications of the meaning of the concepts and entities that scientists study - provide a way to address these and other challenges, and thus to accelerate progress in behavioral research and its application. © 2022 by the National Academy of Sciences. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Cavalleri2024,
	author = {Cavalleri, Emanuele and Cabri, Alberto and Soto-Gomez, Mauricio and Bonfitto, Sara and Perlasca, Paolo and Gliozzo, Jessica and Callahan, Tiffany J. and Reese, Justin and Robinson, Peter N. and Casiraghi, Elena and Valentini, Giorgio and Mesiti, Marco},
	title = {An ontology-based knowledge graph for representing interactions involving RNA molecules},
	year = {2024},
	journal = {Scientific Data},
	volume = {11},
	number = {1},
	doi = {10.1038/s41597-024-03673-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201794498&doi=10.1038%2fs41597-024-03673-7&partnerID=40&md5=832f7dc714cd53969603bb3994dfcbdf},
	abstract = {The “RNA world” represents a novel frontier for the study of fundamental biological processes and human diseases and is paving the way for the development of new drugs tailored to each patient’s biomolecular characteristics. Although scientific data about coding and non-coding RNA molecules are constantly produced and available from public repositories, they are scattered across different databases and a centralized, uniform, and semantically consistent representation of the “RNA world” is still lacking. We propose RNA-KG, a knowledge graph (KG) encompassing biological knowledge about RNAs gathered from more than 60 public databases, integrating functional relationships with genes, proteins, and chemicals and ontologically grounded biomedical concepts. To develop RNA-KG, we first identified, pre-processed, and characterized each data source; next, we built a meta-graph that provides an ontological description of the KG by representing all the bio-molecular entities and medical concepts of interest in this domain, as well as the types of interactions connecting them. Finally, we leveraged an instance-based semantically abstracted knowledge model to specify the ontological alignment according to which RNA-KG was generated. RNA-KG can be downloaded in different formats and also queried by a SPARQL endpoint. A thorough topological analysis of the resulting heterogeneous graph provides further insights into the characteristics of the “RNA world”. RNA-KG can be both directly explored and visualized, and/or analyzed by applying computational methods to infer bio-medical knowledge from its heterogeneous nodes and edges. The resource can be easily updated with new experimental data, and specific views of the overall KG can be extracted according to the bio-medical problem to be studied. © The Author(s) 2024.},
	publication_stage = {Final}
}

@ARTICLE{Daponte2022,
	author = {Daponte, Vincenzo and Hayes, Catherine and Mariethoz, Julien and Lisacek, Frederique},
	title = {Dealing with the ambiguity of glycan substructure search},
	year = {2022},
	journal = {Molecules},
	volume = {27},
	number = {1},
	doi = {10.3390/molecules27010065},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121731672&doi=10.3390%2fmolecules27010065&partnerID=40&md5=ce5842d4f2e9c81f97933161e9749108},
	abstract = {The level of ambiguity in describing glycan structure has significantly increased with the upsurge of large-scale glycomics and glycoproteomics experiments. Consequently, an ontology-based model appears as an appropriate solution for navigating these data. However, navigation is not sufficient and the model should also enable advanced search and comparison. A new ontology with a tree logical structure is introduced to represent glycan structures irrespective of the precision of molecular details. The model heavily relies on the GlycoCT encoding of glycan structures. Its imple-mentation in the GlySTreeM knowledge base was validated with GlyConnect data and benchmarked with the Glycowork library. GlySTreeM is shown to be fast, consistent, reliable and more flexible than existing solutions for matching parts of or whole glycan structures. The model is also well suited for painless future expansion. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	publication_stage = {Final}
}

@ARTICLE{Xu2022,
	author = {Xu, Jiabao and Xi, Xuefeng and Chen, Jie and Sheng, Victor S. and Ma, Jieming and Cui, Zhiming},
	title = {A Survey of Deep Learning for Electronic Health Records},
	year = {2022},
	journal = {Applied Sciences (Switzerland)},
	volume = {12},
	number = {22},
	doi = {10.3390/app122211709},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142531397&doi=10.3390%2fapp122211709&partnerID=40&md5=cc8f858df07579b60019c5fce44c2cb1},
	abstract = {Medical data is an important part of modern medicine. However, with the rapid increase in the amount of data, it has become hard to use this data effectively. The development of machine learning, such as feature engineering, enables researchers to capture and extract valuable information from medical data. Many deep learning methods are conducted to handle various subtasks of EHR from the view of information extraction and representation learning. This survey designs a taxonomy to summarize and introduce the existing deep learning-based methods on EHR, which could be divided into four types (Information Extraction, Representation Learning, Medical Prediction and Privacy Protection). Furthermore, we summarize the most recognized EHR datasets, MIMIC, eICU, PCORnet, Open NHS, NCBI-disease and i2b2/n2c2 NLP Research Data Sets, and introduce the labeling scheme of these datasets. Furthermore, we provide an overview of deep learning models in various EHR applications. Finally, we conclude the challenges that EHR tasks face and identify avenues of future deep EHR research. © 2022 by the authors.},
	publication_stage = {Final}
}

@ARTICLE{Zhang2023,
	author = {Zhang, Shuang and Fan, Rui and Liu, Yuti and Chen, Shuang and Liu, Qiao and Zeng, Wanwen},
	title = {Applications of transformer-based language models in bioinformatics: A survey},
	year = {2023},
	journal = {Bioinformatics Advances},
	volume = {3},
	number = {1},
	doi = {10.1093/bioadv/vbad001},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150185329&doi=10.1093%2fbioadv%2fvbad001&partnerID=40&md5=61b074ba4e36a202a7488911ea4d9feb},
	abstract = {The transformer-based language models, including vanilla transformer, BERT and GPT-3, have achieved revolutionary breakthroughs in the field of natural language processing (NLP). Since there are inherent similarities between various biological sequences and natural languages, the remarkable interpretability and adaptability of these models have prompted a new wave of their application in bioinformatics research. To provide a timely and comprehensive review, we introduce key developments of transformer-based language models by describing the detailed structure of transformers and summarize their contribution to a wide range of bioinformatics research from basic sequence analysis to drug discovery. While transformer-based applications in bioinformatics are diverse and multifaceted, we identify and discuss the common challenges, including heterogeneity of training data, computational expense and model interpretability, and opportunities in the context of bioinformatics research. We hope that the broader community of NLP researchers, bioinformaticians and biologists will be brought together to foster future research and development in transformer-based language models, and inspire novel bioinformatics applications that are unattainable by traditional methods. © 2023 The Author(s). Published by Oxford University Press.},
	publication_stage = {Final}
}

@BOOK{Dubey20241,
	author = {Dubey, Archi and Kishor Kumar Reddy, C. and Doss, Srinath and Hanafiah, Marlia Mohd},
	title = {Exploring the advancements and future directions of digital twins in healthcare 6.0},
	year = {2024},
	journal = {Exploring the Advancements and Future Directions of Digital Twins in Healthcare 6.0},
	pages = {1 – 468},
	doi = {10.4018/979-8-3693-5893-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201867418&doi=10.4018%2f979-8-3693-5893-1&partnerID=40&md5=e9215a261789122a265ce0defd960d49},
	abstract = {The healthcare industry is increasingly complex, demanding personalized treatments and efficient operational processes. Traditional research methods need help to keep pace with these demands, often leading to inefficiencies and suboptimal outcomes. Integrating digital twin technology presents a promising solution to these challenges, offering a virtual platform for modeling and simulating complex healthcare scenarios. However, the full potential of digital twins still needs to be explored mainly due to a lack of comprehensive guidance and practical insights for researchers and practitioners. Exploring the Advancements and Future Directions of Digital Twins in Healthcare 6.0 is not just a theoretical exploration. It is a practical guide that bridges the gap between theory and practice, offering real-world case studies, best practices, and insights into personalized medicine, real-time patient monitoring, and healthcare process optimization. By equipping you with the knowledge and tools needed to effectively integrate digital twins into your healthcare research and operations, this book is a valuable resource for researchers, academicians, medical practitioners, scientists, and students. As the healthcare industry transitions towards data-driven, patient-centric models, the insights presented in this book are invaluable for driving innovation and improving healthcare outcomes. By exploring the theoretical foundations and practical applications of digital twins in healthcare, this publication catalyzes transformative research, empowering readers to navigate the complexities of healthcare 6.0 with confidence and expertise. © 2024 by IGI Global. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Conceição2021,
	author = {Conceição, Sofia I. R. and Couto, Francisco M.},
	title = {Text mining for building biomedical networks using cancer as a case study},
	year = {2021},
	journal = {Biomolecules},
	volume = {11},
	number = {10},
	doi = {10.3390/biom11101430},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115931771&doi=10.3390%2fbiom11101430&partnerID=40&md5=d75d7fa28ed6cd4002e3225b884b696d},
	abstract = {In the assembly of biological networks it is important to provide reliable interactions in an effort to have the most possible accurate representation of real-life systems. Commonly, the data used to build a network comes from diverse high-throughput essays, however most of the interaction data is available through scientific literature. This has become a challenge with the notable increase in scientific literature being published, as it is hard for human curators to track all recent discoveries without using efficient tools to help them identify these interactions in an automatic way. This can be surpassed by using text mining approaches which are capable of ex-tracting knowledge from scientific documents. One of the most important tasks in text mining for biological network building is relation extraction, which identifies relations between the entities of interest. Many interaction databases already use text mining systems, and the development of these tools will lead to more reliable networks, as well as the possibility to personalize the networks by selecting the desired relations. This review will focus on different approaches of automatic information extraction from biomedical text that can be used to enhance existing networks or create new ones, such as deep learning state-of-the-art approaches, focusing on cancer disease as a case-study. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
	publication_stage = {Final}
}

@ARTICLE{Wang20211543,
	author = {Wang, Haiying and Pujos-Guillot, Estelle and Comte, Blandine and De Miranda, Joao Luis and Spiwok, Vojtech and Chorbev, Ivan and Castiglione, Filippo and Tieri, Paolo and Watterson, Steven and McAllister, Roisin and De Melo Malaquias, Tiago and Zanin, Massimiliano and Rai, Taranjit Singh and Zheng, Huiru},
	title = {Deep learning in systems medicine},
	year = {2021},
	journal = {Briefings in Bioinformatics},
	volume = {22},
	number = {2},
	pages = {1543 – 1559},
	doi = {10.1093/bib/bbaa237},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103474440&doi=10.1093%2fbib%2fbbaa237&partnerID=40&md5=2deea994e62577680b2e1570ebb56314},
	abstract = {Systems medicine (SM) has emerged as a powerful tool for studying the human body at the systems level with the aim of improving our understanding, prevention and treatment of complex diseases. Being able to automatically extract relevant features needed for a given task from high-dimensional, heterogeneous data, deep learning (DL) holds great promise in this endeavour. This review paper addresses the main developments of DL algorithms and a set of general topics where DL is decisive, namely, within the SM landscape. It discusses how DL can be applied to SM with an emphasis on the applications to predictive, preventive and precision medicine. Several key challenges have been highlighted including delivering clinical impact and improving interpretability. We used some prototypical examples to highlight the relevance and significance of the adoption of DL in SM, one of them is involving the creation of a model for personalized Parkinson's disease. The review offers valuable insights and informs the research in DL and SM.  © 2020 The Author(s) 2020. Published by Oxford University Press. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Mazein2023,
	author = {Mazein, Alexander and Acencio, Marcio Luis and Balaur, Irina and Rougny, Adrien and Welter, Danielle and Niarakis, Anna and Ramirez Ardila, Diana and Dogrusoz, Ugur and Gawron, Piotr and Satagopam, Venkata and Gu, Wei and Kremer, Andreas and Schneider, Reinhard and Ostaszewski, Marek},
	title = {A guide for developing comprehensive systems biology maps of disease mechanisms: planning, construction and maintenance},
	year = {2023},
	journal = {Frontiers in Bioinformatics},
	volume = {3},
	doi = {10.3389/fbinf.2023.1197310},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174817011&doi=10.3389%2ffbinf.2023.1197310&partnerID=40&md5=80d0bb9b3a2eb7e62f08a46f0075cb95},
	abstract = {As a conceptual model of disease mechanisms, a disease map integrates available knowledge and is applied for data interpretation, predictions and hypothesis generation. It is possible to model disease mechanisms on different levels of granularity and adjust the approach to the goals of a particular project. This rich environment together with requirements for high-quality network reconstruction makes it challenging for new curators and groups to be quickly introduced to the development methods. In this review, we offer a step-by-step guide for developing a disease map within its mainstream pipeline that involves using the CellDesigner tool for creating and editing diagrams and the MINERVA Platform for online visualisation and exploration. We also describe how the Neo4j graph database environment can be used for managing and querying efficiently such a resource. For assessing the interoperability and reproducibility we apply FAIR principles. Copyright © 2023 Mazein, Acencio, Balaur, Rougny, Welter, Niarakis, Ramirez Ardila, Dogrusoz, Gawron, Satagopam, Gu, Kremer, Schneider and Ostaszewski.},
	publication_stage = {Final}
}

@ARTICLE{Silva2022,
	author = {Silva, Marta Contreiras and Eugénio, Patrícia and Faria, Daniel and Pesquita, Catia},
	title = {Ontologies and Knowledge Graphs in Oncology Research},
	year = {2022},
	journal = {Cancers},
	volume = {14},
	number = {8},
	doi = {10.3390/cancers14081906},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127988176&doi=10.3390%2fcancers14081906&partnerID=40&md5=737f719f47def43558093a0ff1e9867b},
	abstract = {The complexity of cancer research stems from leaning on several biomedical disciplines for relevant sources of data, many of which are complex in their own right. A holistic view of cancer—which is critical for precision medicine approaches—hinges on integrating a variety of heterogeneous data sources under a cohesive knowledge model, a role which biomedical ontologies can fill. This study reviews the application of ontologies and knowledge graphs in cancer research. In total, our review encompasses 141 published works, which we categorized under 14 hierarchical categories according to their usage of ontologies and knowledge graphs. We also review the most commonly used ontologies and newly developed ones. Our review highlights the growing traction of ontologies in biomedical research in general, and cancer research in particular. Ontologies enable data accessibility, interoperability and integration, support data analysis, facilitate data interpretation and data mining, and more recently, with the emergence of the knowledge graph paradigm, support the application of Artificial Intelligence methods to unlock new knowledge from a holistic view of the available large volumes of heterogeneous data. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	publication_stage = {Final}
}

@BOOK{Hassan20241,
	author = {Hassan, Viana and Albattat, Ahmad Rasmi and Basheer, Shakeel},
	title = {Impact of AI and robotics on the medical tourism industry},
	year = {2024},
	journal = {Impact of AI and Robotics on the Medical Tourism Industry},
	pages = {1 – 361},
	doi = {10.4018/9798369322482},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191891191&doi=10.4018%2f9798369322482&partnerID=40&md5=9fcb955ee081a79e4f9721e93f9ff54c},
	abstract = {In the ancient world, health seekers traversed vast distances to pursue well-being. Fast forward to the 21st century, and the concept of medical tourism has evolved into a global industry worth billions of dollars. The burgeoning demand for quality healthcare has pushed traditional boundaries, necessitating a more sophisticated and interconnected healthcare ecosystem. Integrating Artificial Intelligence (AI) into medical practices empowers individuals to actively manage their health while providing healthcare practitioners with invaluable insights for tailored patient care. Impact of AI and Robotics on the Medical Tourism Industry delves into the transformative impact of AI and robotics on this dynamic sector. It explores how AI can encourage and assist patients in their health plans while augmenting healthcare practitioners' capabilities to utilize specific patient data to offer unique guidance. It dissects real-world examples, such as Cleveland Clinic Abu Dhabi's integration of robotic surgery, showcasing how these technologies revolutionize traditional medical practices. As AI takes center stage, the impact on medical tourism becomes palpable. Telemedicine and telehealth are examined as bridges to overcome geographical constraints, presenting a vision of a future where healthcare transcends borders. The efficacy of AI in marketing medical tourism destinations and its role in economic expansion are dissected, offering insights for professionals navigating this dynamic landscape. The book does not shy away from the multitude of complexities this integration is sure to bring. It delves into the ethical challenges and regulatory landscapes of these technologies, providing a compass for practitioners navigating this uncharted terrain. However, despite the challenges, the opportunities have the potential to create meaningful change. The chapters on competitive advantage and the perceptions of medical tourists unravel avenues for growth and improvement. This book is ideal for tourism industry professionals and academics. Medical travel agents, tour operators, healthcare professionals, and tourism specialists seeking a deeper comprehension of the role of AI and robotics in medical tourism will find this book invaluable. © 2024 by IGI Global. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Sreekrishna20235265,
	author = {Sreekrishna, M. and Prem Jacob, T.},
	title = {Quantitative feature extraction of unstructured data from GitLab BioAI pathology reports of cancer using an enhanced RPA NLP method},
	year = {2023},
	journal = {Journal of Intelligent and Fuzzy Systems},
	volume = {45},
	number = {4},
	pages = {5265 – 5276},
	doi = {10.3233/JIFS-231625},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174352987&doi=10.3233%2fJIFS-231625&partnerID=40&md5=dba3dd94a1b4eb6aa32e89c54315a557},
	abstract = {Unstructured pathology report plays a major role in definitive cancer diagnosis. Accessing or searching unstructured textual information from the clinical pathology reports is one of the major concerns in cancer healthcare sector to provide precise medicine, analysis of cancer outcomes, providing cancer care services, accurate measurement for future prediction, treatment history, and comparative future research work. An efficient methodology has to be introduced for to extract quantitative information from the unstructured cancer data. Integrating computational intelligence in Robotic Process Automation can be done to process this data and automate repetitive activities for evaluating patients clinical pathology report. RPA-based NLP BERT system is designed and evaluated to automatically extract information on these variables for the patients from pathology report. In order to detect tumour and outcomes from documented pathology reports, a supervised machine learning keyword based extraction algorithm was developed in which the pathology data are examined to extract keywords from 2087 reports with 1579 of data reports being processed for the development phase and 508 of data being used for evaluation. The precision recall and accuracy are calculated for organ specimens for cancer test as (0.984, 0.982, 0.9839), test methodology(0.986, 0.981,0.9956) and pathological result(0.986, 0.9938, 0.9795) were achieved. The feasibility of autonomously extracting pre-defined data from clinical narratives for cancer research were established in this work. The outcomes showed that our methodology was suitable for actual use in obtaining essential information from pathology reports.  © 2023 - IOS Press. All rights reserved.},
	publication_stage = {Final}
}

@CONFERENCE{Pereira2023106,
	author = {Pereira, Mariana C. and Sinigaglia, Marialva and Cazella, Silvio C.},
	title = {Proposal of a domain ontology for systemic view of oncopediatric patients; [Proposta de uma Ontologia de Domínio para a Visualização Sistêmica do Paciente Oncopediátrico]},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3564},
	pages = {106 – 113},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179611952&partnerID=40&md5=6a2f884d538ea26e077dd2ceb5e7711c},
	abstract = {The significant increase in data collection and generation in healthcare has forced healthcare institutions to invest in the development of consistent and standardized databases in order to provide continuous patient follow-up as well as efficient knowledge management. However, the distribution of information in different information systems and the different terminologies in the health area have presented themselves as challenges to the effective use of information systems. Thus, ontologies have been developed to facilitate data access and retrieval, allowing better understanding of terms applied in the health context. This paper presents research in progress that seeks to propose a domain ontology for the systemic view of oncopediatric patients. For the preliminary modeling of the ontology, Methodology 101 will be used, and other methods may be added as the ontology acquires complexity. Knowledge related to tumors and pediatric clinical follow-up will be included. The open source tool Protégé will be used to develop the ontology. As a result, it is expected a domain ontology that represents the knowledge related to the domain about oncopediatric patients, helping to understand how domain ontologies can help in the representation and manipulation of data about oncopediatric patients. © 2023 CEUR-WS. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Kosvyra2024,
	author = {Kosvyra, Alexandra and Filos, Dimitrios T. and Fotopoulos, Dimitris Th. and Tsave, Olga and Chouvarda, Ioanna},
	title = {Toward Ensuring Data Quality in Multi-Site Cancer Imaging Repositories},
	year = {2024},
	journal = {Information (Switzerland)},
	volume = {15},
	number = {9},
	doi = {10.3390/info15090533},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205245355&doi=10.3390%2finfo15090533&partnerID=40&md5=4b6ba1ef888c994b4dff7fac01945489},
	abstract = {Cancer remains a major global health challenge, affecting diverse populations across various demographics. Integrating Artificial Intelligence (AI) into clinical settings to enhance disease outcome prediction presents notable challenges. This study addresses the limitations of AI-driven cancer care due to low-quality datasets by proposing a comprehensive three-step methodology to ensure high data quality in large-scale cancer-imaging repositories. Our methodology encompasses (i) developing a Data Quality Conceptual Model with specific metrics for assessment, (ii) creating a detailed data-collection protocol and a rule set to ensure data homogeneity and proper integration of multi-source data, and (iii) implementing a Data Integration Quality Check Tool (DIQCT) to verify adherence to quality requirements and suggest corrective actions. These steps are designed to mitigate biases, enhance data integrity, and ensure that integrated data meets high-quality standards. We applied this methodology within the INCISIVE project, an EU-funded initiative aimed at a pan-European cancer-imaging repository. The use-case demonstrated the effectiveness of our approach in defining quality rules and assessing compliance, resulting in improved data integration and higher data quality. The proposed methodology can assist the deployment of big data centralized or distributed repositories with data from diverse data sources, thus facilitating the development of AI tools. © 2024 by the authors.},
	publication_stage = {Final}
}

@ARTICLE{Campistol2022273,
	author = {Campistol, Míriam and Morote, Juan and Regis, Lucas and Celma, Ana and Planas, Jacques and Trilla, Enrique},
	title = {Proclarix, A New Biomarker for the Diagnosis of Clinically Significant Prostate Cancer: A Systematic Review},
	year = {2022},
	journal = {Molecular Diagnosis and Therapy},
	volume = {26},
	number = {3},
	pages = {273 – 281},
	doi = {10.1007/s40291-022-00584-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128890050&doi=10.1007%2fs40291-022-00584-4&partnerID=40&md5=164794c4386ccbcea1da1dc2f5edc099},
	abstract = {Introduction: Multiparametric magnetic resonance imaging (mpMRI) has improved the early detection of clinically significant prostate cancer (csPCa). However, an appropriate selection of men for mpMRI or prostate biopsy is still challenging, which is why new biomarkers or predictive models are recommended to determine those patients who will benefit from prostate biopsy. Proclarix is a new test that provides the risk of csPCa based on thrombospondin-1 (THBS1), cathepsin D (CTSD), prostate-specific antigen (PSA), and percentage of free PSA (%fPSA), as well as age. This systematic review analyzes the current clinical status of Proclarix and future development. Evidence Acquisition: A systematic review of the literature was carried out by two independent reviewers. The Medical Subject Heading (MeSH) terms ‘prostate’, ‘thrombospondin-1’, ‘cathepsin-D’ and ‘Proclarix’ were used. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and the Population, Intervention, Comparison and Outcomes (PICO) selection criteria were followed. Finally, four articles analyzed the clinical usefulness of Proclarix. Evidence Synthesis: Proclarix has been developed in men with PSA levels between 2 and 10 ng/mL, normal digital rectal examination (DRE), and prostate volume (PV)​ ≥ 35 cm3. Proclarix is associated with the PCa grade group and is more effective than %fPSA in detecting csPCa. Two studies analyzed the efficacy of Proclarix in men undergoing guided and systematic biopsies, obtaining similar results to PSA density. Conclusion: Initial studies have shown the potential benefit of Proclarix in patients with specific characteristics. Future studies are needed to verify the clinical usefulness of Proclarix in men with suspected PCa before and after mpMRI. © 2022, The Author(s), under exclusive licence to Springer Nature Switzerland AG.},
	publication_stage = {Final}
}

@ARTICLE{Brady202464,
	author = {Brady, Drew and Al-Mubaid, Hisham},
	title = {Disease Similarity and Disease Clustering},
	year = {2024},
	journal = {Communications in Computer and Information Science},
	volume = {2158 CCIS},
	pages = {64 – 77},
	doi = {10.1007/978-3-031-67871-4_5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202609573&doi=10.1007%2f978-3-031-67871-4_5&partnerID=40&md5=52383cd366a726d2a1321f2af8472f32},
	abstract = {In bioinformatics research, identifying the common characteristics of human diseases is crucial for understanding disease-disease associations. In this direction, disease similarity and disease clustering methodologies provide many insights by identifying underlying patterns, disease mechanisms, and other factors associated with disease causation and progression. Furthermore, the information and outcomes from these insights can inform drug repurposing, precision medicine, and other medical and healthcare applications. However, there are limitations in computational techniques, data integration, and disease knowledge that prevent or hinder a comprehensive understanding of the relationships between diseases. With our study, we aim to assist ongoing research by providing an integrated overview of disease similarity and disease clustering. We will examine the biomedical databases, disease databases and vocabularies, and disease-related information commonly used for this research. Then, we analyze some of the computational methods and integrated approaches used to quantify disease-disease associations. Additionally, we provide insights that can be derived from disease similarity and clustering analysis. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.},
	publication_stage = {Final}
}

@BOOK{Schoenherr20221,
	author = {Schoenherr, Jordan Richard},
	title = {Ethical artificial intelligence from popular to cognitive science: Trust in the age of entanglement},
	year = {2022},
	journal = {Ethical Artificial Intelligence from Popular to Cognitive Science: Trust in the Age of Entanglement},
	pages = {1 – 212},
	doi = {10.4324/9781003143284},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134904219&doi=10.4324%2f9781003143284&partnerID=40&md5=b246e225ed9dbaf07ae2a3a2162d0170},
	abstract = {This book offers a unique interdisciplinary perspective on the ethics of 'artificial intelligence' - autonomous, intelligent, (and connected) systems, or AISs, applying principles of social cognition to understand the social and ethical issues associated with the creation, adoption, and implementation of AISs. © 2022 Taylor and Francis. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Wilson20231291,
	author = {Wilson, Lydia J. and Kiffer, Frederico C. and Berrios, Daniel C. and Bryce-Atkinson, Abigail and Costes, Sylvain V. and Gevaert, Olivier and Matarèse, Bruno F. E. and Miller, Jack and Mukherjee, Pritam and Peach, Kristen and Schofield, Paul N. and Slater, Luke T. and Langen, Britta},
	title = {Machine intelligence for radiation science: summary of the Radiation Research Society 67th annual meeting symposium},
	year = {2023},
	journal = {International Journal of Radiation Biology},
	volume = {99},
	number = {8},
	pages = {1291 – 1300},
	doi = {10.1080/09553002.2023.2173823},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147670509&doi=10.1080%2f09553002.2023.2173823&partnerID=40&md5=a1923dd7d5f87bcb3fbbf28428f12d0e},
	abstract = {The era of high-throughput techniques created big data in the medical field and research disciplines. Machine intelligence (MI) approaches can overcome critical limitations on how those large-scale data sets are processed, analyzed, and interpreted. The 67th Annual Meeting of the Radiation Research Society featured a symposium on MI approaches to highlight recent advancements in the radiation sciences and their clinical applications. This article summarizes three of those presentations regarding recent developments for metadata processing and ontological formalization, data mining for radiation outcomes in pediatric oncology, and imaging in lung cancer. © 2023 The Author(s). Published with license by Taylor & Francis Group, LLC.},
	publication_stage = {Final}
}

@ARTICLE{Eckenrode2023,
	author = {Eckenrode, Kelly B. and Righelli, Dario and Ramos, Marcel and Argelaguet, Ricard and Vanderaa, Christophe and Geistlinger, Ludwig and Culhane, Aedin C. and Gatto, Laurent and Carey, Vincent and Morgan, Martin and Risso, Davide and Waldron, Levi},
	title = {Curated single cell multimodal landmark datasets for R/Bioconductor},
	year = {2023},
	journal = {PLoS Computational Biology},
	volume = {19},
	number = {8 August},
	doi = {10.1371/journal.pcbi.1011324},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170717169&doi=10.1371%2fjournal.pcbi.1011324&partnerID=40&md5=1f0df437eac045e2d122b6e952133756},
	abstract = {Background The majority of high-throughput single-cell molecular profiling methods quantify RNA expression; however, recent multimodal profiling methods add simultaneous measurement of genomic, proteomic, epigenetic, and/or spatial information on the same cells. The development of new statistical and computational methods in Bioconductor for such data will be facilitated by easy availability of landmark datasets using standard data classes. Results We collected, processed, and packaged publicly available landmark datasets from important single-cell multimodal protocols, including CITE-Seq, ECCITE-Seq, SCoPE2, scNMT, 10X Multiome, seqFISH, and G&T. We integrate data modalities via the MultiAssayExperiment Bioconductor class, document and re-distribute datasets as the SingleCellMultiModal package in Bioconductor’s Cloud-based ExperimentHub. The result is single-command actualization of landmark datasets from seven single-cell multimodal data generation technologies, without need for further data processing or wrangling in order to analyze and develop methods within Bioconductor’s ecosystem of hundreds of packages for single-cell and multimodal data. Conclusions We provide two examples of integrative analyses that are greatly simplified by SingleCellMultiModal. The package will facilitate development of bioinformatic and statistical methods in Bioconductor to meet the challenges of integrating molecular layers and analyzing phenotypic outputs including cell differentiation, activity, and disease. © 2023 Eckenrode et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	publication_stage = {Final}
}

@ARTICLE{Costa2023,
	author = {Costa, Mireia and García, Alberto S. and Pastor, Oscar},
	title = {The consequences of data dispersion in genomics: a comparative analysis of data sources for precision medicine},
	year = {2023},
	journal = {BMC Medical Informatics and Decision Making},
	volume = {23},
	doi = {10.1186/s12911-023-02342-w},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176086593&doi=10.1186%2fs12911-023-02342-w&partnerID=40&md5=8197c5e15e792f79fa9e01fae867f817},
	abstract = {Background: Genomics-based clinical diagnosis has emerged as a novel medical approach to improve diagnosis and treatment. However, advances in sequencing techniques have increased the generation of genomics data dramatically. This has led to several data management problems, one of which is data dispersion (i.e., genomics data is scattered across hundreds of data repositories). In this context, geneticists try to remediate the above-mentioned problem by limiting the scope of their work to a single data source they know and trust. This work has studied the consequences of focusing on a single data source rather than considering the many different existing genomics data sources. Methods: The analysis is based on the data associated with two groups of disorders (i.e., oncology and cardiology) accessible from six well-known genomic data sources (i.e., ClinVar, Ensembl, GWAS Catalog, LOVD, CIViC, and CardioDB). Two dimensions have been considered in this analysis, namely, completeness and concordance. Completeness has been evaluated at two levels. First, by analyzing the information provided by each data source with regard to a conceptual schema data model (i.e., the schema level). Second, by analyzing the DNA variations provided by each data source as related to any of the disorders selected (i.e., the data level). Concordance has been evaluated by comparing the consensus among the data sources regarding the clinical relevance of each variation and disorder. Results: The data sources with the highest completeness at the schema level are ClinVar, Ensembl, and CIViC. ClinVar has the highest completeness at the data level data source for the oncology and cardiology disorders. However, there are clinically relevant variations that are exclusive to other data sources, and they must be considered in order to provide the best clinical diagnosis. Although the information available in the data sources is predominantly concordant, discordance among the analyzed data exist. This can lead to inaccurate diagnoses. Conclusion: Precision medicine analyses using a single genomics data source leads to incomplete results. Also, there are concordance problems that threaten the correctness of the genomics-based diagnosis results. © 2023, The Author(s).},
	publication_stage = {Final}
}

@ARTICLE{Yadav20232616,
	author = {Yadav, Honey and Sagar, Mahim},
	title = {Exploring COVID-19 vaccine hesitancy and behavioral themes using social media big-data: a text mining approach},
	year = {2023},
	journal = {Kybernetes},
	volume = {52},
	number = {7},
	pages = {2616 – 2648},
	doi = {10.1108/K-06-2022-0810},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161329179&doi=10.1108%2fK-06-2022-0810&partnerID=40&md5=1060ca4470af22a9b5e9911368a3a10f},
	abstract = {Purpose: India has the biggest number of active users on social media platforms, particularly Twitter. The purpose of this paper is to examine public sentiment on COVID-19 vaccines and COVID Appropriate Behaviour (CAB) by text mining (topic modeling) and network analysis supported by thematic modeling. Design/methodology/approach: A sample dataset of 115,000 tweets from the Twitter platform was used to examine the perception of the COVID-19 vaccination and CAB from January 2021 to August 2021. The research applied a machine-learning algorithm and network analysis to extract hidden and latent patterns in unstructured data to identify the most prevalent themes. The COVID-19 Vaccine Hesitancy Amplification Model was formulated, which included five key topics based on sample big data from social media. Findings: The identified themes are Social Media Adaptivity, Lack of Knowledge Providing Mechanism, Perception of Vaccine Safety Measures, Health Care Infrastructure Capabilities and Fear of Coronavirus (Coronaphobia). The study implication assists communication strategists and stakeholders design effective communication strategies using digital platforms. The study reveals CAB themes as with Mask Wearing Issues and Employment Issues as relevant themes discussed on digital channels. Research limitations/implications: The themes extracted in the present study provide a roadmap for policy-makers and communication experts to utilize social media platforms for communicating and understanding the perception of preventive measures of vaccination and CAB. As evidenced by the increased engagement on social media platforms during the COVID-19-induced lockdown, digital platforms are indeed valuable from the communication perspective to be proactive in the event of a similar situation. Moreover, significant themes, including social media adaptivity, absence of knowledge-providing mechanism and perception of safety measures of the vaccine, are the critical parameters leading to an amplified effect on vaccine hesitancy. Practical implications: The COVID-19 Vaccine Hesitancy Amplification Themes (CVHAT) equips stakeholders and government strategists with a preconfigured paradigm to tackle dedicated communication campaigns and assess digital community behavior during health emergencies COVID-19. Social implications: The increased acceptance of vaccines and the following of CAB decrease the advocacy of mutation of the virus and promote the healthy being of the people. As CAB has been mentioned as a preventive strategy against the COVID-19 pandemic, the research preposition promotes communication intervention which helps to mitigate future such pandemics. As developing, economies require effective communication strategies for vaccine acceptance and CAB, this study contributes to filling the gap using a digital environment. Originality/value: Chan et al. (2020) recommended using social media platforms for public knowledge dissemination. The study observed that the value of a communication strategy is increased when communication happens using highly trusted and accessible channels such as Twitter and Facebook. With the preceding context, the present study is a novel approach to contribute toward digital communication strategies related to vaccination and CAB. © 2023, Emerald Publishing Limited.},
	publication_stage = {Final}
}

@ARTICLE{Taira2023,
	author = {Taira, Ricky K. and Garlid, Anders O. and Speier, William},
	title = {Design considerations for a hierarchical semantic compositional framework for medical natural language understanding},
	year = {2023},
	journal = {PLoS ONE},
	volume = {18},
	number = {3 March},
	doi = {10.1371/journal.pone.0282882},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150464090&doi=10.1371%2fjournal.pone.0282882&partnerID=40&md5=c30efe02f751575850f08536993a87b7},
	abstract = {Medical natural language processing (NLP) systems are a key enabling technology for transforming Big Data from clinical report repositories to information used to support disease models and validate intervention methods. However, current medical NLP systems fall considerably short when faced with the task of logically interpreting clinical text. In this paper, we describe a framework inspired by mechanisms of human cognition in an attempt to jump the NLP performance curve. The design centers on a hierarchical semantic compositional model (HSCM), which provides an internal substrate for guiding the interpretation process. The paper describes insights from four key cognitive aspects: semantic memory, semantic composition, semantic activation, and hierarchical predictive coding. We discuss the design of a generative semantic model and an associated semantic parser used to transform a free-Text sentence into a logical representation of its meaning. The paper discusses supportive and antagonistic arguments for the key features of the architecture as a long-Term foundational framework. © 2023 Public Library of Science. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Leipzig2021,
	author = {Leipzig, Jeremy and Nüst, Daniel and Hoyt, Charles Tapley and Ram, Karthik and Greenberg, Jane},
	title = {The role of metadata in reproducible computational research},
	year = {2021},
	journal = {Patterns},
	volume = {2},
	number = {9},
	doi = {10.1016/j.patter.2021.100322},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120051431&doi=10.1016%2fj.patter.2021.100322&partnerID=40&md5=f9388ab7605cd539521b0eb0d45c195b},
	abstract = {Reproducible computational research (RCR) is the keystone of the scientific method for in silico analyses, packaging the transformation of raw data to published results. In addition to its role in research integrity, improving the reproducibility of scientific studies can accelerate evaluation and reuse. This potential and wide support for the FAIR principles have motivated interest in metadata standards supporting reproducibility. Metadata provide context and provenance to raw data and methods and are essential to both discovery and validation. Despite this shared connection with scientific data, few studies have explicitly described how metadata enable reproducible computational research. This review employs a functional content analysis to identify metadata standards that support reproducibility across an analytic stack consisting of input data, tools, notebooks, pipelines, and publications. Our review provides background context, explores gaps, and discovers component trends of embeddedness and methodology weight from which we derive recommendations for future work. © 2021 The Authors},
	publication_stage = {Final}
}

@ARTICLE{Bona2023,
	author = {Bona, Jonathan P and Utecht, Joseph and Bost, Sarah and Brochhausen, Mathias and Prior, Fred},
	title = {The PRISM semantic cohort builder: a novel tool to search and access clinical data in TCIA imaging collections},
	year = {2023},
	journal = {Physics in Medicine and Biology},
	volume = {68},
	number = {1},
	doi = {10.1088/1361-6560/ac9d1d},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145592478&doi=10.1088%2f1361-6560%2fac9d1d&partnerID=40&md5=cc2b9c16576b598490c8cef9fa041bd2},
	abstract = {The cancer imaging archive (TICA) receives and manages an ever-increasing quantity of clinical (non-image) data containing valuable information about subjects in imaging collections. To harmonize and integrate these data, we have first cataloged the types of information occurring across public TCIA collections. We then produced mappings for these diverse instance data using ontology-based representation patterns and transformed the data into a knowledge graph in a semantic database. This repository combined the transformed instance data with relevant background knowledge from domain ontologies. The resulting repository of semantically integrated data is a rich source of information about subjects that can be queried across imaging collections. Building on this work we have implemented and deployed a REST API and a user-facing semantic cohort builder tool. This tool allows allow researchers and other users to search and identify groups of subject-level records based on non-image data that were not queryable prior to this work. The search results produced by this interface link to images, allowing users to quickly identify and view images matching the selection criteria, as well as allowing users to export the harmonized clinical data. © 2022 The Author(s). Published on behalf of Institute of Physics and Engineering in Medicine by IOP Publishing Ltd.},
	publication_stage = {Final}
}

@ARTICLE{von Itzstein20211559,
	author = {von Itzstein, Mitchell S. and Hullings, Melanie and Mayo, Helen and Beg, M. Shaalan and Williams, Erin L. and Gerber, David E.},
	title = {Application of Information Technology to Clinical Trial Evaluation and Enrollment A Review},
	year = {2021},
	journal = {JAMA Oncology},
	volume = {7},
	number = {10},
	pages = {1559 – 1566},
	doi = {10.1001/jamaoncol.2021.1165},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109897609&doi=10.1001%2fjamaoncol.2021.1165&partnerID=40&md5=16ce2d2d3d8ede77bd8ea6d22afbadf7},
	abstract = {IMPORTANCE As cancer treatment has become more individualized, oncologic clinical trials have become more complex. Increasingly numerous and stringent eligibility criteria frequently include tumor molecular or genomic characteristics that may not be readily identified in medical records, rendering it difficult to best match clinical trials with clinical sites and to identify potentially eligible patients once a clinical trial has been selected and activated. Partly because of these factors, enrollment rates for cancer clinical trials remain low, creating delays and increased costs for drug development. Information technology (IT) platforms have been applied to the implementation and conduct of clinical trials to improve efficiencies in several medical fields, and these platforms have recently been introduced to oncologic studies. OBSERVATIONS This review summarizes cancer and noncancer studies that used IT platforms for assistance with clinical trial site selection, patient recruitment, and patient screening. The review does not address the use of IT in other aspects of clinical research, such as wearable physical activity monitors or telehealth visits. A large number of IT platforms (which may be patient facing, site or investigator facing, or sponsor facing) are now commercially available. These applications use artificial intelligence and/or natural language processing to identify and summarize protocol eligibility criteria, institutional patient populations, and individual electronic health records. Although there is an expanding body of literature examining the role of this technology, relatively few studies to date have been performed in oncologic settings. CONCLUSIONS AND RELEVANCE This review found that an increasing number and variety of IT platforms were available to assist in the planning and conduct of clinical trials. Because oncologic clinical care and clinical trial protocols are particularly complex, nuanced, and individualized, published experience with this technology in other fields may not be fully applicable to cancer settings. The extent to which these services will overcome ongoing and increasing challenges in cancer clinical research remains unclear. © 2021 American Medical Association. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Solarte-Pabón2023,
	author = {Solarte-Pabón, Oswaldo and Montenegro, Orlando and García-Barragán, Alvaro and Torrente, Maria and Provencio, Mariano and Menasalvas, Ernestina and Robles, Víctor},
	title = {Transformers for extracting breast cancer information from Spanish clinical narratives},
	year = {2023},
	journal = {Artificial Intelligence in Medicine},
	volume = {143},
	doi = {10.1016/j.artmed.2023.102625},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166485840&doi=10.1016%2fj.artmed.2023.102625&partnerID=40&md5=5a48c672dfaa67b58027cdd3aaf6a40d},
	abstract = {The wide adoption of electronic health records (EHRs) offers immense potential as a source of support for clinical research. However, previous studies focused on extracting only a limited set of medical concepts to support information extraction in the cancer domain for the Spanish language. Building on the success of deep learning for processing natural language texts, this paper proposes a transformer-based approach to extract named entities from breast cancer clinical notes written in Spanish and compares several language models. To facilitate this approach, a schema for annotating clinical notes with breast cancer concepts is presented, and a corpus for breast cancer is developed. Results indicate that both BERT-based and RoBERTa-based language models demonstrate competitive performance in clinical Named Entity Recognition (NER). Specifically, BETO and multilingual BERT achieve F-scores of 93.71% and 94.63%, respectively. Additionally, RoBERTa Biomedical attains an F-score of 95.01%, while RoBERTa BNE achieves an F-score of 94.54%. The findings suggest that transformers can feasibly extract information in the clinical domain in the Spanish language, with the use of models trained on biomedical texts contributing to enhanced results. The proposed approach takes advantage of transfer learning techniques by fine-tuning language models to automatically represent text features and avoiding the time-consuming feature engineering process. © 2023 The Author(s)},
	publication_stage = {Final}
}

@ARTICLE{Chen20247496,
	author = {Chen, Lujiao and Chen, Bo and Zhao, Zhenhua and Shen, Liyijing},
	title = {Using artificial intelligence based imaging to predict lymph node metastasis in non-small cell lung cancer: a systematic review and meta-analysis},
	year = {2024},
	journal = {Quantitative Imaging in Medicine and Surgery},
	volume = {14},
	number = {10},
	pages = {7496 – 7512},
	doi = {10.21037/qims-24-664},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205466330&doi=10.21037%2fqims-24-664&partnerID=40&md5=9fe0ee54d9139d86c171ca738bb98dae},
	abstract = {Background: Lung cancer, especially non-small cell lung cancer (NSCLC), is one of the most-deadly malignancies worldwide. Lung cancer has a worse 5-year survival rate than many primary malignancies. Thus, the early detection and prognosis prediction of lung cancer are crucial. The early detection and prognosis prediction of lung cancer have improved with the widespread use of artificial intelligence (AI) technologies. This meta-analysis examined the accuracy and efficacy of AI-based models in predicting lymph node metastasis (LNM) in NSCLC patients using imaging data. Our findings could help clinicians predict patient prognosis and select alternative therapies. Methods: We searched the PubMed, Web of Science, Cochrane Library, and Embase databases for relevant articles published up to January 31, 2024. Two reviewers individually evaluated all the retrieved articles to assess their eligibility for inclusion in the meta-analysis. The systematic assessment and meta-analysis comprised articles that satisfied the inclusion criteria (e.g., randomized or non-randomized trials, and observational studies) and exclusion criteria (e.g., articles not published in English), and provided data for the quantitative synthesis. The quality of the included articles was assessed using the Quality Assessment of Diagnostic Accuracy Studies-2 (QUADAS-2). The pooled sensitivity, specificity, and area under the curve (AUC) were used to evaluate the ability of AI-based imaging models to predict LNM in NSCLC patients. Sources of heterogeneity were investigated using meta-regression. Covariates, including country, sample size, imaging modality, model validation technique, and model algorithm, were examined in the subgroup analysis. Results: The final meta-analysis comprised 11 retrospective studies of 6,088 NSCLC patients, of whom 1,483 had LNM. The pooled sensitivity, specificity, and AUC of the AI-based imaging model for predicting LNM in NSCLC patients were 0.87 [95% confidence interval (CI): 0.80–0.91], 0.85 (95% CI: 0.78–0.89), and 0.92 (95% CI: 0.90–0.94). Based on the QUADAS-2 results, a risk of bias was detected in the patient selection and diagnostic tests of the included articles. However, the quality of the included articles was generally acceptable. The pooled sensitivity and specificity were heterogeneous (I2>75%). The meta-regression and subgroup analyses showed that imaging modality [computed tomography (CT) or positron emission tomography (PET)/CT], and the neural network method model design significantly affected heterogeneity of this study. Models employing sample size data from a single center and the least absolute shrinkage and selection operator (LASSO) method had greater sensitivity than other techniques. Using the Deek’ s funnel plot, no publishing bias was found. The results of the sensitivity analysis showed that deleting each article one by one did not change the findings. Conclusions: Imaging data models based on AI algorithms have good diagnostic accuracy in predicting LNM in patients with NSCLC and could be applied in clinical settings. © AME Publishing Company.},
	publication_stage = {Final}
}

@ARTICLE{Yang2023,
	author = {Yang, Yang and Lu, Yuwei and Yan, Wenying},
	title = {A comprehensive review on knowledge graphs for complex diseases},
	year = {2023},
	journal = {Briefings in Bioinformatics},
	volume = {24},
	number = {1},
	doi = {10.1093/bib/bbac543},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147044662&doi=10.1093%2fbib%2fbbac543&partnerID=40&md5=48e2d4be575850e79540133244228e71},
	abstract = {In recent years, knowledge graphs (KGs) have gained a great deal of popularity as a tool for storing relationships between entities and for performing higher level reasoning. KGs in biomedicine and clinical practice aim to provide an elegant solution for diagnosing and treating complex diseases more efficiently and flexibly. Here, we provide a systematic review to characterize the state-of-the-art of KGs in the area of complex disease research. We cover the following topics: (1) knowledge sources, (2) entity extraction methods, (3) relation extraction methods and (4) the application of KGs in complex diseases. As a result, we offer a complete picture of the domain. Finally, we discuss the challenges in the field by identifying gaps and opportunities for further research and propose potential research directions of KGs for complex disease diagnosis and treatment.  © 2022 The Author(s). Published by Oxford University Press. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Nastase20211,
	author = {Nastase, Vivi and Szpakowicz, Stan and Nakov, Preslav and Séagdha, Diarmuid Ó.},
	title = {Semantic Relations between Nominals},
	year = {2021},
	journal = {Synthesis Lectures on Human Language Technologies},
	volume = {14},
	number = {1},
	pages = {1 – 218},
	doi = {10.2200/S01078ED2V01Y202002HLT049},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104016854&doi=10.2200%2fS01078ED2V01Y202002HLT049&partnerID=40&md5=3946b55b8a7753d994d56c4db5205545},
	abstract = {Opportunity and Curiosity find similar rocks on Mars. One can generally understand this statement if one knows that Opportunity and Curiosity are instances of the class of Mars rovers, and recognizes that, as signalled by the word on, rocks are located on Mars. Two mental operations contribute to understanding: Recognize how entities/concepts mentioned in a text interact and recall already known facts (which often themselves consist of relations between entities/concepts). Concept interactions one identifies in the text can be added to the repository of known facts, and aid the processing of future texts. The amassed knowledge can assist many advanced language-processing tasks, including summarization, question answering and machine translation. Semantic relations are the connections we perceive between things which interact. The book explores two, now intertwined, threads in semantic relations: How they are expressed in texts and what role they play in knowledge repositories. A historical perspective takes us back more than 2000 years to their beginnings, and then to developments much closer to our time: Various attempts at producing lists of semantic relations, necessary and sufficient to express the interaction between entities/concepts. A look at relations outside context, then in general texts, and then in texts in specialized domains, has gradually brought new insights, and led to essential adjustments in how the relations are seen. At the same time, datasets which encompass these phenomena have become available. They started small, then grew somewhat, then became truly large. The large resources are inevitably noisy because they are constructed automatically. The available corpora - to be analyzed, or used to gather relational evidence - have also grown, and some systems now operate at the Web scale. The learning of semantic relations has proceeded in parallel, in adherence to supervised, unsupervised or distantly supervised paradigms. Detailed analyses of annotated datasets in supervised learning have granted insights useful in developing unsupervised and distantly supervised methods. These in turn have contributed to the understanding of what relations are and how to find them, and that has led to methods scalable to Web-sized textual data. The size and redundancy of information in very large corpora, which at first seemed problematic, have been harnessed to improve the process of relation extraction/learning. The newest technology, deep learning, supplies innovative and surprising solutions to a variety of problems in relation learning. This book aims to paint a big picture and to offer interesting details. Table of Contents: Preface to the Second Edition / Introduction / Relations Between Nominals, Relations Between Concepts / Extracting Semantic Relations with Supervision / Extracting Semantic Relations with Little or No Supervision / Semantic Relations and Deep Learning / Conclusion / Bibliography / Authors' Biographies / Index  Copyright © 2021 by Morgan and Claypool.},
	publication_stage = {Final}
}

@ARTICLE{2023175,
	title = {Quality Evaluation of Academic User Generated Content on Social Media; [社交媒体环境下学术型用户生成内容质量评估研究]},
	year = {2023},
	journal = {Information studies: Theory and Application},
	volume = {46},
	number = {2},
	pages = {175 – 183},
	doi = {10.16353/j.cnki.1000-7490.2023.02.020},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182588029&doi=10.16353%2fj.cnki.1000-7490.2023.02.020&partnerID=40&md5=e53551c61dd3a790d4e4003cf67c32ee},
	abstract = {[Purpose/ significance] This paper aims to explore how to evaluate the quality of academic user-generated content on social media. [Method/ process] By recruiting users who have used academic information on social media, this study firstly conducts academic information retrieval user experiments to obtain criteria of users’ quality evaluation of academic user-generated content, and constructs a initial criteria framework for its quality evaluation; then, the importance of each evaluation criterion is collected through questionnaire data; finally, the evaluation criteria and their weight in the model are obtained by principal component analysis. [Result/ conclusion] A quality evaluation framework for academic user generated content is constructed, which includes 2 first-level criteria and 11 second-level criteria that are assigned weights, of which the top three criteria for importance are demonstration, interestingness, and rationality. This study enriches and improves the social media academic information quality evaluation theory and criteria framework, and provides a reference for automatically identifying high-quality academic user-generated content. © 2023 Information studies: Theory and Application. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Valous2024205,
	author = {Valous, Nektarios A. and Popp, Ferdinand and Zörnig, Inka and Jäger, Dirk and Charoentong, Pornpimol},
	title = {Graph machine learning for integrated multi-omics analysis},
	year = {2024},
	journal = {British Journal of Cancer},
	volume = {131},
	number = {2},
	pages = {205 – 211},
	doi = {10.1038/s41416-024-02706-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192828074&doi=10.1038%2fs41416-024-02706-7&partnerID=40&md5=9d523626fb96092523042af877ed86fa},
	abstract = {Multi-omics experiments at bulk or single-cell resolution facilitate the discovery of hypothesis-generating biomarkers for predicting response to therapy, as well as aid in uncovering mechanistic insights into cellular and microenvironmental processes. Many methods for data integration have been developed for the identification of key elements that explain or predict disease risk or other biological outcomes. The heterogeneous graph representation of multi-omics data provides an advantage for discerning patterns suitable for predictive/exploratory analysis, thus permitting the modeling of complex relationships. Graph-based approaches—including graph neural networks—potentially offer a reliable methodological toolset that can provide a tangible alternative to scientists and clinicians that seek ideas and implementation strategies in the integrated analysis of their omics sets for biomedical research. Graph-based workflows continue to push the limits of the technological envelope, and this perspective provides a focused literature review of research articles in which graph machine learning is utilized for integrated multi-omics data analyses, with several examples that demonstrate the effectiveness of graph-based approaches. © The Author(s) 2024.},
	publication_stage = {Final}
}

@CONFERENCE{Hao20212946,
	author = {Hao, Junheng and Lei, Chuan and Efthymiou, Vasilis and Quamar, Abdul and Özcan, Fatma and Sun, Yizhou and Wang, Wei},
	title = {MEDTO: Medical Data to Ontology Matching Using Hybrid Graph Neural Networks},
	year = {2021},
	journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	pages = {2946 – 2954},
	doi = {10.1145/3447548.3467138},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109825299&doi=10.1145%2f3447548.3467138&partnerID=40&md5=afe6920b5e8f8b7f6f73a51eaf1de426},
	abstract = {Medical ontologies are widely used to describe and organize medical terminologies and to support many critical applications on healthcare databases. These ontologies are often manually curated (e.g., UMLS, SNOMED CT, and MeSH) by medical experts. Medical databases, on the other hand, are often created by database administrators, using different terminology and structures. The discrepancies between medical ontologies and databases compromise interoperability between them. Data to ontology matching is the process of finding semantic correspondences between tables in databases to standard ontologies. Existing solutions such as ontology matching have mostly focused on engineering features from terminological, structural, and semantic model information extracted from the ontologies. However, this is often labor intensive and the accuracy varies greatly across different ontologies. Worse yet, the ontology capturing a medical database is often not given in practice. In this paper, we propose MEDTO, a novel end-to-end framework that consists of three innovative techniques: (1) a lightweight yet effective method that bootstrap a semantically rich ontology from a given medical database, (2) a hyperbolic graph convolution layer that encodes hierarchical concepts in the hyperbolic space, and (3) a heterogeneous graph layer that encodes both local and global context information of a concept. Experiments on two real-world medical datasets matching against SNOMED CT show significant improvements compared to the state-of-the-art methods. MEDTO also consistently achieves competitive results on a benchmark from the Ontology Alignment Evaluation Initiative. © 2021 ACM.},
	publication_stage = {Final}
}

@ARTICLE{Dai2024322,
	author = {Dai, Hong-Jie and Chen, Chien-Chang and Mir, Tatheer Hussain and Wang, Ting-Yu and Wang, Chen-Kai and Chang, Ya-Chen and Yu, Shu-Jung and Shen, Yi-Wen and Huang, Cheng-Jiun and Tsai, Chia-Hsuan and Wang, Ching-Yun and Chen, Hsiao-Jou and Weng, Pei-Shan and Lin, You-Xiang and Chen, Sheng-Wei and Tsai, Ming-Ju and Juang, Shian-Fei and Wu, Su-Ying and Tsai, Wen-Tsung and Huang, Ming-Yii and Huang, Chih-Jen and Yang, Chih-Jen and Liu, Ping-Zun and Huang, Chiao-Wen and Huang, Chi-Yen and Wang, William Yu Chung and Chong, Inn-Wen and Yang, Yi-Hsin},
	title = {Integrating predictive coding and a user-centric interface for enhanced auditing and quality in cancer registry data},
	year = {2024},
	journal = {Computational and Structural Biotechnology Journal},
	volume = {24},
	pages = {322 – 333},
	doi = {10.1016/j.csbj.2024.04.007},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190866217&doi=10.1016%2fj.csbj.2024.04.007&partnerID=40&md5=697f4d851f8b323eb289d92654a3cc8a},
	abstract = {Data curation for a hospital-based cancer registry heavily relies on the labor-intensive manual abstraction process by cancer registrars to identify cancer-related information from free-text electronic health records. To streamline this process, a natural language processing system incorporating a hybrid of deep learning-based and rule-based approaches for identifying lung cancer registry-related concepts, along with a symbolic expert system that generates registry coding based on weighted rules, was developed. The system is integrated with the hospital information system at a medical center to provide cancer registrars with a patient journey visualization platform. The embedded system offers a comprehensive view of patient reports annotated with significant registry concepts to facilitate the manual coding process and elevate overall quality. Extensive evaluations, including comparisons with state-of-the-art methods, were conducted using a lung cancer dataset comprising 1428 patients from the medical center. The experimental results illustrate the effectiveness of the developed system, consistently achieving F1-scores of 0.85 and 1.00 across 30 coding items. Registrar feedback highlights the system's reliability as a tool for assisting and auditing the abstraction. By presenting key registry items along the timeline of a patient's reports with accurate code predictions, the system improves the quality of registrar outcomes and reduces the labor resources and time required for data abstraction. Our study highlights advancements in cancer registry coding practices, demonstrating that the proposed hybrid weighted neural-symbolic cancer registry system is reliable and efficient for assisting cancer registrars in the coding workflow and contributing to clinical outcomes. © 2024 The Authors},
	publication_stage = {Final}
}

@BOOK{Rubin2021299,
	author = {Rubin, Daniel L. and Greenspan, Hayit and Hoogi, Assaf},
	title = {Biomedical imaging informatics},
	year = {2021},
	journal = {Biomedical Informatics: Computer Applications in Health Care and Biomedicine: Fifth Edition},
	pages = {299 – 362},
	doi = {10.1007/978-3-030-58721-5_10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149655070&doi=10.1007%2f978-3-030-58721-5_10&partnerID=40&md5=1c3a94b5480748675ab26cb58ecfccd4},
	abstract = {Images are pervasive in biomedicine, providing key information used for understanding the phenotype of disease. Biomedical imaging informatics is a field that involves computational methods related to acquisition, processing, and analysis of images in biomedicine. Major topics in biomedical imaging informatics follow the life cycle of images in the healthcare system, including image acquisition (generating images from the modality and converting them to digital form), image content representation (making the information in images accessible to machines for processing), image management and storage (methods for storing, transmitting, displaying, retrieving, and organizing images), image processing (methods to enhance, segment, visualize, fuse, or analyze the images), and finally image interpretation and computerized reasoning (methods by which the computer can assist the individual viewing the image to recognize content in the image or make better interpretations of images). The number of images being acquired in research and clinical practice is exploding, and the methods of biomedical informatics are increasingly essential in order to make optimal use of these key data. There is particular excitement in helping physicians to make better decisions by leveraging massive amounts of image data, and topics related to artificial intelligence in imaging and machine learning recently are showing promising results to improve image interpretation. At the same time, the types of medical images and imaging modalities continue to expand, bringing with them the need for new biomedical imaging informatics applications and demand for leveraging the content in new types images to characterize disease in patients. Continued research and development in biomedical imaging informatics will build on the core principles outlined in this chapter. © Springer Nature Switzerland AG 2021.},
	publication_stage = {Final}
}

@ARTICLE{Mithun2023812,
	author = {Mithun, Sneha and Jha, Ashish Kumar and Sherkhane, Umesh B. and Jaiswar, Vinay and Purandare, Nilendu C. and Dekker, Andre and Puts, Sander and Bermejo, Inigo and Rangarajan, V. and Zegers, Catharina M. L. and Wee, Leonard},
	title = {Clinical Concept-Based Radiology Reports Classification Pipeline for Lung Carcinoma},
	year = {2023},
	journal = {Journal of Digital Imaging},
	volume = {36},
	number = {3},
	pages = {812 – 826},
	doi = {10.1007/s10278-023-00787-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148025292&doi=10.1007%2fs10278-023-00787-z&partnerID=40&md5=85cd1b6ec93ed8585e427ed25916b5ef},
	abstract = {Rising incidence and mortality of cancer have led to an incremental amount of research in the field. To learn from preexisting data, it has become important to capture maximum information related to disease type, stage, treatment, and outcomes. Medical imaging reports are rich in this kind of information but are only present as free text. The extraction of information from such unstructured text reports is labor-intensive. The use of Natural Language Processing (NLP) tools to extract information from radiology reports can make it less time-consuming as well as more effective. In this study, we have developed and compared different models for the classification of lung carcinoma reports using clinical concepts. This study was approved by the institutional ethics committee as a retrospective study with a waiver of informed consent. A clinical concept-based classification pipeline for lung carcinoma radiology reports was developed using rule-based as well as machine learning models and compared. The machine learning models used were XGBoost and two more deep learning model architectures with bidirectional long short-term neural networks. A corpus consisting of 1700 radiology reports including computed tomography (CT) and positron emission tomography/computed tomography (PET/CT) reports were used for development and testing. Five hundred one radiology reports from MIMIC-III Clinical Database version 1.4 was used for external validation. The pipeline achieved an overall F1 score of 0.94 on the internal set and 0.74 on external validation with the rule-based algorithm using expert input giving the best performance. Among the machine learning models, the Bi-LSTM_dropout model performed better than the ML model using XGBoost and the Bi-LSTM_simple model on internal set, whereas on external validation, the Bi-LSTM_simple model performed relatively better than other 2. This pipeline can be used for clinical concept-based classification of radiology reports related to lung carcinoma from a huge corpus and also for automated annotation of these reports. © 2023, The Author(s).},
	publication_stage = {Final}
}

@ARTICLE{Sajovic2022,
	author = {Sajovic, Irena and Boh Podgornik, Bojana},
	title = {Bibliometric Analysis of Visualizations in Computer Graphics: A Study},
	year = {2022},
	journal = {SAGE Open},
	volume = {12},
	number = {1},
	doi = {10.1177/21582440211071105},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123627645&doi=10.1177%2f21582440211071105&partnerID=40&md5=212a4cfedfc469cb0258cdba8ad19f25},
	abstract = {This study examined the field of visualizations in computer graphics (VCG) using bibliometric methods, including performance and science mapping analysis. A dataset of documents from SCOPUS, 1986 to 2019, was analyzed and visualized using VOSviewer. The results showed an increasing trend of new documents in VCG. The five most cited publications were all related to data visualization software. The most prolific authors, examined by Citation per Paper (CPP) and Relative Citation Impact (RCI), contributed to research advances in computer graphics, information visualization, interactive data analysis, human computer interfaces, and visualization software development. A document source analysis identified the major scientific journals and conference proceedings in VCG research, and showed that researchers in VCG tend to publish extensively in conference proceedings, but that articles in scientific journals have a higher citation impact. A co-citation analysis of cited sources revealed seven clusters of thematically similar sources in computer science, genomics, neuroimaging, physics & chemistry, mathematics, education, and communication. Co-authorship analysis of countries pointed out collaboration networks in scientific research, where the USA, UK, Germany, France, and Italy collaborated most frequently. Collaborations were fostered by the same language group, or the geographical proximity. The co-occurrence of research terms showed six clusters of related concepts, thematically grouped around search queries, graphical processing, education, genetics, scientific numerical data, and medicine. The study contributed to a better understanding of the field and is expected to help researchers and educators identify research areas, developments, quality scientific literature, and appropriate journals for publishing their own findings related to VCG. © The Author(s) 2022.},
	publication_stage = {Final}
}

@BOOK{Zeng2023433,
	author = {Zeng, Zheni and Liu, Zhiyuan and Lin, Yankai and Sun, Maosong},
	title = {Biomedical Knowledge Representation Learning},
	year = {2023},
	journal = {Representation Learning for Natural Language Processing, Second Edition},
	pages = {433 – 462},
	doi = {10.1007/978-981-99-1600-9_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197026693&doi=10.1007%2f978-981-99-1600-9_12&partnerID=40&md5=5d4a2c702aba1dab51799e90328670a9},
	abstract = {As a subject closely related to our life and understanding of the world, biomedicine keeps drawing much attention from researchers in recent years. To help improve the efficiency of people and accelerate the progress of this subject, AI techniques especially NLP methods are widely adopted in biomedical research. In this chapter, with biomedical knowledge as the core, we launch a discussion on knowledge representation and acquisition as well as biomedical knowledge-guided NLP tasks and explain them in detail with practical scenarios. We also discuss current research progress and several future directions. © The Editor(s) (if applicable) and The Author(s) 2020, 2023.},
	publication_stage = {Final}
}

@CONFERENCE{Nguyen202431,
	author = {Nguyen, Eloisa and Lin, Rebecca Z. and Gong, Yang and Tao, Cui and Amith, Muhammad Tuan},
	title = {Developing a Computational Representation of Human Physical Activity and Exercise Using Open Ontology-Based Approach: A Tai Chi Use Case},
	year = {2024},
	journal = {Proceedings - 2024 IEEE 12th International Conference on Healthcare Informatics, ICHI 2024},
	pages = {31 – 39},
	doi = {10.1109/ICHI61247.2024.00012},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203704281&doi=10.1109%2fICHI61247.2024.00012&partnerID=40&md5=e6a29ae644a67dec6a4d4a63afd06a49},
	abstract = {Many studies have examined the impact of exercise and other physical activities in influencing the health outcomes of individuals. These physical activities entail an intricate sequence and series of physical anatomy, physiological movement, movement of the anatomy, etc. To better understand how these components interact with one another and their downstream impact on health outcomes, there needs to be an information model that conceptualizes all entities involved. In this study, we introduced our early development of an ontology model to computationally describe human physical activities and the various entities that compose each activity. We developed an open-sourced biomedical ontology called the Kinetic Human Movement Ontology that reused OBO Foundry terminologies and encoded in OWL2. We applied this ontology in modeling and linking a specific Tai Chi movement. The contribution of this work could enable modeling of information relating to human physical activity, like exercise, and lead towards information standardization of human movement for analysis. Future work will include expanding our ontology to include more expressive information and completely modeling entire sets of movement from human physical activity. © 2024 IEEE.},
	publication_stage = {Final}
}

@ARTICLE{Bernasconi2023,
	author = {Bernasconi, Anna and Canakoglu, Arif and Comolli, Federico},
	title = {Processing genome-wide association studies within a repository of heterogeneous genomic datasets},
	year = {2023},
	journal = {BMC Genomic Data},
	volume = {24},
	number = {1},
	doi = {10.1186/s12863-023-01111-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149541411&doi=10.1186%2fs12863-023-01111-y&partnerID=40&md5=1cb5cc21419a2643b72abeacf3a03e76},
	abstract = {Background: Genome Wide Association Studies (GWAS) are based on the observation of genome-wide sets of genetic variants – typically single-nucleotide polymorphisms (SNPs) – in different individuals that are associated with phenotypic traits. Research efforts have so far been directed to improving GWAS techniques rather than on making the results of GWAS interoperable with other genomic signals; this is currently hindered by the use of heterogeneous formats and uncoordinated experiment descriptions. Results: To practically facilitate integrative use, we propose to include GWAS datasets within the META-BASE repository, exploiting an integration pipeline previously studied for other genomic datasets that includes several heterogeneous data types in the same format, queryable from the same systems. We represent GWAS SNPs and metadata by means of the Genomic Data Model and include metadata within a relational representation by extending the Genomic Conceptual Model with a dedicated view. To further reduce the gap with the descriptions of other signals in the repository of genomic datasets, we perform a semantic annotation of phenotypic traits. Our pipeline is demonstrated using two important data sources, initially organized according to different data models: the NHGRI-EBI GWAS Catalog and FinnGen (University of Helsinki). The integration effort finally allows us to use these datasets within multi-sample processing queries that respond to important biological questions. These are then made usable for multi-omic studies together with, e.g., somatic and reference mutation data, genomic annotations, epigenetic signals. Conclusions: As a result of the our work on GWAS datasets, we enable 1) their interoperable use with several other homogenized and processed genomic datasets in the context of the META-BASE repository; 2) their big data processing by means of the GenoMetric Query Language and associated system. Future large-scale tertiary data analysis may extensively benefit from the addition of GWAS results to inform several different downstream analysis workflows. © 2023, The Author(s).},
	publication_stage = {Final}
}

@ARTICLE{Halper20235,
	author = {Halper, Michael and Soldatova, Larisa N. and Brochhausen, Mathias and Sabiu Maikore, Fatima and Ochs, Christopher and Perl, Yehoshua},
	title = {Guidelines for the reuse of ontology content},
	year = {2023},
	journal = {Applied Ontology},
	volume = {18},
	number = {1},
	pages = {5 – 29},
	doi = {10.3233/AO-230275},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161265044&doi=10.3233%2fAO-230275&partnerID=40&md5=beb5f0154a2f6f9302b4ec20bba1f24e},
	abstract = {Reuse of elements from existing ontologies in the construction of new ontologies is a foundational principle in ontological design. It offers the benefits, among others, of consistency and interoperability between such knowledge structures as well as sharing resources. Reuse is widely found within important collections of established ontologies, such as BioPortal and the OBO Foundry. However, reuse comes with its own potential problems involving ontological commitment, granularity, and ambiguity. Guidelines are proposed to aid ontology developers and curators in their prospective reuse of content. These guidelines have been gleaned over years of practice in the ontology field. The guidelines are couched in experiential reports on designing and curating particular ontologies (e.g., EXACT and EXACT2) and using generally accepted approaches (e.g., MIREOT) in doing so. Various software tools to assist in ontology reuse are surveyed and discussed.  © 2023-IOS Press. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Schulz2023207,
	author = {Schulz, Stefan and Case, James T. and Hendler, Peter and Karlsson, Daniel and Lawley, Michael and Cornet, Ronald and Hausam, Robert and Solbrig, Harold and Nashar, Karim and Martínez-Costa, Catalina and Gao, Yongsheng},
	title = {SNOMED CT and Basic Formal Ontology - convergence or contradiction between standards? The case of 'clinical finding'},
	year = {2023},
	journal = {Applied Ontology},
	volume = {18},
	number = {3},
	pages = {207 – 237},
	doi = {10.3233/AO-230018},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177090216&doi=10.3233%2fAO-230018&partnerID=40&md5=e7399a5cf6af7907c518f8af011c04d4},
	abstract = {Background: SNOMED CT is a large terminology system designed to represent all aspects of healthcare. Its current form and content result from decades of bottom-up evolution. Due to SNOMED CT's formal descriptions, it can be considered an ontology. The Basic Formal Ontology (BFO) is a foundational ontology that proposes a small set of disjoint, hierarchically ordered classes, supported by relations and axioms. In contrast, as a typical top-down endeavor, BFO was designed as a foundational framework for domain ontologies in the natural sciences and related disciplines. Whereas it is mostly assumed that domain ontologies should be created as extensions of foundational ontologies, a post-hoc harmonization of consolidated domain ontologies in use, such as SNOMED CT, is known to be challenging. Methods: We explored the feasibility of harmonizing SNOMED CT with BFO, with a focus on the SNOMED CT Clinical Finding hierarchy. With more than 100,000 classes, it accounts for about one third of SNOMED CT's content. In particular, we represented typical SNOMED CT finding/disorder concepts using description logics under BFO. Three representational patterns were created and the logical entailments analyzed. Results: Under a first scrutiny, the clinical intuition that diseases, disorders, signs and symptoms form a homogeneous ontological upper-level class appeared incompatible with BFO's upper-level distinction into continuants and occurrents. The Clinical finding class seemed to be an umbrella for all kinds of entities of clinical interest, such as material entities, processes, states, dispositions, and qualities. This suggests the conclusion that Clinical finding would not be a suitable upper-level class from an BFO perspective. On closer inspection of the taxonomic links within this hierarchy and the implicit meaning derived thereof, it became clear that Clinical finding classes do not characterize the entity (e.g. a fracture, allergy, tumor, pain, hemorrhage, seizure, fever) in a literal sense but rather the condition of a patient having that fracture, allergy, pain etc. This gives sense to the current characteristic of the Clinical Finding hierarchy, in which complex classes are modeled as subclasses of their constituents. Most of these taxonomic links are inferred, as the consequence of the ‘role group' design pattern, which is ubiquitous in SNOMED CT and has often been subject of controversy regarding its semantics. Conclusion: Our analyses resulted in the proposal of (i) equating SNOMED CT's role group' property with the reflexive and transitive BFO relation has occurrent part'; and (ii) reinterpreting Clinical Findings as Clinical Occurrents, i.e. temporally extended entities in an organism, having one or more occurrents as temporal parts that occur in continuants. This re-interpretation was corroborated by a manual analysis of classes under Clinical Finding, as well as the identification of similar modeling patterns in other ontologies. As a result, SNOMED CT does not require any content redesign to establish compatibility with BFO, apart from this re-interpretation, and a suggested re-labeling. Regarding the feasibility of harmonizing terminologies with principled foundational ontologies post-hoc, our results provide support to the assumption that this does not necessarily require major redesign efforts, but rather a careful analysis of the implicit assumptions of terminology curators and users. © 2023 - The authors. Published by IOS Press.},
	publication_stage = {Final}
}

@ARTICLE{Parimbelli2021,
	author = {Parimbelli, E. and Wilk, S. and Cornet, R. and Sniatala, P. and Sniatala, K. and Glaser, S.L.C. and Fraterman, I. and Boekhout, A.H. and Ottaviano, M. and Peleg, M.},
	title = {A review of AI and Data Science support for cancer management},
	year = {2021},
	journal = {Artificial Intelligence in Medicine},
	volume = {117},
	doi = {10.1016/j.artmed.2021.102111},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107838674&doi=10.1016%2fj.artmed.2021.102111&partnerID=40&md5=6db733d4681ef391b4a701814c11f238},
	abstract = {Introduction: Thanks to improvement of care, cancer has become a chronic condition. But due to the toxicity of treatment, the importance of supporting the quality of life (QoL) of cancer patients increases. Monitoring and managing QoL relies on data collected by the patient in his/her home environment, its integration, and its analysis, which supports personalization of cancer management recommendations. We review the state-of-the-art of computerized systems that employ AI and Data Science methods to monitor the health status and provide support to cancer patients managed at home. Objective: Our main objective is to analyze the literature to identify open research challenges that a novel decision support system for cancer patients and clinicians will need to address, point to potential solutions, and provide a list of established best-practices to adopt. Methods: We designed a review study, in compliance with the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, analyzing studies retrieved from PubMed related to monitoring cancer patients in their home environments via sensors and self-reporting: what data is collected, what are the techniques used to collect data, semantically integrate it, infer the patient's state from it and deliver coaching/behavior change interventions. Results: Starting from an initial corpus of 819 unique articles, a total of 180 papers were considered in the full-text analysis and 109 were finally included in the review. Our findings are organized and presented in four main sub-topics consisting of data collection, data integration, predictive modeling and patient coaching. Conclusion: Development of modern decision support systems for cancer needs to utilize best practices like the use of validated electronic questionnaires for quality-of-life assessment, adoption of appropriate information modeling standards supplemented by terminologies/ontologies, adherence to FAIR data principles, external validation, stratification of patients in subgroups for better predictive modeling, and adoption of formal behavior change theories. Open research challenges include supporting emotional and social dimensions of well-being, including PROs in predictive modeling, and providing better customization of behavioral interventions for the specific population of cancer patients. © 2021 The Authors},
	publication_stage = {Final}
}

@ARTICLE{Kim Yeary2022,
	author = {Kim Yeary, Karen H. and Clark, Nikia and Saad-Harfouche, Frances and Erwin, Deborah and Kuliszewski, Margaret Gates and Li, Qiang and McCann, Susan E. and Yu, Han and Lincourt, Catherine and Zoellner, Jamie and Tang, Li},
	title = {Cruciferous Vegetable Intervention to Reduce the Risk of Cancer Recurrence in Non–Muscle-Invasive Bladder Cancer Survivors: Development Using a Systematic Process},
	year = {2022},
	journal = {JMIR Cancer},
	volume = {8},
	number = {1},
	doi = {10.2196/32291},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124939876&doi=10.2196%2f32291&partnerID=40&md5=ad0fc712141a5ccd62a04868669056a9},
	abstract = {Background: Bladder cancer is one of the top 10 most common cancers in the United States. Most bladder cancers (70%-80%) are diagnosed at early stages as non–muscle-invasive bladder cancer (NMIBC), which can be removed surgically. However, 50% to 80% of NMIBC cases recur within 5 years, and 15% to 30% progress with poor survival. Current treatments are limited and expensive. A wealth of preclinical and epidemiological evidence suggests that dietary isothiocyanates in cruciferous vegetables (Cruciferae) could be a novel, noninvasive, and cost-effective strategy to control NMIBC recurrence and progression. Objective: The aim of this study is to develop a scalable dietary intervention that increases isothiocyanate exposure through Cruciferae intake in NMIBC survivors. Methods: We worked with a community advisory board (N=8) to identify relevant factors, evidence-based behavior change techniques, and behavioral theory constructs used to increase Cruciferae intake in NMIBC survivors; use the PEN-3 Model focused on incorporating cultural factors salient to the group’s shared experiences to review the intervention components (eg, the saliency of behavioral messages); administer the revised intervention to community partners for their feedback; and refine the intervention. Results: We developed a multicomponent intervention for NMIBC survivors consisting of a magazine, tracking book, live telephone call script, and interactive voice messages. Entitled POW-R Health: Power to Redefine Your Health, the intervention incorporated findings from our adaptation process to ensure saliency to NMIBC survivors. Conclusions: This is the first evidence-based, theoretically grounded dietary intervention developed to reduce bladder cancer recurrence in NMIBC survivors using a systematic process for community adaptation. This study provides a model for others who aim to develop behavioral, community-relevant interventions for cancer prevention and control with the overall goal of wide-scale implementation and dissemination. © Karen H Kim Yeary, Nikia Clark, Frances Saad-Harfouche, Deborah Erwin, Margaret Gates Kuliszewski, Qiang Li, Susan E McCann, Han Yu, Catherine Lincourt, Jamie Zoellner, Li Tang. Originally published in JMIR Cancer (https://cancer.jmir.org),15.02.2022. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Cancer, is properly cited. The complete bibliographic information, a link to the original publication on https://cancer.jmir.org/, as well as this copyright and license information must be included.},
	publication_stage = {Final}
}

@ARTICLE{Deshmukh20211751,
	author = {Deshmukh, Pratiksha R. and Phalnikar, Rashmi},
	title = {Information extraction for prognostic stage prediction from breast cancer medical records using NLP and ML},
	year = {2021},
	journal = {Medical and Biological Engineering and Computing},
	volume = {59},
	number = {9},
	pages = {1751 – 1772},
	doi = {10.1007/s11517-021-02399-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111084496&doi=10.1007%2fs11517-021-02399-7&partnerID=40&md5=7849d2a56136c310ac11c1def5c2e1c6},
	abstract = {For cancer prediction, the prognostic stage is the main factor that helps medical experts to decide the optimal treatment for a patient. Specialists study prognostic stage information from medical reports, often in an unstructured form, and take a larger review time. The main objective of this study is to suggest a generic clinical decision-unifying staging method to extract the most reliable prognostic stage information of breast cancer from medical records of various health institutions. Additional prognostic elements should be extracted from medical reports to identify the cancer stage for getting an exact measure of cancer and improving care quality. This study has collected 465 pathological and clinical reports of breast cancer sufferers from India’s reputed medical institutions. The unstructured records were found distinct from each institute. Anatomic and biologic factors are extracted from medical records using the natural language processing, machine learning and rule-based method for prognostic stage detection. This study has extracted anatomic stage, grade, estrogen receptor (ER), progesterone receptor (PR), and human epidermal growth factor receptor 2 (HER2) from medical reports with high accuracy and predicted prognostic stage for both regions. The prognostic stage prediction’s average accuracy is found 92% and 82% in rural and urban areas, respectively. It was essential to combine biological and anatomical elements under a single prognostic staging method. A generic clinical decision-unifying staging method for prognostic stage detection with great accuracy in various institutions of different regional areas suggests that the proposed research improves the prognosis of breast cancer. Graphical abstract: [Figure not available: see fulltext.]. © 2021, International Federation for Medical and Biological Engineering.},
	publication_stage = {Final}
}

@ARTICLE{Wagenpfeil2022,
	author = {Wagenpfeil, Stefan and Mc Kevitt, Paul and Cheddad, Abbas and Hemmje, Matthias},
	title = {Explainable Multimedia Feature Fusion for Medical Applications},
	year = {2022},
	journal = {Journal of Imaging},
	volume = {8},
	number = {4},
	doi = {10.3390/jimaging8040104},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132040087&doi=10.3390%2fjimaging8040104&partnerID=40&md5=702d75c4f32186a9e2d4bfe57d0677a4},
	abstract = {Due to the exponential growth of medical information in the form of, e.g., text, images, Electrocardiograms (ECGs), X-ray, multimedia, etc., the management of a patient’s data has become a huge challenge. Particularly, the extraction of features from various different formats and their representation in a homogeneous way are areas of particular interest in medical applications. Multimedia Information Retrieval (MMIR) frameworks, like the Generic Multimedia Analysis Framework (GMAF), can contribute to solving this problem, when adapted to special requirements and modalities of medical applications. In this paper, we demonstrate how typical multimedia processing techniques can be extended and adapted to medical applications and how these applications benefit from employing a Multimedia Feature Graph (MMFG) and specialized, highly efficient indexing structures in the form of Graph Codes. These Graph Codes are transformed to feature relevant Graph Codes by employing a modified Term Frequency Inverse Document Frequency (TFIDF) algorithm, which further supports value ranges and Boolean operations required in the medical context. On this basis, various metrics for the calculation of similarity, recommendations, and automated inferencing and reasoning can be applied supporting the field of diagnostics. Finally, the presentation of these new facilities in the form of explainability is introduced and demonstrated. Thus, in this paper, we show how Graph Codes contribute new querying options for diagnosis and how Explainable Graph Codes can help to quickly understand medical multimedia formats. © 2022 by the authors.},
	publication_stage = {Final}
}

@ARTICLE{Brochhausen2022,
	author = {Brochhausen, Mathias and Whorton, Justin M. and Zayas, Cilia E. and Singh, Nitya and Brochhausen, Christoph and Sexton, Kevin W. and Kimbrell, Monica P. and Bost, Sarah J. and Blobel, Bernd},
	title = {Assessing the Need for Semantic Data Integration for Surgical Biobanks—A Knowledge Representation Perspective},
	year = {2022},
	journal = {Journal of Personalized Medicine},
	volume = {12},
	number = {5},
	doi = {10.3390/jpm12050757},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130137514&doi=10.3390%2fjpm12050757&partnerID=40&md5=28e8981a3bfb6673c26a13f89c38066c},
	abstract = {To improve patient outcomes after trauma, the need to decrypt the post-traumatic immune response has been identified. One prerequisite to drive advancement in understanding that domain is the implementation of surgical biobanks. This paper focuses on the outcomes of patients with one of two diagnoses: post-traumatic arthritis and osteomyelitis. In creating surgical biobanks, currently, many obstacles must be overcome. Roadblocks exist around scoping of data that is to be collected, and the semantic integration of these data. In this paper, the generic component model and the Semantic Web technology stack are used to solve issues related to data integration. The results are twofold: (a) a scoping analysis of data and the ontologies required to harmonize and integrate it, and (b) resolution of common data integration issues in integrating data relevant to trauma surgery. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
	publication_stage = {Final}
}

@ARTICLE{Ryckman2022919,
	author = {Ryckman, Jeffrey M. and Thomas, Toms V. and Wang, Ming and Wu, Xue and Siva, Shankar and Spratt, Daniel E. and Slotman, Ben and Pal, Sumanta and Chapin, Brian F. and Fitzal, Florian and Soran, Atilla and Bex, Axel and Louie, Alexander V. and Lehrer, Eric J. and Zaorsky, Nicholas G.},
	title = {Local Treatment of the Primary Tumor for Patients With Metastatic Cancer (PRIME-TX): A Meta-Analysis},
	year = {2022},
	journal = {International Journal of Radiation Oncology Biology Physics},
	volume = {114},
	number = {5},
	pages = {919 – 935},
	doi = {10.1016/j.ijrobp.2022.06.095},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140919360&doi=10.1016%2fj.ijrobp.2022.06.095&partnerID=40&md5=0065c3b20c56a7723a4b13d9df283f5e},
	abstract = {Purpose: Local treatment of the primary tumor for patients with metastases is controversial, and prospective data across many disease sites have conflicting conclusions regarding benefits. Methods and Materials: A comprehensive search was conducted in PubMed/MEDLINE including randomized controlled trials (RCTs) published in the past 50 years. Inclusion criteria were multi-institutional RCTs of patients with metastatic disease receiving systemic therapy randomized to addition of local treatment to the primary tumor. Two primary outcome measures, overall survival (OS) and progression-free survival (PFS), were quantitatively assessed using random effects, and meta-analyses were conducted using the inverse variance method for pooling. Secondary endpoints were qualitatively assessed and included toxicity and patient-reported quality of life. Exploratory analyses were performed by treatment type and volume of disease. Results: Eleven studies comprising 4952 patients were included (1558 patients received radiation therapy and 913 patients received surgery as primary tumor treatment). OS and PFS were not significantly improved from treatment of the primary (OS: hazard ratio [HR], 0.91; 95% confidence interval [CI], 0.80-1.05; PFS: HR, 0.88; 95% CI, 0.72-1.07). Assessment of primary local treatment modality demonstrated a significant difference in summary effect size on PFS between trials using surgery (HR, 1.15; 95% CI, 0.99-1.33) compared with radiation therapy (HR, 0.73; 95% CI, 0.56-0.96) as the local treatment modality (P =.005). In low metastatic burden patients, radiation therapy was associated with significantly improved OS (HR, 0.67; 95% CI, 0.52-0.85), but surgery was not associated with improved OS compared with no local treatment (HR, 1.12; 95% CI, 0.94-1.34). Conclusions: In RCTs conducted to date enrolling a variety of cancer types with variable metastatic burden, there is no consistent improvement in PFS or OS from the addition of local therapy to the primary tumor in unselected patients with metastatic disease. Carefully selected patients may derive oncologic benefit and should be discussed in tumor boards. Future prospective studies should aim to further optimize patient selection and the optimal systemic and local therapy treatment types. © 2022 Elsevier Inc.},
	publication_stage = {Final}
}

@BOOK{Tulchinsky20231,
	author = {Tulchinsky, Theodore H. and Varavikova, Elena A. and Cohen, Matan J.},
	title = {The New Public Health, Fourth Edition},
	year = {2023},
	journal = {The New Public Health, Fourth Edition},
	pages = {1 – 1182},
	doi = {10.1016/C2019-0-04675-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159394029&doi=10.1016%2fC2019-0-04675-4&partnerID=40&md5=f1ccf1fe71c48ebffabb26e78a5d2d00},
	abstract = {The New Public Health has established itself as a solid textbook throughout the world. Translated into seven languages, this work distinguishes itself from other public health textbooks, which are either highly locally oriented or, if international, lack the specificity of local issues relevant to students' understanding of applied public health in their own setting. Fully revised, The New Public Health, Fourth Edition provides a unified approach to public health appropriate for graduate students and advance undergraduate students especially for courses in MPH, community health, preventive medicine, community health education programs, community health nursing programs. It is also a valuable resource for health professionals requiring an overview of public health. © 2023 Elsevier Inc. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Xu2024,
	author = {Xu, Liang and Lu, Lu and Liu, Minglu and Song, Chengxuan and Wu, Lizhen},
	title = {Nanjing Yunjin intelligent question-answering system based on knowledge graphs and retrieval augmented generation technology},
	year = {2024},
	journal = {Heritage Science},
	volume = {12},
	number = {1},
	doi = {10.1186/s40494-024-01231-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189988524&doi=10.1186%2fs40494-024-01231-3&partnerID=40&md5=fa422be6944ff08e26848af12bba0f7b},
	abstract = {Nanjing Yunjin, a traditional Chinese silk weaving craft, is celebrated globally for its unique local characteristics and exquisite workmanship, forming an integral part of the world's intangible cultural heritage. However, with the advancement of information technology, the experiential knowledge of the Nanjing Yunjin production process is predominantly stored in text format. As a highly specialized and vertical domain, this information is not readily convert into usable data. Previous studies on a knowledge graph-based Nanjing Yunjin Question-Answering System have partially addressed this issue. However, knowledge graphs need to be constantly updated and rely on predefined entities and relationship types. Faced with ambiguous or complex natural language problems, knowledge graph information retrieval faces some challenges. Therefore, this study proposes a Nanjing Yunjin Question-Answering System that integrates Knowledge Graphs and Retrieval Augmented Generation techniques. In this system, the ROBERTA model is first utilized to vectorize Nanjing Yunjin textual information, delving deep into textual semantics to unveil its profound cultural connotations. Additionally, the FAISS vector database is employed for efficient storage and retrieval of Nanjing Yunjin information, achieving a deep semantic match between questions and answers. Ultimately, related retrieval results are fed into the Large Language Model for enhanced generation, aiming for more accurate text generation outcomes and improving the interpretability and logic of the Question-Answering System. This research merges technologies like text embedding, vectorized retrieval, and natural language generation, aiming to overcome the limitations of knowledge graphs-based Question-Answering System in terms of graph updating, dependency on predefined types, and semantic understanding. System implementation and testing have shown that the Nanjing Yunjin Intelligent Question-Answering System, constructed on the basis of Knowledge Graphs and Retrieval Augmented Generation, possesses a broader knowledge base that considers context, resolving issues of polysemy, vague language, and sentence ambiguity, and efficiently and accurately generates answers to natural language queries. This significantly facilitates the retrieval and utilization of Yunjin knowledge, providing a paradigm for constructing Question-Answering System for other intangible cultural heritages, and holds substantial theoretical and practical significance for the deep exploration and discovery of the knowledge structure of human intangible heritage, promoting cultural inheritance and protection. © The Author(s) 2024.},
	publication_stage = {Final}
}

@ARTICLE{Kong2022,
	author = {Kong, Jindi and He, Yuting and Zhu, Xiaomei and Shao, Pengfei and Xu, Yi and Chen, Yang and Coatrieux, Jean-Louis and Yang, Guanyu},
	title = {BKC-Net: Bi-Knowledge Contrastive Learning for renal tumor diagnosis on 3D CT images},
	year = {2022},
	journal = {Knowledge-Based Systems},
	volume = {252},
	doi = {10.1016/j.knosys.2022.109369},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134729027&doi=10.1016%2fj.knosys.2022.109369&partnerID=40&md5=51ff6f45a2da4d48a10449f03550afd3},
	abstract = {Renal tumor diagnosis on abdominal enhanced CT volumes is one of the most significant tasks in kidney disease diagnosis. It helps clinicians decide whether to perform the surgery (subtype classification), perform radical operations or minimally invasive treatment (grade classification). However, inherent challenges greatly limit the performance of the model: 1) Tumor appearance differences caused by non-tumor factors. 2) Small inter-class differences and large intra-class variations. In this paper, we propose a novel diagnosis framework for renal tumors, Bi-knowledge Contrastive Network (BKC-Net), which has two innovations: (1) Focus-perceive learning segments the tumors while perceiving the surrounding healthy tissues, thus adjusting the model's representation of tumor appearance, helping the BKC-Net represent the inherent features of tumors. (2) Bi-knowledge contrastive learning introduces prior radiomics features, makes the prior radiomics knowledge and latent deep knowledge complementary to each other from the intra-case level, and forces the high cohesion and low coupling embedding feature space from the inter-case levels, helping to discover subtle but essential differences among classes. Experiments demonstrate that our BKC-Net has the best performance in renal tumor diagnosis. Results reveal that our framework has great potential for renal tumor diagnosis in clinical use. Source codes will be released at https://github.com/DD0922/BKC-Net-pytorch. © 2022 Elsevier B.V.},
	publication_stage = {Final}
}

@ARTICLE{Mamilos2023,
	author = {Mamilos, Andreas and Lein, Alexander and Winter, Lina and Ettl, Tobias and Künzel, Julian and Reichert, Torsten E. and Spanier, Gerrit and Brochhausen, Christoph},
	title = {Tumor Immune Microenvironment Heterogeneity at the Invasion Front and Tumor Center in Oral Squamous Cell Carcinoma as a Perspective of Managing This Cancer Entity},
	year = {2023},
	journal = {Journal of Clinical Medicine},
	volume = {12},
	number = {4},
	doi = {10.3390/jcm12041704},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148934842&doi=10.3390%2fjcm12041704&partnerID=40&md5=092e69cd56fec749bdffe230a39ddcb2},
	abstract = {Background: Evaluating the tumor microenvironment and its influence on clinical management and therapy response is becoming increasingly important. However, only a few studies deal with the spatial distribution of immune cells within the tumor. This study aimed to describe the topology of immune cells in the microenvironment of oral squamous cell carcinoma (OSCC) sectioned by tumor invasion front and tumor center and to test their prognostic relevance regarding patient survival. Methods: A total of 55 OSCC patient specimens were collected retrospectively. The cancer tissue was immunohistochemically stained using an automated tissue stainer Ventana Benchmark Ultra (Roche) and analyzed using discrete expression marker profiles on immune cells. We investigated CD4+ lymphocytes, CD8+ lymphocytes, CD68+ macrophages, CD163+ macrophages, and M1 macrophages regarding their spatial distribution. Results: The statistical analysis revealed that the quantity and distribution of CD4+ (p = 0.007), CD8+ (p < 0.001), CD68+ (p < 0.001), CD163+ cells (p = 0.004), and M1 (p < 0.001) macrophages were significantly higher at the invasion front compared to the tumor center in all observed cases. However, high and low immune cell counts in the tumor center and invasion front were not associated with overall survival. Conclusion: Our results show two distinct immune microenvironments of the tumor center compared to the invasion front. Future studies are needed to explore how these results can be leveraged to improve patient therapy and outcome. © 2023 by the authors.},
	publication_stage = {Final}
}

@ARTICLE{Blobel2022,
	author = {Blobel, Bernd and Oemig, Frank and Ruotsalainen, Pekka and Lopez, Diego M.},
	title = {Transformation of Health and Social Care Systems—An Interdisciplinary Approach Toward a Foundational Architecture},
	year = {2022},
	journal = {Frontiers in Medicine},
	volume = {9},
	doi = {10.3389/fmed.2022.802487},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128281612&doi=10.3389%2ffmed.2022.802487&partnerID=40&md5=ce3c46b752826b6e19fb9869650580d6},
	abstract = {Objective: For realizing pervasive and ubiquitous health and social care services in a safe and high quality as well as efficient and effective way, health and social care systems have to meet new organizational, methodological, and technological paradigms. The resulting ecosystems are highly complex, highly distributed, and highly dynamic, following inter-organizational and even international approaches. Even though based on international, but domain-specific models and standards, achieving interoperability between such systems integrating multiple domains managed by multiple disciplines and their individually skilled actors is cumbersome. Methods: Using the abstract presentation of any system by the universal type theory as well as universal logics and combining the resulting Barendregt Cube with parameters and the engineering approach of cognitive theories, systems theory, and good modeling best practices, this study argues for a generic reference architecture model moderating between the different perspectives and disciplines involved provide on that system. To represent architectural elements consistently, an aligned system of ontologies is used. Results: The system-oriented, architecture-centric, and ontology-based generic reference model allows for re-engineering the existing and emerging knowledge representations, models, and standards, also considering the real-world business processes and the related development process of supporting IT systems for the sake of comprehensive systems integration and interoperability. The solution enables the analysis, design, and implementation of dynamic, interoperable multi-domain systems without requesting continuous revision of existing specifications. Copyright © 2022 Blobel, Oemig, Ruotsalainen and Lopez.},
	publication_stage = {Final}
}

@ARTICLE{Vetter2022283,
	author = {Vetter, Thomas R.},
	title = {Managing a perioperative medicine program},
	year = {2022},
	journal = {Best Practice and Research: Clinical Anaesthesiology},
	volume = {36},
	number = {2},
	pages = {283 – 298},
	doi = {10.1016/j.bpa.2022.04.008},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130542123&doi=10.1016%2fj.bpa.2022.04.008&partnerID=40&md5=4f001a17a22ed4aab1d883b69a4c5dfa},
	abstract = {Perioperative medicine is now a well-recognized albeit still evolving, interdisciplinary subspecialty, which encompasses a wide array of equally invested stakeholders and equally important contributors. The practice of perioperative medicine is fundamentally and optimally a collaborative effort, which aims to provide a comprehensive framework encompassing all aspects of the patient's surgical journey. Moving from a conceptual model of perioperative medicine to an operational perioperative medicine program and clinic requires a methodical management approach. This comprehensive management approach considers a variety of factors, such as defining the mission of a perioperative medicine program, expanding the role of the anesthesiologist and internal medicine hospitalist, recognizing the role of the advanced practice provider, stratifying perioperative management of surgical patients, developing and implementing a program, undertaking a clinical proof-of-concept pilot of a program, scaling up and building out a program, maximizing the electronic health record, leveraging telemedicine and virtual health, and providing adjunctive services. © 2022 Elsevier Ltd},
	publication_stage = {Final}
}

@ARTICLE{Jackson2023,
	author = {Jackson, David B. and Racz, Rebecca and Kim, Sarah and Brock, Stephan and Burkhart, Keith},
	title = {Rewiring Drug Research and Development through Human Data-Driven Discovery (HD3)},
	year = {2023},
	journal = {Pharmaceutics},
	volume = {15},
	number = {6},
	doi = {10.3390/pharmaceutics15061673},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163745962&doi=10.3390%2fpharmaceutics15061673&partnerID=40&md5=abb1c24165fa8b7136c36cf6ad744a64},
	abstract = {In an era of unparalleled technical advancement, the pharmaceutical industry is struggling to transform data into increased research and development efficiency, and, as a corollary, new drugs for patients. Here, we briefly review some of the commonly discussed issues around this counterintuitive innovation crisis. Looking at both industry- and science-related factors, we posit that traditional preclinical research is front-loading the development pipeline with data and drug candidates that are unlikely to succeed in patients. Applying a first principles analysis, we highlight the critical culprits and provide suggestions as to how these issues can be rectified through the pursuit of a Human Data-driven Discovery (HD3) paradigm. Consistent with other examples of disruptive innovation, we propose that new levels of success are not dependent on new inventions, but rather on the strategic integration of existing data and technology assets. In support of these suggestions, we highlight the power of HD3, through recently published proof-of-concept applications in the areas of drug safety analysis and prediction, drug repositioning, the rational design of combination therapies and the global response to the COVID-19 pandemic. We conclude that innovators must play a key role in expediting the path to a largely human-focused, systems-based approach to drug discovery and research. © 2023 by the authors.},
	publication_stage = {Final}
}

@ARTICLE{Post2022,
	author = {Post, Andrew R. and Burningham, Zachary and Halwani, Ahmad S.},
	title = {Electronic Health Record Data in Cancer Learning Health Systems: Challenges and Opportunities},
	year = {2022},
	journal = {JCO Clinical Cancer Informatics},
	volume = {6},
	doi = {10.1200/CCI.21.00158},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127268707&doi=10.1200%2fCCI.21.00158&partnerID=40&md5=5354378b84af6b8c5ae5633a71a56793},
	publication_stage = {Final}
}

@ARTICLE{Hu20231216,
	author = {Hu, Danqing and Li, Shaolei and Wu, Nan and Lu, Xudong},
	title = {A Multi-Modal Heterogeneous Graph Forest to Predict Lymph Node Metastasis of Non-Small Cell Lung Cancer},
	year = {2023},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	volume = {27},
	number = {3},
	pages = {1216 – 1224},
	doi = {10.1109/JBHI.2022.3233387},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147209625&doi=10.1109%2fJBHI.2022.3233387&partnerID=40&md5=57a969cbbde746e9f6822f92fd24bae4},
	abstract = {Lymph node metastasis (LNM) is critical for treatment decision-making for cancer patients, but it is difficult to diagnose accurately before surgery. Machine learning can learn nontrivial knowledge from multi-modal data to support accurate diagnosis. In this paper, we proposed a Multi-modal Heterogeneous Graph Forest (MHGF) approach to extract the deep representations of LNM from multi-modal data. Specifically, we first extracted the deep image features from CT images to represent the pathological anatomic extent of the primary tumor (pathological T stage) using a ResNet-Trans network. And then, a heterogeneous graph with six vertices and seven bi-directional relations was defined by medical experts to describe the possible relations between the clinical and image features. After that, we proposed a graph forest approach to construct the sub-graphs by removing each vertex in the complete graph iteratively. Finally, we used graph neural networks to learn the representations of each sub-graph in the forest to predict LNM and averaged all the prediction results as final results. We conducted experiments on 681 patients' multi-modal data. The proposed MHGF achieves the best performances with a 0.806 AUC value and 0.513 AP value compared with state-of-art machine learning and deep learning methods. The results indicate that the graph method can explore the relations between different types of features to learn effective deep representations for LNM prediction. Moreover, we found that the deep image features about the pathological anatomic extent of the primary tumor are useful for LNM prediction. And the graph forest approach can further improve the generalization ability and stability of the LNM prediction model. © 2013 IEEE.},
	publication_stage = {Final}
}@ARTICLE{Chang2020,
	author = {Chang, Kai-Po and Wang, John and Chang, Chi-Chang and Chu, Yen-Wei},
	title = {Development of a Novel Tool for the Retrieval and Analysis of Hormone Receptor Expression Characteristics in Metastatic Breast Cancer via Data Mining on Pathology Reports},
	year = {2020},
	journal = {BioMed Research International},
	volume = {2020},
	doi = {10.1155/2020/2654815},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086841604&doi=10.1155%2f2020%2f2654815&partnerID=40&md5=8053270f0e3d48ab54c327fd4b15cd31},
	abstract = {Information about the expression status of hormone receptors such as estrogen receptor (ER), progesterone receptor (PR), and Her-2 is crucial in the management and prognosis of breast cancer. Therefore, the retrieval and analysis of hormone receptor expression characteristics in metastatic breast cancer may be valuable in breast cancer study. Herein, we report a text mining tool based on word/phrase matching that retrieves hormone receptor expression data of regional or distant metastatic breast cancer from pathology reports. It was tested on pathology reports at the China Medical University Hospital from 2013 to 2018. The tool showed specificities of 91.6% and 63.3% for the detection of regional lymph node metastasis and distant metastasis, respectively. Sensitivity in immunohistochemical study result extraction in these cases was 98.6% for distant metastasis and 78.3% for regional lymph node metastasis. Statistical analysis on these retrieved data showed significant difference s in PR and Her-2 expressions between regional and metastatic breast cancer, which is compatible with previous studies. In conclusion, our study shows that metastatic breast cancer hormone receptor expression characteristics can be retrieved by text mining. The algorithm designed in this study may be useful in future studies about text mining in pathology reports.  © 2020 Kai-Po Chang et al.},
	publication_stage = {Final}
}

@CONFERENCE{Zheng20192001,
	author = {Zheng, Ling and Chen, Yan and Perl, Yehoshua and Halper, Michael and Geller, James and De Coronado, Sherri},
	title = {Quality Assurance of Concept Roles in the National Cancer Institute thesaurus},
	year = {2019},
	journal = {Proceedings - 2018 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2018},
	pages = {2001 – 2008},
	doi = {10.1109/BIBM.2018.8621277},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062559785&doi=10.1109%2fBIBM.2018.8621277&partnerID=40&md5=a5684bf2f3f9f199fce7345bc04cca4f},
	abstract = {Ontologies are integral components of many health information processing environments. Hence, quality assurance (QA) of the conceptual content of any ontology is essential. A methodology for facilitating the identification and correction of a kind of error called 'missing-role error' for ontology concepts is presented. Specifically, being a member of the 'null-role set' is used as an indicator that missing-role errors are likely. Our QA methodology was applied to the Biological Process hierarchy, one of the hierarchies of the National Cancer Institute thesaurus (NCIt), a prominent ontology in the biomedical domain. Two hypotheses pertaining to the effectiveness of our QA approach were investigated. Many missing-role errors were discovered and confirmed in our analysis. In comparison to a control sample, our QA approach yielded a statistically significant number of concepts with missing-role errors, confirming our two hypotheses. QA is a critical part of an ontology's life-cycle, and automated or semi-automated tools for supporting this process are invaluable. The presented approach is a useful addition to the arsenal of tools available to QA personnel. © 2018 IEEE.},
	publication_stage = {Final}
}

@ARTICLE{Sharma2020491,
	author = {Sharma, Ashish and Tarbox, Lawrence and Kurc, Tahsin and Bona, Jonathan and Smith, Kirk and Kathiravelu, Pradeeban and Bremer, Erich and Saltz, Joel H. and Prior, Fred},
	title = {PRISM: A platform for imaging in precision medicine},
	year = {2020},
	journal = {JCO Clinical Cancer Informatics},
	number = {4},
	pages = {491 – 499},
	doi = {10.1200/CCI.20.00001},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085853978&doi=10.1200%2fCCI.20.00001&partnerID=40&md5=8956ad6fe0545445110166f172193cc4},
	abstract = {PURPOSE Precision medicine requires an understanding of individual variability, which can only be acquired from large data collections such as those supported by the Cancer Imaging Archive (TCIA). We have undertaken a program to extend the types of data TCIA can support. This, in turn, will enable TCIA to play a key role in precision medicine research by collecting and disseminating high-quality, state-of-the-art, quantitative imaging data that meet the evolving needs of the cancer research community METHODS A modular technology platform is presented that would allow existing data resources, such as TCIA, to evolve into a comprehensive data resource that meets the needs of users engaged in translational research for imaging-based precision medicine. This Platform for Imaging in Precision Medicine (PRISM) helps streamline the deployment and improve TCIA's efficiency and sustainability. More importantly, its inherent modular architecture facilitates a piecemeal adoption by other data repositories. RESULTS PRISM includes services for managing radiology and pathology images and features and associated clinical data. A semantic layer is being built to help users explore diverse collections and pool data sets to create specialized cohorts. PRISM includes tools for image curation and de-identification. It includes image visualization and feature exploration tools. The entire platform is distributed as a series of containerized microservices with representational state transfer interfaces. CONCLUSION PRISM is helping modernize, scale, and sustain the technology stack that powers TCIA. Repositories can take advantage of individual PRISM services such as de-identification and quality control. PRISM is helping scale image informatics for cancer research at a time when the size, complexity, and demands to integrate image data with other precision medicine data-intensive commons are mounting. © 2020 by American Society of Clinical Oncology.},
	publication_stage = {Final}
}

@CONFERENCE{De Figueiredo2021165,
	author = {De Figueiredo, Elaine Barbosa and Dametto, Mariangela and De Franco Rosa, Ferrucio and Bonacin, Rodrigo},
	title = {A Multidimensional Framework for Semantic Electronic Health Records in Oncology Domain},
	year = {2021},
	journal = {Proceedings of the Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises, WETICE},
	volume = {2021-October},
	pages = {165 – 170},
	doi = {10.1109/WETICE53228.2021.00041},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125168881&doi=10.1109%2fWETICE53228.2021.00041&partnerID=40&md5=2c93c8e316e470fa928ff71f4bdbdda3},
	abstract = {Electronic Health Record (EHR) is a valuable tool to store patients' history as well as to promote evidence-based best practices and advances in healthcare. However, these benefits are limited due to problems related to data quality, difficulties in information retrieval, and semantic interoperability issues. We present the BioFrame aiming to standardize and semantically annotate EHR forms. Our proposal is composed of a multi-dimensional conceptual model, an EHR specification process, and a catalog of software tools. We expect to contribute with a framework that enables the efficient real-life application of semantic technologies and EHR standards to healthcare software systems. A case study was carried out on the specification of pediatric oncology nutrition records in a specialized hospital. The results point to the feasibility of BioFrame, as well as limitations, improvements, and challenges regarding the increase of productivity in the specification process.  © 2021 IEEE.},
	publication_stage = {Final}
}

@ARTICLE{Arteaga2019145,
	author = {Arteaga, Maria Clara and Escudero, Nayeli and Gasca-Pineda, Jaime and Bello-Bedoy, Rafael and Guilliams, C. Matt},
	title = {Genetic and phenotypic diversity of Branchinecta sandiegonensis (Crustacea: Anostraca) in the vernal pools of Baja California, México},
	year = {2019},
	journal = {Zootaxa},
	volume = {4646},
	number = {1},
	pages = {145 – 163},
	doi = {10.11646/zootaxa.4646.1.8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069605607&doi=10.11646%2fzootaxa.4646.1.8&partnerID=40&md5=ad20530eb2dd23a439e7d6b551b73b3b},
	abstract = {Branchinecta sandiegonensis is a passively dispersed species that occurs in the vernal pool complexes of southern California, the USA, and northwestern Baja California, México. The fragmented distribution of these vernal pool complexes could limit the gene flow, generating high genetic structure and morphometric variation across the landscape. Here we estimate the genetic and phenotypic variation of B. sandiegonensis in the southern part of its range. We sampled 15 vernal pools from four geographic regions of the Baja California Peninsula. We genotyped 150 individuals using nuclear microsatellites and 31 individuals using the mitochondrial COI region. We also conducted a morphometric analysis on a sample of 232 individuals. We found moderate levels of genetic diversity and different patterns of structure depending upon the spatial scale of analysis. Demographic models suggest contrasting trends among populations. Phenotypically, we found high levels of heterogeneity in body size of fairy shrimps within and across the regions. Our findings highlight that vernal pools in Baja California are important reservoirs of genetic and phenotypic diversity for B. sandiegonensis. The interplay between gene flow and genetic drift may have influenced the patterns we detected in the southern part of the range of this species. Copyright © 2019 Magnolia Press},
	publication_stage = {Final}
}

@ARTICLE{Zitt201925,
	author = {Zitt, Michel and Lelu, Alain and Cadot, Martine and Cabanac, Guillaume},
	title = {Bibliometric delineation of scientific fields},
	year = {2019},
	journal = {Springer Handbooks},
	pages = {25 – 68},
	doi = {10.1007/978-3-030-02511-3_2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075943657&doi=10.1007%2f978-3-030-02511-3_2&partnerID=40&md5=4828d0102d1a7e91d722ac05e9c90775},
	abstract = {Delineation of scientific domains (fields, areas of science) is a preliminary task in bibliometric studies at the mesolevel, far from straightforward in domains with high multidisciplinarity, variety, and instability. The Sect. 2.2 shows the connection of the delineation problem to the question of disciplines versus invisible colleges, through three combinable models: ready-made classifications of science, classical information-retrieval searches, mapping and clustering. They differ in the role and modalities of supervision. The Sect. 2.3 sketches various bibliometric techniques against the background of information retrieval (information retrieval (IR)), data analysis, and network theory, showing both their power and their limitations in delineation processes. The role and modalities of supervision are emphasized. The Sect. 2.4 addresses the comparison and combination of bibliometric networks (actors, texts, citations) and the various ways to hybridize. In the Sect. 2.5, typical protocols and further questions are proposed. © Springer Nature Switzerland AG 2019.},
	publication_stage = {Final}
}

@ARTICLE{Xie2017245,
	author = {Xie, Lei and Draizen, Eli J. and Bourne, Philip E.},
	title = {Harnessing Big Data for Systems Pharmacology},
	year = {2017},
	journal = {Annual Review of Pharmacology and Toxicology},
	volume = {57},
	pages = {245 – 262},
	doi = {10.1146/annurev-pharmtox-010716-104659},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009135184&doi=10.1146%2fannurev-pharmtox-010716-104659&partnerID=40&md5=2cd3f360bed8f1845a0eb1975891eb76},
	abstract = {Systems pharmacology aims to holistically understand mechanisms of drug actions to port drug discovery and clinical practice. Systems pharmacology modeling (SPM) is data driven. It integrates an exponentially growing amount of data at multiple scales (genetic, molecular, cellular, organismal, and environmental). The goal of SPM is to develop mechanistic or predictive multiscale models that are interpretable and actionable. The current explosions in genomics and other omics data, as well as the tremendous advances in big data technologies, have already enabled biologists to generate novel hypotheses and gain new knowledge through computational models of genome-wide, heterogeneous, and dynamic data sets. More work is needed to interpret and predict a drug response phenotype, which is dependent on many known and unknown factors. To gain a comprehensive understanding of drug actions, SPM requires close collaborations between domain experts from diverse fields and integration of heterogeneous models from biophysics, mathematics, statistics, machine learning, and semantic webs. This creates challenges in model management, model integration, model translation, and knowledge integration. In this review, we discuss several emergent issues in SPM and potential solutions using big data technology and analytics. The concurrent development of high-throughput techniques, cloud computing, data science, and the semantic web will likely allow SPM to be findable, accessible, interoperable, reusable, reliable, interpretable, and actionable. © 2017 by Annual Reviews. All rights reserved.},
	publication_stage = {Final}
}

@CONFERENCE{Miranda-Escalada2020303,
	author = {Miranda-Escalada, Antonio and Farré, Eulàlia and Krallinger, Martin},
	title = {Named entity recognition, concept normalization and clinical coding: Overview of the cantemist track for cancer text mining in Spanish, corpus, guidelines, methods and results},
	year = {2020},
	journal = {CEUR Workshop Proceedings},
	volume = {2664},
	pages = {303 – 323},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092250425&partnerID=40&md5=7500f9ab4e4566e738599690a3bc0ada},
	abstract = {Cancer still represents one of the leading causes of death worldwide, resulting in a considerable healthcare impact. Recent research efforts from the clinical and molecular oncology scientific communities were able to increase considerably life expectancy of patients for some cancer types. Most of the current cancer diagnoses are primarily determined by pathology laboratories, providing an essential source for information to guide the treatment of patients with cancer. Pathology observations essentially characterize the results of microscopic or macroscopic studies of cells or tissues following a biopsy or surgery. Clinicians and researchers alike, require systems that automatically detect, read and generate structured data representations from pathology examinations. The resulting structured or coded clinical information, normalized using controlled vocabularies like the ICD-O or SNOMED-CT is critical for large-scale analysis of specific tumor types or to determine response to specific treatments or prognosis. Text mining and NLP approaches are showing promising results to transform medical text into useful clinical information, bridging the gap between free-text and structured representation of clinical information. Nonetheless, in the case of cancer text mining approaches, most efforts were exclusively focused on medical records in English. Moreover, due to the lack of high quality manually labeled clinical texts annotated by oncology experts most previous efforts, even for English relied mainly on customized dictionaries of names or rules to recognize clinical concept mentions despite the promising results of advanced deep learning technologies. To address these issues we have organized the Cantemist (CANcer TExt Mining Shared Task) track at IberLEF 2020. It represents the first community effort to evaluate and promote the development of resources for named entity recognition, concept normalization and clinical coding specifically focusing on cancer data in Spanish. Evaluation of participating systems was done using the Cantemist corpus, a publicly accessible dataset (together with annotation consistency analysis and guidelines) of manually annotated mentions of tumor morphology entities and their mappings to the Spanish version of ICD-O.We received a total of 121 systems or runs from 25 teams for one of the three Cantemist sub-tasks, obtaining very competitive results. Most participants implemented sophisticated AI approaches; mainly deep learning algorithms based on Long-Short Term Memory Units and language models (BERT, BETO, RoBERTa, etc) with a classifier layer such as a Conditional Random Field. In addition to using pre-trained language models, word and character embeddings were also explored. Cantemist corpus: Https://doi.org/10.5281/zenodo.3773228. © 2020 Copyright for this paper by its authors. Use permitted under.},
	publication_stage = {Final}
}

@ARTICLE{Moreno2019437,
	author = {Moreno, Pedro T. Nevado-Batalla},
	title = {Administration 4.0: The challenge of institutional competitiveness as a requisite for development},
	year = {2019},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {801},
	pages = {437 – 443},
	doi = {10.1007/978-3-319-99608-0_61},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061697340&doi=10.1007%2f978-3-319-99608-0_61&partnerID=40&md5=f6114f3045146026c0c4f659a57994bf},
	abstract = {Public Administration must live up to the standards of the new environment 4.0. This economic-industrial paradigm is concerned with the whole society. The need for modernization in Public Administration brings to light the directly proportional relationship between institutional and economic competitiveness. © Springer Nature Switzerland AG 2019.},
	publication_stage = {Final}
}

@ARTICLE{Shahid2019638,
	author = {Shahid, Afzal Hussain and Singh, M.P.},
	title = {Computational intelligence techniques for medical diagnosis and prognosis: Problems and current developments},
	year = {2019},
	journal = {Biocybernetics and Biomedical Engineering},
	volume = {39},
	number = {3},
	pages = {638 – 672},
	doi = {10.1016/j.bbe.2019.05.010},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068056224&doi=10.1016%2fj.bbe.2019.05.010&partnerID=40&md5=beed56b34a8a365e1f9bcdd42df5076e},
	abstract = {Diagnosis, being the first step in medical practice, is very crucial for clinical decision making. This paper investigates state-of-the-art computational intelligence (CI) techniques applied in the field of medical diagnosis and prognosis. The paper presents the performance of these techniques in diagnosing different diseases along with the detailed description of the data used. This paper includes basic as well as hybrid CI techniques that have been used in recent years so as to know the current trends in medical diagnosis domain. The paper presents the merits and demerits of different techniques in general as well as application specific context. This paper discusses some critical issues related to the medical diagnosis and prognosis such as uncertainties in the medical domain, problems in the medical data especially dealing with time-stamped (temporal) data, and knowledge acquisition. Moreover, this paper also discusses the features of good CI techniques in medical diagnosis. Overall, this review provides new insight for future research requirements in the medical diagnosis domain. © 2019 Nalecz Institute of Biocybernetics and Biomedical Engineering of the Polish Academy of Sciences},
	publication_stage = {Final}
}

@ARTICLE{Ochs201663,
	author = {Ochs, Christopher and He, Zhe and Zheng, Ling and Geller, James and Perl, Yehoshua and Hripcsak, George and Musen, Mark A.},
	title = {Utilizing a structural meta-ontology for family-based quality assurance of the BioPortal ontologies},
	year = {2016},
	journal = {Journal of Biomedical Informatics},
	volume = {61},
	pages = {63 – 76},
	doi = {10.1016/j.jbi.2016.03.007},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962224558&doi=10.1016%2fj.jbi.2016.03.007&partnerID=40&md5=fddade28b3c7895d4fc7e61585ec3122},
	abstract = {An Abstraction Network is a compact summary of an ontology's structure and content. In previous research, we showed that Abstraction Networks support quality assurance (QA) of biomedical ontologies. The development of an Abstraction Network and its associated QA methodologies, however, is a labor-intensive process that previously was applicable only to one ontology at a time. To improve the efficiency of the Abstraction-Network-based QA methodology, we introduced a QA framework that uses uniform Abstraction Network derivation techniques and QA methodologies that are applicable to whole families of structurally similar ontologies. For the family-based framework to be successful, it is necessary to develop a method for classifying ontologies into structurally similar families. We now describe a structural meta-ontology that classifies ontologies according to certain structural features that are commonly used in the modeling of ontologies (e.g., object properties) and that are important for Abstraction Network derivation. Each class of the structural meta-ontology represents a family of ontologies with identical structural features, indicating which types of Abstraction Networks and QA methodologies are potentially applicable to all of the ontologies in the family. We derive a collection of 81 families, corresponding to classes of the structural meta-ontology, that enable a flexible, streamlined family-based QA methodology, offering multiple choices for classifying an ontology. The structure of 373 ontologies from the NCBO BioPortal is analyzed and each ontology is classified into multiple families modeled by the structural meta-ontology. © 2016 Elsevier Inc.},
	publication_stage = {Final}
}

@ARTICLE{Paine2020335,
	author = {Paine, Drew and Lee, Charlotte P.},
	title = {Coordinative Entities: Forms of Organizing in Data Intensive Science},
	year = {2020},
	journal = {Computer Supported Cooperative Work: CSCW: An International Journal},
	volume = {29},
	number = {3},
	pages = {335 – 380},
	doi = {10.1007/s10606-020-09372-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079416466&doi=10.1007%2fs10606-020-09372-2&partnerID=40&md5=0534599b02449253c44256d7ed5ec0fe},
	abstract = {Scientific collaboration is a long-standing subject of CSCW scholarship that typically focuses on the development and use of computing systems to facilitate research. The research presented in this article investigates the sociality of science by identifying and describing particular, common forms of organizing that researchers in four different scientific realms employ to conduct work in both local contexts and as part of distributed, global projects. This paper introduces five prototypical forms of organizing we categorize as coordinative entities: the Principal Group, Intermittent Exchange, Sustained Aggregation, Federation, and Facility Organization. Coordinative entities as a categorization help specify, articulate, compare, and trace overlapping and evolving arrangements scientists use to facilitate data intensive research. We use this typology to unpack complexities of data intensive scientific collaboration in four cases, showing how scientists invoke different coordinative entities across three types of research activities: data collection, processing, and analysis. Our contribution scrutinizes the sociality of scientific work to illustrate how these actors engage in relational work within and among diverse, dispersed forms of organizing across project, funding, and disciplinary boundaries. © 2020, Springer Nature B.V.},
	publication_stage = {Final}
}

@ARTICLE{Crangle2018,
	author = {Crangle, Colleen E. and Bradley, Colin and Carlin, Paul F. and Esterhay, Robert J. and Harper, Roy and Kearney, Patricia M. and McCarthy, Vera J.C. and McTear, Michael F. and Savage, Eileen and Tuttle, Mark S. and Wallace, Jonathan G.},
	title = {Exploring patient information needs in type 2 diabetes: A cross sectional study of questions},
	year = {2018},
	journal = {PLoS ONE},
	volume = {13},
	number = {11},
	doi = {10.1371/journal.pone.0203429},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056590722&doi=10.1371%2fjournal.pone.0203429&partnerID=40&md5=8686cd769f770f7238c915f6f3fcf183},
	abstract = {This study set out to analyze questions about type 2 diabetes mellitus (T2DM) from patients and the public. The aim was to better understand people's information needs by starting with what they do not know, discovered through their own questions, rather than starting with what we know about T2DM and subsequently finding ways to communicate that information to people affected by or at risk of the disease. One hundred and sixty-four questions were collected from 120 patients attending outpatient diabetes clinics and 300 questions from 100 members of the public through the Amazon Mechanical Turk crowdsourcing platform. Twenty-three general and diabetes-specific topics and five phases of disease progression were identified; these were used to manually categorize the questions. Analyses were performed to determine which topics, if any, were significant predictors of a question's being asked by a patient or the public, and similarly for questions from a woman or a man. Further analysis identified the individual topics that were assigned significantly more often to the crowdsourced or clinic questions. These were Causes (CI: [-0.07, -0.03], p < .001), Risk Factors ([-0.08, -0.03], p < .001), Prevention ([-0.06, -0.02], p < .001), Diagnosis ([-0.05, -0.02], p < .001), and Distribution of a Disease in a Population ([-0.05,-0.01], p = .0016) for the crowdsourced questions and Treatment ([0.03, 0.01], p = .0019), Disease Complications ([0.02, 0.07], p < .001), and Psychosocial ([0.05, 0.1], p < .001) for the clinic questions. No highly significant gender-specific topics emerged in our study, but questions about Weight were more likely to come from women and Psychosocial questions from men. There were significantly more crowdsourced questions about the time Prior to any Diagnosis ([(-0.11, -0.04], p = .0013) and significantly more clinic questions about Health Maintenance and Prevention after diagnosis ([0.07. 0.17], p < .001). A descriptive analysis pointed to the value provided by the specificity of questions, their potential to disclose emotions behind questions, and the as-yet unrecognized information needs they can reveal. Large-scale collection of questions from patients across the spectrum of T2DM progression and from the public-a significant percentage of whom are likely to be as yet undiagnosed-is expected to yield further valuable insights. © 2018 Crangle et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
	publication_stage = {Final}
}

@CONFERENCE{Zheng20191948,
	author = {Zheng, Fengbo and Abeysinghe, Rashmie and Cui, Licong},
	title = {A Hybrid Method to Detect Missing Hierarchical Relations in NCI Thesaurus},
	year = {2019},
	journal = {Proceedings - 2019 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2019},
	pages = {1948 – 1953},
	doi = {10.1109/BIBM47256.2019.8983265},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084335978&doi=10.1109%2fBIBM47256.2019.8983265&partnerID=40&md5=3fd41bc21c805e34c60a3fc7e3d62969},
	abstract = {Biomedical terminologies such as National Cancer Institute thesaurus (NCIt) have been widely used in supporting various biomedical research and applications. Therefore, the quality of biomedical terminologies directly impacts their downstream applications. In this paper, we introduce a hybrid method to identify missing hierarchical IS-A relations in NCIt, by leveraging both role definitions and lexical features of concepts in non-lattice subgraphs. We first extract non-lattice subgraphs in NCIt, problematic areas with quality issue. We model each concept using its role definitions and words in its concept name as well as words in the names of its ancestors. Then we perform a two-step subsumption testing for candidate pairs of concepts in the non-lattice subgraphs to automatically suggest potentially missing IS-A relations. We applied our method to the 19.01d version of NCIt. A total of 9,512 non-lattice subgraphs were extracted, among which 654 of them revealed 268 potentially missing IS-A relations. After the removal of duplication and redundancy, 121 potentially missing IS-A relations were obtained. To evaluate our method, we adopted a retrospective ground truth (RGT)-based idea to use version difference as the reference standard. We constructed a reference standard based on the IS-A changes between the 19.01d and 19.07e versions of NCIt. Among 121 potentially missing IS-A relations suggested by our method, 46 out of them are valid according to the reference standard. The RGT-based evaluation indicates that our hybrid method is promising in detecting missing IS-A relations which motivates us to perform a thorough evaluation by domain experts in future work. © 2019 IEEE.},
	publication_stage = {Final}
}

@ARTICLE{Alonso-Calvo2017179,
	author = {Alonso-Calvo, Raul and Paraiso-Medina, Sergio and Perez-Rey, David and Alonso-Oset, Enrique and van Stiphout, Ruud and Yu, Sheng and Taylor, Marian and Buffa, Francesca and Fernandez-Lozano, Carlos and Pazos, Alejandro and Maojo, Victor},
	title = {A semantic interoperability approach to support integration of gene expression and clinical data in breast cancer},
	year = {2017},
	journal = {Computers in Biology and Medicine},
	volume = {87},
	pages = {179 – 186},
	doi = {10.1016/j.compbiomed.2017.06.005},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020462913&doi=10.1016%2fj.compbiomed.2017.06.005&partnerID=40&md5=38d5e5ad7165cddcf1c049b6b1c64018},
	abstract = {Introduction The introduction of omics data and advances in technologies involved in clinical treatment has led to a broad range of approaches to represent clinical information. Within this context, patient stratification across health institutions due to omic profiling presents a complex scenario to carry out multi-center clinical trials. Methods This paper presents a standards-based approach to ensure semantic integration required to facilitate the analysis of clinico-genomic clinical trials. To ensure interoperability across different institutions, we have developed a Semantic Interoperability Layer (SIL) to facilitate homogeneous access to clinical and genetic information, based on different well-established biomedical standards and following International Health (IHE) recommendations. Results The SIL has shown suitability for integrating biomedical knowledge and technologies to match the latest clinical advances in healthcare and the use of genomic information. This genomic data integration in the SIL has been tested with a diagnostic classifier tool that takes advantage of harmonized multi-center clinico-genomic data for training statistical predictive models. Conclusions The SIL has been adopted in national and international research initiatives, such as the EURECA-EU research project and the CIMED collaborative Spanish project, where the proposed solution has been applied and evaluated by clinical experts focused on clinico-genomic studies. © 2017 Elsevier Ltd},
	publication_stage = {Final}
}

@ARTICLE{Yoon2019,
	author = {Yoon, Seyeol and Lee, Doheon},
	title = {Meta-path Based Prioritization of Functional Drug Actions with Multi-Level Biological Networks},
	year = {2019},
	journal = {Scientific Reports},
	volume = {9},
	number = {1},
	doi = {10.1038/s41598-019-41814-w},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063747670&doi=10.1038%2fs41598-019-41814-w&partnerID=40&md5=0a1f9103154d72bcb5df5efec75e1589},
	abstract = {Functional drug actions refer to drug-affected GO terms. They aid in the investigation of drug effects that are therapeutic or adverse. Previous studies have utilized the linkage information between drugs and functions in molecular level biological networks. Since the current knowledge of molecular level mechanisms of biological functions is still limited, such previous studies were incomplete. We expected that the multi-level biological networks would allow us to more completely investigate the functional drug actions. We constructed multi-level biological networks with genes, GO terms, and diseases. Meta-paths were utilized to extract the features of each GO term. We trained 39 SVM models to prioritize the functional drug actions of the various 39 drugs. Through the multi-level networks, more functional drug actions were utilized for the 39 models and inferred by the models. Multi-level based features improved the performance of the models, and the average AUROC value in the cross-validation was 0.86. Moreover, 60% of the candidates were true. © 2019, The Author(s).},
	publication_stage = {Final}
}

@ARTICLE{Petersen2016666,
	author = {Petersen, Alexander M. and Rotolo, Daniele and Leydesdorff, Loet},
	title = {A triple helix model of medical innovation: Supply, demand, and technological capabilities in terms of Medical Subject Headings},
	year = {2016},
	journal = {Research Policy},
	volume = {45},
	number = {3},
	pages = {666 – 681},
	doi = {10.1016/j.respol.2015.12.004},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977539627&doi=10.1016%2fj.respol.2015.12.004&partnerID=40&md5=a06c930e15ce07ce34b50a7e8208cd18},
	abstract = {We develop a model of innovation that enables us to trace the interplay among three key dimensions of the innovation process: (i) demand of and (ii) supply for innovation, and (iii) technological capabilities available to generate innovation in the forms of products, processes, and services. Building on triple helix research, we use entropy statistics to elaborate an indicator of mutual information among these dimensions that can provide indication of reduction of uncertainty. To do so, we focus on the medical context, where uncertainty poses significant challenges to the governance of innovation. We use the Medical Subject Headings (MeSH) of MEDLINE/PubMed to identify publications within the categories “Diseases” (C), “Drugs and Chemicals” (D), “Analytic, Diagnostic, and Therapeutic Techniques and Equipment” (E) and use these as knowledge representations of demand, supply, and technological capabilities, respectively. Three case-studies of medical research areas are used as representative ‘entry perspectives’ of the medical innovation process. These are: (i) human papilloma virus, (ii) RNA interference, and (iii) magnetic resonance imaging. We find statistically significant periods of synergy among demand, supply, and technological capabilities (C-D-E) that point to three-dimensional interactions as a fundamental perspective for the understanding and governance of the uncertainty associated with medical innovation. Among the pairwise configurations in these contexts, the demand–technological capabilities (C-E) provided the strongest link, followed by the supply–demand (D-C) and the supply–technological capabilities (D-E) channels. © 2015 Elsevier B.V.},
	publication_stage = {Final}
}

@ARTICLE{Warner2016701,
	author = {Warner, Jeremy L. and Rioth, Matthew J. and Mandl, Kenneth D. and Mandel, Joshua C. and Kreda, David A. and Kohane, Isaac S. and Carbone, Daniel and Oreto, Ross and Wang, Lucy and Zhu, Shilin and Yao, Heming and Alterovitz, Gil},
	title = {SMART precision cancer medicine: A FHIR-based app to provide genomic information at the point of care},
	year = {2016},
	journal = {Journal of the American Medical Informatics Association},
	volume = {23},
	number = {4},
	pages = {701 – 710},
	doi = {10.1093/jamia/ocw015},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981274175&doi=10.1093%2fjamia%2focw015&partnerID=40&md5=d3fb872a3f3330d8c64a8e6a5930c1ab},
	abstract = {Background: Precision cancer medicine (PCM) will require ready access to genomic data within the clinical workflow and tools to assist clinical interpretation and enable decisions. Since most electronic health record (EHR) systems do not yet provide such functionality, we developed an EHRagnostic, clinico-genomic mobile app to demonstrate several features that will be needed for point-of-care conversations. Methods: Our prototype, called Substitutable Medical Applications and Reusable Technology (SMART)VR PCM, visualizes genomic information in real time, comparing a patient's diagnosis-specific somatic gene mutations detected by PCR-based hotspot testing to a population-level set of comparable data. The initial prototype works for patient specimens with 0 or 1 detected mutation. Genomics extensions were created for the Health Level SevenVR Fast Healthcare Interoperability Resources (FHIR)VR standard; otherwise, the prototype is a normal SMART on FHIR app. Results: The PCM prototype can rapidly present a visualization that compares a patient's somatic genomic alterations against a distribution built from more than 3000 patients, along with context-specific links to external knowledge bases. Initial evaluation by oncologists provided important feedback about the prototype's strengths and weaknesses. We added several requested enhancements and successfully demonstrated the app at the inaugural American Society of Clinical Oncology Interoperability Demonstration; we have also begun to expand visualization capabilities to include cancer specimens with multiple mutations. Discussion: PCM is open-source software for clinicians to present the individual patient within the population-level spectrum of cancer somatic mutations. The app can be implemented on any SMART on FHIR-enabled EHRs, and future versions of PCM should be able to evolve in parallel with external knowledge bases. © The Author 2016. Published by Oxford University Press on behalf of the American Medical Informatics Association.},
	publication_stage = {Final}
}

@ARTICLE{Houde2019,
	author = {Houde, Aimee Lee S. and Günther, Oliver P. and Strohm, Jeffrey and Ming, Tobi J. and Li, Shaorong and Kaukinen, Karia H. and Patterson, David A. and Farrell, Anthony P. and Hinch, Scott G. and Miller, Kristina M.},
	title = {Discovery and validation of candidate smoltification gene expression biomarkers across multiple species and ecotypes of Pacific salmonids},
	year = {2019},
	journal = {Conservation Physiology},
	volume = {7},
	number = {1},
	doi = {10.1093/conphys/coz051},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081056415&doi=10.1093%2fconphys%2fcoz051&partnerID=40&md5=a78eff30928f82b92cb81b6c323a9d5b},
	abstract = {Early marine survival of juvenile salmon is intimately associated with their physiological condition during smoltification and ocean entry. Smoltification (parr-smolt transformation) is a developmental process that allows salmon to acquire seawater tolerance in preparation for marine living. Traditionally, this developmental process has been monitored using gill Na+/K+-ATPase (NKA) activity or plasma hormones, but gill gene expression offers the possibility of another method. Here, we describe the discovery of candidate genes from gill tissue for staging smoltification using comparisons of microarray studies with particular focus on the commonalities between anadromous Rainbow trout and Sockeye salmon datasets, as well as a literature comparison encompassing more species. A subset of 37 candidate genes mainly from the microarray analyses was used for TaqMan quantitative PCR assay design and their expression patterns were validated using gill samples from four groups, representing three species and two ecotypes: Coho salmon, Sockeye salmon, stream-type Chinook salmon and ocean-type Chinook salmon. The best smoltification biomarkers, as measured by consistent changes across these four groups, were genes involved in ion regulation, oxygen transport and immunity. Smoltification gene expression patterns (using the top 10 biomarkers) were confirmed by significant correlations with NKA activity and were associated with changes in body brightness, caudal fin darkness and caudal peduncle length. We incorporate gene expression patterns of pre-smolt, smolt and de-smolt trials from acute seawater transfers from a companion study to develop a preliminary seawater tolerance classification model for ocean-type Chinook salmon. This work demonstrates the potential of gene expression biomarkers to stage smoltification and classify juveniles as pre-smolt, smolt or de-smolt. © 2019 The Author(s) 2019. Published by Oxford University Press and the Society for Experimental Biology.},
	publication_stage = {Final}
}

@BOOK{Dalianis20181,
	author = {Dalianis, Hercules},
	title = {Clinical text mining: Secondary use of electronic patient records},
	year = {2018},
	journal = {Clinical Text Mining: Secondary Use of Electronic Patient Records},
	pages = {1 – 181},
	doi = {10.1007/978-3-319-78503-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062487617&doi=10.1007%2f978-3-319-78503-5&partnerID=40&md5=af7722dee2ab528fcf548bcf42f90aa9},
	abstract = {This open access book describes the results of natural language processing and machine learning methods applied to clinical text from electronic patient records. It is divided into twelve chapters. Chapters 1-4 discuss the history and background of the original paper-based patient records, their purpose, and how they are written and structured. These initial chapters do not require any technical or medical background knowledge. The remaining eight chapters are more technical in nature and describe various medical classifications and terminologies such as ICD diagnosis codes, SNOMED CT, MeSH, UMLS, and ATC. Chapters 5-10 cover basic tools for natural language processing and information retrieval, and how to apply them to clinical text. The difference between rule-based and machine learning-based methods, as well as between supervised and unsupervised machine learning methods, are also explained. Next, ethical concerns regarding the use of sensitive patient records for research purposes are discussed, including methods for de-identifying electronic patient records and safely storing patient records. The book's closing chapters present a number of applications in clinical text mining and summarise the lessons learned from the previous chapters. The book provides a comprehensive overview of technical issues arising in clinical text mining, and offers a valuable guide for advanced students in health informatics, computational linguistics, and information retrieval, and for researchers entering these fields. © The Editor(s) (if applicable) and The Author(s) 2018.. All rights reserved.},
	publication_stage = {Final}
}

@CONFERENCE{Zheng20201757,
	author = {Zheng, Fengbo and Cui, Licong},
	title = {A Lexical-based Formal Concept Analysis Method to Identify Missing Concepts in the NCI Thesaurus},
	year = {2020},
	journal = {Proceedings - 2020 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020},
	pages = {1757 – 1760},
	doi = {10.1109/BIBM49941.2020.9313186},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100342374&doi=10.1109%2fBIBM49941.2020.9313186&partnerID=40&md5=cea01b186b82a2c490dccf0ba35cbf1a},
	abstract = {Biomedical terminologies have been increasingly used in modern biomedical research and applications to facilitate data management and ensure semantic interoperability. As part of the evolution process, new concepts are regularly added to biomedical terminologies in response to the evolving domain knowledge and emerging applications. Most existing concept enrichment methods suggest new concepts via directly importing knowledge from external sources. In this paper, we introduced a lexical method based on formal concept analysis (FCA) to identify potentially missing concepts in a given terminology by leveraging its intrinsic knowledge - concept names. We first construct the FCA formal context based on the lexical features of concepts. Then we perform multistage intersection to formalize new concepts and detect potentially missing concepts. We applied our method to the Disease or Disorder sub-hierarchy in the National Cancer Institute (NCI) Thesaurus (19.08d version) and identified a total of 8,983 potentially missing concepts. As a preliminary evaluation of our method to validate the potentially missing concepts, we further checked whether they were included in any external source terminology in the Unified Medical Language System (UMLS). The result showed that 592 out of 8,937 potentially missing concepts were found in the UMLS. © 2020 IEEE.},
	publication_stage = {Final}
}

@ARTICLE{Sheeba2020263,
	author = {Sheeba, T. and Krishnan, Reshmy},
	title = {A Semantic Approach of Building Dynamic Learner Profile Model Using WordNet},
	year = {2020},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {1082},
	pages = {263 – 272},
	doi = {10.1007/978-981-15-1081-6_22},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081166866&doi=10.1007%2f978-981-15-1081-6_22&partnerID=40&md5=468e428d530452381ab6f374d969424b},
	abstract = {The learners’ interest forms the essential characteristics of the learner profile in various applications, such as information retrieval, classification, and recommender systems. This paper proposes a method to improve learner interest extraction from the frequently used documents of the learner by exploring the concept of WordNet. Initially, the web log files of each learner are obtained from the learning management system, and then the frequently visited documents of each learner are downloaded and processed to identify domain-related words. The learner’s interest is then extracted initially using the standard vector space model and then improved using the semantic-based representation of WordNet. The WordNet identifies a set of semantic concepts related to the document words. To select the appropriate meaning of a word from a set of concepts, “Word Sense Disambiguation (WSD)” semantic similarity algorithm is used. The experiments were performed in NetBeans IDE using Java language and WordNet 2.1. The effect of the proposed method is examined with classification experiments, and the result proved that the use of WordNet concepts in learner interest retrieval shows better classification performance than compared to the existing method of term representation, thereby obtaining a classification accuracy of 89%. © 2020, Springer Nature Singapore Pte Ltd.},
	publication_stage = {Final}
}

@ARTICLE{Ahmed2021261,
	author = {Ahmed, Mobyen Uddin and Barua, Shaibal and Begum, Shahina},
	title = {Artificial intelligence, machine learning and reasoning in health informatics—case studies},
	year = {2021},
	journal = {Intelligent Systems Reference Library},
	volume = {192},
	pages = {261 – 291},
	doi = {10.1007/978-3-030-54932-9_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092367036&doi=10.1007%2f978-3-030-54932-9_12&partnerID=40&md5=f363ccd8432b8de0be917dcd267abfdc},
	abstract = {To apply Artificial Intelligence (AI), Machine Learning (ML) and Machine Reasoning (MR) in health informatics are often challenging as they comprise with multivariate information coming from heterogeneous sources e.g. sensor signals, text, etc. This book chapter presents the research development of AI, ML and MR as applications in health informatics. Five case studies on health informatics have been discussed and presented as (1) advanced Parkinson’s disease, (2) stress management, (3) postoperative pain treatment, (4) driver monitoring, and (5) remote health monitoring. Here, the challenges, solutions, models, results, limitations are discussed with future wishes. © 2021, Springer Nature Switzerland AG.},
	publication_stage = {Final}
}

@CONFERENCE{Bucur2019179,
	author = {Bucur, Cristina-Iulia and Kuhn, Tobias and Ceolin, Davide},
	title = {Peer reviewing revisited: Assessing research with interlinked semantic comments},
	year = {2019},
	journal = {K-CAP 2019 - Proceedings of the 10th International Conference on Knowledge Capture},
	pages = {179 – 187},
	doi = {10.1145/3360901.3364434},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077290870&doi=10.1145%2f3360901.3364434&partnerID=40&md5=c8cec2fcd4cf5ac8a3ef4c7ac13b1acb},
	abstract = {Scientific publishing seems to be at a turning point. Its paradigm has stayed basically the same for 300 years but is now challenged by the increasing volume of articles that makes it very hard for scientists to stay up to date in their respective fields. In fact, many have pointed out serious flaws of current scientific publishing practices, including the lack of accuracy and efficiency of the reviewing process. To address some of these problems, we apply here the general principles of the Web and the Semantic Web to scientific publishing, focusing on the reviewing process. We want to determine if a fine-grained model of the scientific publishing workflow can help us make the reviewing processes better organized and more accurate, by ensuring that review comments are created with formal links and semantics from the start. Our contributions include a novel model called Linkflows that allows for such detailed and semantically rich representations of reviews and the reviewing processes. We evaluate our approach on a manually curated dataset from several recent Computer Science journals and conferences that come with open peer reviews. We gathered ground-truth data by contacting the original reviewers and asking them to categorize their own review comments according to our model. Comparing this ground truth to answers provided by model experts, peers, and automated techniques confirms that our approach of formally capturing the reviewers' intentions from the start prevents substantial discrepancies compared to when this information is later extracted from the plain-text comments. In general, our analysis shows that our model is well understood and easy to apply, and it revealed the semantic properties of such review comments. © 2019 ACM.},
	publication_stage = {Final}
}

@ARTICLE{Shamimul Hasan20201952,
	author = {Shamimul Hasan, S.M. and Rivera, Donna and Wu, Xiao-Cheng and Durbin, Eric B. and Christian, J. Blair and Tourassi, Georgia},
	title = {Knowledge Graph-Enabled Cancer Data Analytics},
	year = {2020},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	volume = {24},
	number = {7},
	pages = {1952 – 1967},
	doi = {10.1109/JBHI.2020.2990797},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087530486&doi=10.1109%2fJBHI.2020.2990797&partnerID=40&md5=afca5d3089825856a56cc3f605553ddb},
	abstract = {Cancer registries collect unstructured and structured cancer data for surveillance purposes which provide important insights regarding cancer characteristics, treatments, and outcomes. Cancer registry data typically (1) categorize each reportable cancer case or tumor at the time of diagnosis, (2) contain demographic information about the patient such as age, gender, and location at time of diagnosis, (3) include planned and completed primary treatment information, and (4) may contain survival outcomes. As structured data is being extracted from various unstructured sources, such as pathology reports, radiology reports, medical records, and stored for reporting and other needs, the associated information representing a reportable cancer is constantly expanding and evolving. While some popular analytic approaches including SEER*Stat and SAS exist, we provide a knowledge graph approach to organizing cancer registry data. Our approach offers unique advantages for timely data analysis and presentation and visualization of valuable information. This knowledge graph approach semantically enriches the data, and easily enables linking with third-party data which can help explain variation in cancer incidence patterns, disparities, and outcomes. We developed a prototype knowledge graph based on the Louisiana Tumor Registry dataset. We present the advantages of the knowledge graph approach by examining: i) scenario-specific queries, ii) links with openly available external datasets, iii) schema evolution for iterative analysis, and iv) data visualization. Our results demonstrate that this graph based solution can perform complex queries, improve query run-time performance by up to 76%, and more easily conduct iterative analyses to enhance researchers' understanding of cancer registry data. © 2013 IEEE.},
	publication_stage = {Final}
}

@ARTICLE{Xie2021900,
	author = {Xie, Jiangan and Zi, Wenrui and Li, Zhangyong and He, Yongqun},
	title = {Ontology-based precision vaccinology for deep mechanism understanding and precision vaccine development},
	year = {2021},
	journal = {Current Pharmaceutical Design},
	volume = {27},
	number = {7},
	pages = {900 – 910},
	doi = {10.2174/1381612826666201125112131},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103501250&doi=10.2174%2f1381612826666201125112131&partnerID=40&md5=db08c46f545b36dbf6fafadfe2ab4a87},
	abstract = {Vaccination is one of the most important innovations in human history. It has also become a hot research area in a new application-the development of new vaccines against non-infectious diseases such as can-cers. However, effective and safe vaccines still do not exist for many diseases, and where vaccines exist, their protective immune mechanisms are often unclear. Although licensed vaccines are generally safe, various adverse events, and sometimes severe adverse events, still exist for a small population. Precision medicine tailors medical intervention to the personal characteristics of individual patients or sub-populations of individuals with similar immunity-related characteristics. Precision vaccinology is a new strategy that applies precision medicine to the development, administration, and post-administration analysis of vaccines. Several conditions contribute to make this the right time to embark on the development of precision vaccinology. First, the increased level of research in vaccinology has generated voluminous “big data” repositories of vaccinology data. Secondly, new technologies such as multi-omics and immunoinformatics bring new methods for investigating vaccines and immunology. Finally, the advent of AI and machine learning software now makes possible the marriage of Big Data to the development of new vaccines in ways not possible before. However, something is missing in this marriage, and that is a common language that facilitates the correlation, analysis, and reporting nomenclature for the field of vaccinology. Solving this bioinformatics problem is the domain of applied biomedical ontology. Ontology in the informatics field is human-and machine-interpretable representation of entities and the rela-tions among entities in a specific domain. The Vaccine Ontology (VO) and Ontology of Vaccine Adverse Events (OVAE) have been developed to support the standard representation of vaccines, vaccine components, vaccinations, host responses, and vaccine adverse events. Many other biomedical ontologies have also been developed and can be applied in vaccine research. Here, we review the current status of precision vaccinology and how ontological development will enhance this field, and propose an ontology-based precision vaccinology strategy to support precision vaccine research and development. © 2021 Bentham Science Publishers.},
	publication_stage = {Final}
}

@ARTICLE{Najafabadipour2019153,
	author = {Najafabadipour, Marjan and Tuñas, Juan Manuel and Rodríguez-González, Alejandro and Menasalvas, Ernestina},
	title = {Lung cancer concept annotation from Spanish clinical narratives},
	year = {2019},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {11371 LNBI},
	pages = {153 – 163},
	doi = {10.1007/978-3-030-06016-9_15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059691309&doi=10.1007%2f978-3-030-06016-9_15&partnerID=40&md5=2fa56abdc49752f300dad3a3258595d5},
	abstract = {Recent rapid increase in the generation of clinical data and rapid development of computational science make us able to extract new insights from massive datasets in healthcare industry. Oncological Electronic Health Records (EHRs) are creating rich databases for documenting patient’s history and they potentially contain a lot of patterns that can help in better management of the disease. However, these patterns are locked within free text (unstructured) portions of EHRs and consequence in limiting health professionals to extract useful information from them and to finally perform Query and Answering (Q&A) process in an accurate way. The Information Extraction (IE) process requires Natural Language Processing (NLP) techniques to assign semantics to these patterns. Therefore, in this paper, we analyze the design of annotators for specific lung cancer concepts that can be integrated over Apache Unstructured Information Management Architecture (UIMA) framework. In addition, we explain the details of generation and storage of annotation outcomes. © Springer Nature Switzerland AG 2019.},
	publication_stage = {Final}
}

@ARTICLE{Samal20171111,
	author = {Samal, Lipika and D'Amore, John D. and Bates, David W. and Wright, Adam},
	title = {Implementation of a scalable, web-based, automated clinical decision support risk-prediction tool for chronic kidney disease using C-CDA and application programming interfaces},
	year = {2017},
	journal = {Journal of the American Medical Informatics Association},
	volume = {24},
	number = {6},
	pages = {1111 – 1115},
	doi = {10.1093/jamia/ocx065},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032943482&doi=10.1093%2fjamia%2focx065&partnerID=40&md5=aa5afd29effeecb309c1706024b65817},
	abstract = {Background and Objective: Clinical decision support tools for risk prediction are readily available, but typically require workflow interruptions and manual data entry so are rarely used. Due to new data interoperability standards for electronic health records (EHRs), other options are available. As a clinical case study, we sought to build a scalable, web-based system that would automate calculation of kidney failure risk and display clinical decision support to users in primary care practices. Materials and Methods: We developed a single-page application, web server, database, and application programming interface to calculate and display kidney failure risk. Data were extracted from the EHR using the Consolidated Clinical Document Architecture interoperability standard for Continuity of Care Documents (CCDs). EHR users were presented with a noninterruptive alert on the patient's summary screen and a hyperlink to details and recommendations provided through a web application. Clinic schedules and CCDs were retrieved using existing application programming interfaces to the EHR, and we provided a clinical decision support hyperlink to the EHR as a service. Results: We debugged a series of terminology and technical issues. The application was validated with data from 255 patients and subsequently deployed to 10 primary care clinics where, over the course of 1 year, 569 533 CCD documents were processed. Conclusions: We validated the use of interoperable documents and open-source components to develop a lowcost tool for automated clinical decision support. Since Consolidated Clinical Document Architecture-based data extraction extends to any certified EHR, this demonstrates a successful modular approach to clinical decision support. © The Author 2017.},
	publication_stage = {Final}
}

@ARTICLE{Cumbo20203,
	author = {Cumbo, Fabio and Weitschek, Emanuel},
	title = {An In-Memory Cognitive-Based Hyperdimensional Approach to Accurately Classify DNA-Methylation Data of Cancer},
	year = {2020},
	journal = {Communications in Computer and Information Science},
	volume = {1285 CCIS},
	pages = {3 – 10},
	doi = {10.1007/978-3-030-59028-4_1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092118511&doi=10.1007%2f978-3-030-59028-4_1&partnerID=40&md5=32e5640c4ff3768fe3f51dae199df0f5},
	abstract = {With Next Generation DNA Sequencing techniques (NGS) we are witnessing a high growth of genomic data. In this work, we focus on the NGS DNA methylation experiment, whose aim is to shed light on the biological process that controls the functioning of the genome and whose modifications are deeply investigated in cancer studies for biomarker discovery. Because of the abundance of DNA methylation public data and of its high dimension in terms of features, new and efficient classification techniques are highly demanded. Therefore, we propose an energy efficient in-memory cognitive-based hyperdimensional approach for classification of DNA methylation data of cancer. This approach is based on the brain-inspired Hyperdimensional (HD) computing by adopting hypervectors and not single numerical values. This makes it capable of recognizing complex patterns with a great robustness against mistakes even with noisy data, as well as the human brain can do. We perform our experimentation on three cancer datasets (breast, kidney, and thyroid carcinomas) extracted from the Genomic Data Commons portal, the main repository of tumoral genomic and clinical data, obtaining very promising results in terms of accuracy (i.e., breast 97.7%, kidney 98.43%, thyroid 100%, respectively) and low computational time. For proving the validity of our approach, we compare it to another state-of-the-art classification algorithm for DNA methylation data. Finally, processed data and software are freely released at https://github.com/fabio-cumbo/HD-Classifier for aiding field experts in the detection and diagnosis of cancer. © 2020, Springer Nature Switzerland AG.},
	publication_stage = {Final}
}

@BOOK{Parray20191,
	author = {Parray, Javid Ahmad and Mir, Mohammad Yaseen and Shameem, Nowsheen},
	title = {Sustainable agriculture: Biotechniques in plant biology},
	year = {2019},
	journal = {Sustainable Agriculture: Biotechniques in Plant Biology},
	pages = {1 – 528},
	doi = {10.1007/978-981-13-8840-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083583893&doi=10.1007%2f978-981-13-8840-8&partnerID=40&md5=8a3cce5e4be2f65094c56979c7b29bb4},
	abstract = {This book will be of immense helpful to the students of plant biotechnology, Agricultural sciences, Microbiology of both undergraduate and postgraduate levels in universities, colleges, and Research institutes. Besides the book will be quite supportive researchers who work in the field of plant biotechnology and agricultural sciences. In this book, the main focus will be on advanced genome editing approaches for the production of GM crops besides their socioeconomic, ethical and risk-biosafety assessments. Nanotechnology is the new emerging and fascinating field of science finds its application in almost all the major research areas and its uses in agriculture and food sectors are incipient.The books seems to be first in summarizing the two way interactive approach in the field of plant biotechnology and setting of a new arena in shaping the new bio techniques towards the sustainable cause. © Springer Nature Singapore Pte Ltd. 2019.},
	publication_stage = {Final}
}

@ARTICLE{Lin20181,
	author = {Lin, Frank P. and Epstein, Richard J. and Groza, Tudor and Kocbek, Simon and Antezana, Erick},
	title = {Cancer care treatment outcome ontology: A novel computable ontology for profiling treatment outcomes in patients with solid tumors},
	year = {2018},
	journal = {JCO Clinical Cancer Informatics},
	volume = {2018},
	number = {2},
	pages = {1 – 14},
	doi = {10.1200/CCI.18.00026},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062457264&doi=10.1200%2fCCI.18.00026&partnerID=40&md5=61aaf3c90a3649d6b20890172cff8ad8},
	abstract = {Purpose There is as yet no computer-processable resource to describe treatment end points in cancer, hindering our ability to systematically capture and share outcomes data to inform better patient care. To address these unmet needs, we have built an ontology, the Cancer Care Treatment Outcome Ontology (CCTOO), to organize high-level concepts of treatment end points with structured knowledge representation to facilitate standardized sharing of real-world data. MethodsEnd points from oncology trials in ClinicalTrials.govwere extracted, queried using the keyword cancer, and followed by an expert appraisal. Synonyms and relevant terms were imported from the National Cancer Institute Thesaurus and Common Terminology Criteria for Adverse Events. Logical relationships among concepts were manually represented by production rules. The applicability of 1,847 rules was tested in an index case. ResultsAfter removing duplicated terms from 54,705 trial entries, an ontology holding 1,133 terms was built. CCTOO organized concepts into four domains (cancer treatment, health services, physical, and psychosocial health-related concepts), 13 subgroups (including efficacy, safety, and quality of life), and two (taxonomic and evaluative) concept hierarchies. This ontology has a comprehensive term coverage in the cancer trial literature: at least one term was mentioned in 98% of MEDLINE abstracts of phase I to III trials, whereas concepts about efficacy were mentioned in 7,208 (79%) phase I, 15,051 (92%) phase II, and 3,884 (86%) phase III trials. The event sequence of the index case was readily convertible to a comprehensive profile incorporating response, treatment toxicity, and survival by applying the set of production rules curated in the CCTOO. ConclusionCCTOO categorizes high-level treatment end points used in oncology and provides a mechanism for profiling individual patient data by outcomes to facilitate translational analysis. © 2018 American Society of Clinical Oncology.},
	publication_stage = {Final}
}

@BOOK{Kanza2020129,
	author = {Kanza, Samantha and Graham Frey, Jeremy},
	title = {Semantic Technologies in Drug Discovery},
	year = {2020},
	journal = {Systems Medicine: Integrative, Qualitative and Computational Approaches: Volume 1-3},
	volume = {1-3},
	pages = {129 – 144},
	doi = {10.1016/B978-0-12-801238-3.11520-X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106476470&doi=10.1016%2fB978-0-12-801238-3.11520-X&partnerID=40&md5=cac8054e2443e8571ae1ce8ca15c21c8},
	abstract = {Semantic drug discovery has gained significant traction in recent years, with researchers becoming more aware that these technologies enable them to link together and query disparate datasets for information that cannot be extracted from a single dataset. This article provides a comprehensive reference source of the current knowledge available regarding Semantic Web technologies in drug discovery. The main aspects of Semantic Web technologies are explained, detailing the different ways in which they can be used in drug discovery. Over 1000 biomedical ontologies were reviewed as part of the work undertaken for this paper and 34 of the most relevant ontologies in the drug discovery field are categorized and described, followed by details of semantic applications and successes in drug discovery. Some core standards and guidelines have been established for sharing Semantic drug discovery data, both through making well established medical taxonomies available in a Semantic format, and by creating upper-level ontologies and guidelines for creating new ontologies in the biomedical domain. This article concludes that a majority of the prevalent ontologies in drug discovery follow these standards and provides advice for researchers wishing to use Semantic Web technologies in their drug discovery research. © 2021 Elsevier Inc. All rights reserved},
	publication_stage = {Final}
}

@ARTICLE{Bousquet2019,
	author = {Bousquet, Cédric and Souvignet, Julien and Sadou, Éric and Jaulent, Marie-Christine and Declerck, Gunnar},
	title = {Ontological and non-ontological resources for associating medical dictionary for regulatory activities terms to SNOMED clinical terms with semantic properties},
	year = {2019},
	journal = {Frontiers in Pharmacology},
	volume = {10},
	number = {SEP},
	doi = {10.3389/fphar.2019.00975},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073018600&doi=10.3389%2ffphar.2019.00975&partnerID=40&md5=d495cbe6e5335070c74085112e728a63},
	abstract = {Background: Formal definitions allow selecting terms (e.g., identifying all terms related to “Infectious disease” using the query “has causative agent organism”) and terminological reasoning (e.g., “hepatitis B” is a “hepatitis” and is an “infectious disease”). However, the standard international terminology Medical Dictionary for Regulatory Activities (MedDRA) used for coding adverse drug reactions in pharmacovigilance databases does not beneficiate from such formal definitions. Our objective was to evaluate the potential of reuse of ontological and non-ontological resources for generating such definitions for MedDRA. Methods: We developed several methods that collectively allow a semiautomatic semantic enrichment of MedDRA: 1) using MedDRA-to-SNOMED Clinical Terms (SNOMED CT) mappings (available in the Unified Medical Language System metathesaurus or other mapping resources, e.g., the MedDRA preferred term “hepatitis B” is associated to the SNOMED CT concept “type B viral hepatitis”) to extract term definitions (e.g., “hepatitis B” is associated with the following properties: has finding site liver structure, has associated morphology inflammation morphology, and has causative agent hepatitis B virus); 2) using MedDRA labels and lexical/syntactic methods for automatic decomposition of complex MedDRA terms (e.g., the MedDRA systems organ class “blood and lymphatic system disorders” is decomposed in blood system disorders and lymphatic system disorders) or automatic suggestions of properties (e.g., the string “cyclic” in preferred term “cyclic neutropenia” leads to the property has clinical course cyclic). Results: The Unified Medical Language System metathesaurus was the main ontological resource reusable for generating formal definitions for MedDRA terms. The non-ontological resources (another mapping resource provided by Nadkarni and Darer in 2010 and MedDRA labels) allowed defining few additional preferred terms. While the Ci4SeR tool helped the curator to define 1,935 terms by suggesting potential supplemental relations based on the parents' and siblings' semantic definition, defining manually all MedDRA terms remains expensive in time. Discussion: Several ontological and non-ontological resources are available for associating MedDRA terms to SNOMED CT concepts with semantic properties, but providing manual definitions is still necessary. The ontology of adverse events is a possible alternative but does not cover all MedDRA terms either. Perspectives are to implement more efficient techniques to find more logical relations between SNOMED CT and MedDRA in an automated way. © 2019 Bousquet, Souvignet, Sadou, Jaulent and Declerck. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.},
	publication_stage = {Final}
}

@ARTICLE{Saint-Louis2018,
	author = {Saint-Louis, Patrick and Lapalme, James},
	title = {An exploration of the many ways to approach the discipline of enterprise architecture},
	year = {2018},
	journal = {International Journal of Engineering Business Management},
	volume = {10},
	doi = {10.1177/1847979018807383},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056581692&doi=10.1177%2f1847979018807383&partnerID=40&md5=a2048f0a7366c91d1a13019c06b0b9c0},
	abstract = {Despite growing interest in enterprise architecture (EA) around the world in recent years, a lack of common understanding is frequently described by EA researchers/practitioners. We conducted a systematic mapping study and it revealed that the extent to which the authors/researchers are focused on EA, the sectors in which they are working, the academic disciplines in which they have studied, the countries where their affiliated organizations are located, the subject areas of the journals/publishers of their publications and the way they have approached EA and its practitioners are some major elements that might influence the existing uniformity in EA. In addition, this study demonstrates how important it is to pay attention to the definition of ‘enterprise architecture’ itself. The contribution of this study is the organization of the EA literature according to three major questions concerning ‘who’ have been published in the literature, ‘where’ they have been located and ‘what’ their publications are about. This helps to better identify sources of variety which could be on the basis of the lack of common understanding in EA and provides practitioners and stakeholders a better understanding of this challenge. This also provides relevant directions for future studies. © The Author(s) 2018.},
	publication_stage = {Final}
}

@ARTICLE{Meystre201913,
	author = {Meystre, Stéphane M. and Heider, Paul M. and Kim, Youngjun and Aruch, Daniel B. and Britten, Carolyn D.},
	title = {Automatic trial eligibility surveillance based on unstructured clinical data},
	year = {2019},
	journal = {International Journal of Medical Informatics},
	volume = {129},
	pages = {13 – 19},
	doi = {10.1016/j.ijmedinf.2019.05.018},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066271796&doi=10.1016%2fj.ijmedinf.2019.05.018&partnerID=40&md5=53c4542dddbf585b58ea1713e1d2d7f3},
	abstract = {Introduction: Insufficient patient enrollment in clinical trials remains a serious and costly problem and is often considered the most critical issue to solve for the clinical trials community. In this project, we assessed the feasibility of automatically detecting a patient's eligibility for a sample of breast cancer clinical trials by mapping coded clinical trial eligibility criteria to the corresponding clinical information automatically extracted from text in the EHR. Methods: Three open breast cancer clinical trials were selected by oncologists. Their eligibility criteria were manually abstracted from trial descriptions using the OHDSI ATLAS web application. Patients enrolled or screened for these trials were selected as ‘positive’ or ‘possible’ cases. Other patients diagnosed with breast cancer were selected as ‘negative’ cases. A selection of the clinical data and all clinical notes of these 229 selected patients was extracted from the MUSC clinical data warehouse and stored in a database implementing the OMOP common data model. Eligibility criteria were extracted from clinical notes using either manually crafted pattern matching (regular expressions) or a new natural language processing (NLP) application. These extracted criteria were then compared with reference criteria from trial descriptions. This comparison was realized with three different versions of a new application: rule-based, cosine similarity-based, and machine learning-based. Results: For eligibility criteria extraction from clinical notes, the machine learning-based NLP application allowed for the highest accuracy with a micro-averaged recall of 90.9% and precision of 89.7%. For trial eligibility determination, the highest accuracy was reached by the machine learning-based approach with a per-trial AUC between 75.5% and 89.8%. Conclusion: NLP can be used to extract eligibility criteria from EHR clinical notes and automatically discover patients possibly eligible for a clinical trial with good accuracy, which could be leveraged to reduce the workload of humans screening patients for trials. © 2019 Elsevier B.V.},
	publication_stage = {Final}
}

@BOOK{Yan20171,
	author = {Yan, Qing},
	title = {Translational bioinformatics and systems biology methods for personalized medicine},
	year = {2017},
	journal = {Translational Bioinformatics and Systems Biology Methods for Personalized Medicine},
	pages = {1 – 171},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029957720&partnerID=40&md5=d10b91f84d2edffabd2ba69898d892ad},
	abstract = {Translational Bioinformatics and Systems Biology Methods for Personalized Medicine introduces integrative approaches in translational bioinformatics and systems biology to support the practice of personalized, precision, predictive, preventive, and participatory medicine. Through the description of important cutting-edge technologies in bioinformatics and systems biology, readers may gain an essential understanding of state-of-the-art methodologies. The book discusses topics such as the challenges and tasks in translational bioinformatics; pharmacogenomics, systems biology, and personalized medicine; and the applicability of translational bioinformatics for biomarker discovery, epigenomics, and molecular dynamics. It also discusses data integration and mining, immunoinformatics, and neuroinformatics. With broad coverage of both basic scientific and clinical applications, this book is suitable for a wide range of readers who may not be scientists but who are also interested in the practice of personalized medicine. © 2017 Elsevier Inc. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Franzese2019,
	author = {Franzese, Nicholas and Groce, Adam and Murali, T.M. and Ritz, Anna},
	title = {Hypergraph-based connectivity measures for signaling pathway topologies},
	year = {2019},
	journal = {PLoS Computational Biology},
	volume = {15},
	number = {10},
	doi = {10.1371/journal.pcbi.1007384},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074674669&doi=10.1371%2fjournal.pcbi.1007384&partnerID=40&md5=a11ac3a498ab70235e48303487c18e66},
	abstract = {Characterizing cellular responses to different extrinsic signals is an active area of research, and curated pathway databases describe these complex signaling reactions. Here, we revisit a fundamental question in signaling pathway analysis: are two molecules "connected" in a network? This question is the first step towards understanding the potential influence of molecules in a pathway, and the answer depends on the choice of modeling framework. We examined the connectivity of Reactome signaling pathways using four different pathway representations. We find that Reactome is very well connected as a graph, moderately well connected as a compound graph or bipartite graph, and poorly connected as a hypergraph (which captures many-to-many relationships in reaction networks). We present a novel relaxation of hypergraph connectivity that iteratively increases connectivity from a node while preserving the hypergraph topology. This measure, B-relaxation distance, provides a parameterized transition between hypergraph connectivity and graph connectivity. B-relaxation distance is sensitive to the presence of small molecules that participate in many functionally unrelated reactions in the network. We also define a score that quantifies one pathway's downstream influence on another, which can be calculated as B-relaxation distance gradually relaxes the connectivity constraint in hypergraphs. Computing this score across all pairs of 34 Reactome pathways reveals pairs of pathways with statistically significant influence. We present two such case studies, and we describe the specific reactions that contribute to the large influence score. Finally, we investigate the ability for connectivity measures to capture functional relationships among proteins, and use the evidence channels in the STRING database as a benchmark dataset. STRING interactions whose proteins are B-connected in Reactome have statistically significantly higher scores than interactions connected in the bipartite graph representation. Our method lays the groundwork for other generalizations of graph-theoretic concepts to hypergraphs in order to facilitate signaling pathway analysis. © 2019 Franzese et al.},
	publication_stage = {Final}
}

@ARTICLE{Mochida20201408,
	author = {Mochida, Keiichi and Nishii, Ryuei and Hirayama, Takashi},
	title = {Decoding plant–environment interactions that influence crop agronomic traits},
	year = {2020},
	journal = {Plant and Cell Physiology},
	volume = {61},
	number = {8},
	pages = {1408 – 1418},
	doi = {10.1093/pcp/pcaa064},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089712583&doi=10.1093%2fpcp%2fpcaa064&partnerID=40&md5=2a4148449cbdd1ac04f2651b4bcfbec3},
	abstract = {To ensure food security in the face of increasing global demand due to population growth and progressive urbanization, it will be crucial to integrate emerging technologies in multiple disciplines to accelerate overall throughput of gene discovery and crop breeding. Plant agronomic traits often appear during the plants’ later growth stages due to the cumulative effects of their lifetime interactions with the environment. Therefore, decoding plant–environment interactions by elucidating plants’ temporal physiological responses to environmental changes throughout their lifespans will facilitate the identification of genetic and environmental factors, timing and pathways that influence complex end-point agronomic traits, such as yield. Here, we discuss the expected role of the life-course approach to monitoring plant and crop health status in improving crop productivity by enhancing the understanding of plant–environment interactions. We review recent advances in analytical technologies for monitoring health status in plants based on multi-omics analyses and strategies for integrating heterogeneous datasets from multiple omics areas to identify informative factors associated with traits of interest. In addition, we showcase emerging phenomics techniques that enable the noninvasive and continuous monitoring of plant growth by various means, including three-dimensional phenotyping, plant root phenotyping, implantable/injectable sensors and affordable phenotyping devices. Finally, we present an integrated review of analytical technologies and applications for monitoring plant growth, developed across disciplines, such as plant science, data science and sensors and Internet-of-things technologies, to improve plant productivity. © The Author(s) 2020. Published by Oxford University Press on behalf of Japanese Society of Plant Physiologists.},
	publication_stage = {Final}
}

@ARTICLE{Sasikala20206592,
	author = {Sasikala, E. and Radha, R. and Gopalakrishnan, R.},
	title = {A review on pathology report based cancer diagnosing system using intelligent techniques},
	year = {2020},
	journal = {International Journal of Advanced Science and Technology},
	volume = {29},
	number = {3},
	pages = {6592 – 6608},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083281735&partnerID=40&md5=9ddab600ff6d1f29f2653557f8c8e189},
	abstract = {OBJECTIVE: The aim of this paper is to review the significant advances made in the context of extracting information from clinical pathology reports, with an emphasis on reports pertaining to cancer. METHOD: The development of a fast growing body of research focused on applying machine learning and, more recently, deep learning to cancer diagnosis and to the specific domain of extraction of cancer-related information from pathology reports is the motivation for this survey. We have reviewed a large number of relevant research papers in this field and in related domains, with a focus on most recent research, and evaluated them from the viewpoint of progress towards automated cancer surveillance. RESULTS: It is observed that deep learning techniques have significantly outperformed traditional machine learning approaches, as evident from latest research in this field. However, machine learning and NLP techniques are equally important, for high accuracy results as well as to understand the evolution of research trends in the field. The potential of deep learning applications remains to be fully utilized in this domain to achieve enhanced cancer surveillance. © 2019 SERSC.},
	publication_stage = {Final}
}

@ARTICLE{Hochheiser2016,
	author = {Hochheiser, Harry and Castine, Melissa and Harris, David and Savova, Guergana and Jacobson, Rebecca S.},
	title = {An information model for computable cancer phenotypes},
	year = {2016},
	journal = {BMC Medical Informatics and Decision Making},
	volume = {16},
	number = {1},
	doi = {10.1186/s12911-016-0358-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987900165&doi=10.1186%2fs12911-016-0358-4&partnerID=40&md5=e29cb1070d1482e113173fa22e68f8bc},
	abstract = {Background: Standards, methods, and tools supporting the integration of clinical data and genomic information are an area of significant need and rapid growth in biomedical informatics. Integration of cancer clinical data and cancer genomic information poses unique challenges, because of the high volume and complexity of clinical data, as well as the heterogeneity and instability of cancer genome data when compared with germline data. Current information models of clinical and genomic data are not sufficiently expressive to represent individual observations and to aggregate those observations into longitudinal summaries over the course of cancer care. These models are acutely needed to support the development of systems and tools for generating the so called clinical "deep phenotype" of individual cancer patients, a process which remains almost entirely manual in cancer research and precision medicine. Methods: Reviews of existing ontologies and interviews with cancer researchers were used to inform iterative development of a cancer phenotype information model. We translated a subset of the Fast Healthcare Interoperability Resources (FHIR) models into the OWL 2 Description Logic (DL) representation, and added extensions as needed for modeling cancer phenotypes with terms derived from the NCI Thesaurus. Models were validated with domain experts and evaluated against competency questions. Results: The DeepPhe Information model represents cancer phenotype data at increasing levels of abstraction from mention level in clinical documents to summaries of key events and findings. We describe the model using breast cancer as an example, depicting methods to represent phenotypic features of cancers, tumors, treatment regimens, and specific biologic behaviors that span the entire course of a patient's disease. Conclusions: We present a multi-scale information model for representing individual document mentions, document level classifications, episodes along a disease course, and phenotype summarization, linking individual observations to high-level summaries in support of subsequent integration and analysis. © 2016 The Author(s).},
	publication_stage = {Final}
}

@ARTICLE{Sharma201741,
	author = {Sharma, Meenakshi and Aggarwal, Himanshu},
	title = {Methodologies of legacy clinical decision support system - A review},
	year = {2017},
	journal = {Journal of Telecommunication, Electronic and Computer Engineering},
	volume = {9},
	number = {3-6},
	pages = {41 – 47},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039778109&partnerID=40&md5=1df869bee0080ef2e6f5891f704be970},
	abstract = {Information technology playing a prominent role in the field of medical by incorporating the Clinical Decision Support System(CDSS) in their routine practices. CDSS is a computer based interactive program to assist the physician to make the right decision at the right time. Now a day's Clinical decision support system is a dynamic research area in the field of computer, but the lack of the knowledge of the understanding as well as the functioning of the system ,make the adoption slow by the physician and patient. The literature review of this paper will focus on the overview of legacy CDSS, the kind of methodologies and classifier employed to prepare such decision support system using a non-technical approach to the physician and the strategy- makers . This study will provide the scope of understanding the clinical decision support along with the gateway to physician ,policy-makers to develop and deploy the decision support system as a healthcare service to make the quick, agile and right decision. Future direction to handle the uncertainties along with the challenges of clinical decision support system are also enlightened in this study.},
	publication_stage = {Final}
}

@ARTICLE{Krallinger20177673,
	author = {Krallinger, Martin and Rabal, Obdulia and Lourenço, Anália and Oyarzabal, Julen and Valencia, Alfonso},
	title = {Information retrieval and text mining technologies for chemistry},
	year = {2017},
	journal = {Chemical Reviews},
	volume = {117},
	number = {12},
	pages = {7673 – 7761},
	doi = {10.1021/acs.chemrev.6b00851},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022005598&doi=10.1021%2facs.chemrev.6b00851&partnerID=40&md5=99102ef3228da76a3c50f5c60ede412a},
	abstract = {Efficient access to chemical information contained in scientific literature, patents, technical reports, or the web is a pressing need shared by researchers and patent attorneys from different chemical disciplines. Retrieval of important chemical information in most cases starts with finding relevant documents for a particular chemical compound or family. Targeted retrieval of chemical documents is closely connected to the automatic recognition of chemical entities in the text, which commonly involves the extraction of the entire list of chemicals mentioned in a document, including any associated information. In this Review, we provide a comprehensive and in-depth description of fundamental concepts, technical implementations, and current technologies for meeting these information demands. A strong focus is placed on community challenges addressing systems performance, more particularly CHEMDNER and CHEMDNER patents tasks of BioCreative IV and V, respectively. Considering the growing interest in the construction of automatically annotated chemical knowledge bases that integrate chemical information and biological data, cheminformatics approaches for mapping the extracted chemical names into chemical structures and their subsequent annotation together with text mining applications for linking chemistry with biological information are also presented. Finally, future trends and current challenges are highlighted as a roadmap proposal for research in this emerging field. © 2017 American Chemical Society.},
	publication_stage = {Final}
}

@BOOK{Kreitler20191,
	author = {Kreitler, Shulamith},
	title = {Psycho-Oncology for the Clinician: The Patient Behind the Disease},
	year = {2019},
	journal = {Psycho-Oncology for the Clinician: The Patient Behind the Disease},
	pages = {1 – 312},
	doi = {10.1007/978-3-030-06126-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098275658&doi=10.1007%2f978-3-030-06126-5&partnerID=40&md5=9d657e3b9fe15bd6de28b2a40be68836},
	abstract = {This volume strives to give oncologists and other medical practitioners a thorough picture of the oncology patient. It’s designed to show that psychological needs of the patient are an important aspect that should be considered for optimizing the effects of cancer treatment, no less than genetic, immunological, physiological and other medical features that are often considered as necessary components of personalized medicine. Using evidence-based information, the book describes the different ways cancer touches upon a person’s life, including emotional, physical, and social changes, important decisions, and support structure. It also details the phases every cancer patient encounters along the way, from getting tested and waiting for the diagnosis, to treatments, survival, and confronting one’s mortality. Psycho-oncology for the Clinician will serve to contribute to the further scientific development of psycho-oncology, expand its use as a treatment modality, strengthen its status as an essential component of cancer care, and promote the acceptance of psycho-oncology as the new evidence-based constituent of personalized medicine in oncology.  © Springer Nature Switzerland AG 2019.},
	publication_stage = {Final}
}

@ARTICLE{Datta2019,
	author = {Datta, Surabhi and Bernstam, Elmer V. and Roberts, Kirk},
	title = {A frame semantic overview of NLP-based information extraction for cancer-related EHR notes},
	year = {2019},
	journal = {Journal of Biomedical Informatics},
	volume = {100},
	doi = {10.1016/j.jbi.2019.103301},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074198160&doi=10.1016%2fj.jbi.2019.103301&partnerID=40&md5=c4af92893b99c7ca2a2945e4b8cc723f},
	abstract = {Objective: There is a lot of information about cancer in Electronic Health Record (EHR) notes that can be useful for biomedical research provided natural language processing (NLP) methods are available to extract and structure this information. In this paper, we present a scoping review of existing clinical NLP literature for cancer. Methods: We identified studies describing an NLP method to extract specific cancer-related information from EHR sources from PubMed, Google Scholar, ACL Anthology, and existing reviews. Two exclusion criteria were used in this study. We excluded articles where the extraction techniques used were too broad to be represented as frames (e.g., document classification) and also where very low-level extraction methods were used (e.g. simply identifying clinical concepts). 78 articles were included in the final review. We organized this information according to frame semantic principles to help identify common areas of overlap and potential gaps. Results: Frames were created from the reviewed articles pertaining to cancer information such as cancer diagnosis, tumor description, cancer procedure, breast cancer diagnosis, prostate cancer diagnosis and pain in prostate cancer patients. These frames included both a definition as well as specific frame elements (i.e. extractable attributes). We found that cancer diagnosis was the most common frame among the reviewed papers (36 out of 78), with recent work focusing on extracting information related to treatment and breast cancer diagnosis. Conclusion: The list of common frames described in this paper identifies important cancer-related information extracted by existing NLP techniques and serves as a useful resource for future researchers requiring cancer information extracted from EHR notes. We also argue, due to the heavy duplication of cancer NLP systems, that a general purpose resource of annotated cancer frames and corresponding NLP tools would be valuable. © 2019},
	publication_stage = {Final}
}

@ARTICLE{Zhang2018,
	author = {Zhang, Hansi and Guo, Yi and Li, Qian and George, Thomas J. and Shenkman, Elizabeth and Modave, François and Bian, Jiang},
	title = {An ontology-guided semantic data integration framework to support integrative data analysis of cancer survival},
	year = {2018},
	journal = {BMC Medical Informatics and Decision Making},
	volume = {18},
	doi = {10.1186/s12911-018-0636-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050828010&doi=10.1186%2fs12911-018-0636-4&partnerID=40&md5=5e270be1d5be3b9aec42f8c48a06c2db},
	abstract = {Background: Cancer is the second leading cause of death in the United States, exceeded only by heart disease. Extant cancer survival analyses have primarily focused on individual-level factors due to limited data availability from a single data source. There is a need to integrate data from different sources to simultaneously study as much risk factors as possible. Thus, we proposed an ontology-based approach to integrate heterogeneous datasets addressing key data integration challenges. Methods: Following best practices in ontology engineering, we created the Ontology for Cancer Research Variables (OCRV) adapting existing semantic resources such as the National Cancer Institute (NCI) Thesaurus. Using the global-as-view data integration approach, we created mapping axioms to link the data elements in different sources to OCRV. Implemented upon the Ontop platform, we built a data integration pipeline to query, extract, and transform data in relational databases using semantic queries into a pooled dataset according to the downstream multi-level Integrative Data Analysis (IDA) needs. Results: Based on our use cases in the cancer survival IDA, we created tailored ontological structures in OCRV to facilitate the data integration tasks. Specifically, we created a flexible framework addressing key integration challenges: (1) using a shared, controlled vocabulary to make data understandable to both human and computers, (2) explicitly modeling the semantic relationships makes it possible to compute and reason with the data, (3) linking patients to contextual and environmental factors through geographic variables, (4) being able to document the data manipulation and integration processes clearly in the ontologies. Conclusions: Using an ontology-based data integration approach not only standardizes the definitions of data variables through a common, controlled vocabulary, but also makes the semantic relationships among variables from different sources explicit and clear to all users of the same datasets. Such an approach resolves the ambiguity in variable selection, extraction and integration processes and thus improve reproducibility of the IDA. © 2018 The Author(s).},
	publication_stage = {Final}
}

@ARTICLE{Zhang2016286,
	author = {Zhang, Rui and Guo, Degui and Gao, Wenjuan and Liu, Lei},
	title = {Modeling ontology evolution via Pi-Calculus},
	year = {2016},
	journal = {Information Sciences},
	volume = {346-347},
	pages = {286 – 301},
	doi = {10.1016/j.ins.2016.01.059},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959096824&doi=10.1016%2fj.ins.2016.01.059&partnerID=40&md5=3778d9be2137ab4518e5c5d2d51964ab},
	abstract = {Extending and updating real world ontologies is unavoidable challenging with our ever-evolving understanding of the world and the evolution of the world by itself. In the current researches, changes are usually modeled as passive instant results of operations such as to add a child class or to delete a property, executed by the administrators. But this view neglects the real world facts that (1) an ontology evolves continuously over time, not just hopping instantly between static versions; (2) a change is a coherent procedure although it may be separated from the administrative point of view into different phases, such as to request, detect, represent, evaluate, implement and propagate the change. This paper provides an inside perspective of the ontology itself to model its evolution. The ontology entities are regarded as autonomous agents with find-grained state specifications. The operational semantics of the changes are formalized as series of information exchange actions of the agents. Pi-Calculus serves to describe the operational semantics in our modeling. Firstly, an ontology is encoded into a hyper graph. Then the nodes and edges in the hyper graph are formalized as Pi-Calculus processes. The replication operator is used to add new entities to the ontology and the communication rule works for the resolution of the information exchanges. Thus, a change is modeled as a coherent procedure as clarified in its operational semantics. A case study shows the feasibility of our method on the Mobility Workbench (MWB). We believe that the operational semantics in our scenario disclose the autonomous evolving nature of ontology evolution. © 2016 Elsevier Inc. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Dror20201,
	author = {Dror, Rotem and Peled-Cohen, Lotem and Shlomov, Segev and Reichart, Roi},
	title = {Statistical Significance Testing for Natural Language Processing Dror},
	year = {2020},
	journal = {Synthesis Lectures on Human Language Technologies},
	volume = {13},
	number = {2},
	pages = {1 – 116},
	doi = {10.2200/S00994ED1V01Y202002HLT045},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083217834&doi=10.2200%2fS00994ED1V01Y202002HLT045&partnerID=40&md5=9fa7455b59a1d9f5172725e1fc8a409e},
	abstract = {Data-driven experimental analysis has become the main evaluation tool of Natural Language Processing (NLP) algorithms. In fact, in the last decade, it has become rare to see an NLP paper, particularly one that proposes a new algorithm, that does not include extensive experimental analysis, and the number of involved tasks, datasets, domains, and languages is constantly growing. This emphasis on empirical results highlights the role of statistical significance testing in NLP research: If we, as a community, rely on empirical evaluation to validate our hypotheses and reveal the correct language processing mechanisms, we better be sure that our results are not coincidental. The goal of this book is to discuss the main aspects of statistical significance testing in NLP. Our guiding assumption throughout the book is that the basic question NLP researchers and engineers deal with is whether or not one algorithm can be considered better than another one. This question drives the field forward as it allows the constant progress of developing better technology for language processing challenges. In practice, researchers and engineers would like to draw the right conclusion from a limited set of experiments, and this conclusion should hold for other experiments with datasets they do not have at their disposal or that they cannot perform due to limited time and resources. The book hence discusses the opportunities and challenges in using statistical significance testing in NLP, from the point of view of experimental comparison between two algorithms. We cover topics such as choosing an appropriate significance test for the major NLP tasks, dealing with the unique aspects of significance testing for non-convex deep neural networks, accounting for a large number of comparisons between two NLP algorithms in a statistically valid manner (multiple hypothesis testing), and, finally, the unique challenges yielded by the nature of the data and practices of the field. © 2020 by Morgan & Claypool.},
	publication_stage = {Final}
}

@CONFERENCE{Majewska20204810,
	author = {Majewska, Olga and Vulić, Ivan and McCarthy, Diana and Korhonen, Anna},
	title = {Manual Clustering and Spatial Arrangement of Verbs for Multilingual Evaluation and Typology Analysis},
	year = {2020},
	journal = {COLING 2020 - 28th International Conference on Computational Linguistics, Proceedings of the Conference},
	pages = {4810 – 4824},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123916180&partnerID=40&md5=5141fc93ad1d276bb1108042b2759dd2},
	abstract = {We present the first evaluation of the applicability of a spatial arrangement method (SpAM) to a typologically diverse language sample, and its potential to produce semantic evaluation resources to support multilingual NLP, with a focus on verb semantics. We demonstrate SpAM’s utility in allowing for quick bottom-up creation of large-scale evaluation datasets that balance cross-lingual alignment with language specificity. Starting from a shared sample of 825 English verbs, translated into Chinese, Japanese, Finnish, Polish, and Italian, we apply a two-phase annotation process which produces (i) semantic verb classes and (ii) fine-grained similarity scores for nearly 130 thousand verb pairs. We use the two types of verb data to (a) examine cross-lingual similarities and variation, and (b) evaluate the capacity of static and contextualised representation models to accurately reflect verb semantics, contrasting the performance of large language-specific pretraining models with their multilingual equivalent on semantic clustering and lexical similarity, across different domains of verb meaning. We release the data from both phases as a large-scale multilingual resource, comprising 85 verb classes and nearly 130k pairwise similarity scores, offering a wealth of possibilities for further evaluation and research on multilingual verb semantics. © 2020 COLING 2020 - 28th International Conference on Computational Linguistics, Proceedings of the Conference. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Gibaud20181397,
	author = {Gibaud, Bernard and Forestier, Germain and Feldmann, Carolin and Ferrigno, Giancarlo and Gonçalves, Paulo and Haidegger, Tamás and Julliard, Chantal and Katić, Darko and Kenngott, Hannes and Maier-Hein, Lena and März, Keno and de Momi, Elena and Nagy, Dénes Ákos and Nakawala, Hirenkumar and Neumann, Juliane and Neumuth, Thomas and Rojas Balderrama, Javier and Speidel, Stefanie and Wagner, Martin and Jannin, Pierre},
	title = {Toward a standard ontology of surgical process models},
	year = {2018},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	volume = {13},
	number = {9},
	pages = {1397 – 1408},
	doi = {10.1007/s11548-018-1824-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049868025&doi=10.1007%2fs11548-018-1824-5&partnerID=40&md5=53478aacc14c280ec76a98d0fdbb4596},
	abstract = {Purpose: The development of common ontologies has recently been identified as one of the key challenges in the emerging field of surgical data science (SDS). However, past and existing initiatives in the domain of surgery have mainly been focussing on individual groups and failed to achieve widespread international acceptance by the research community. To address this challenge, the authors of this paper launched a European initiative—OntoSPM Collaborative Action—with the goal of establishing a framework for joint development of ontologies in the field of SDS. This manuscript summarizes the goals and the current status of the international initiative. Methods: A workshop was organized in 2016, gathering the main European research groups having experience in developing and using ontologies in this domain. It led to the conclusion that a common ontology for surgical process models (SPM) was absolutely needed, and that the existing OntoSPM ontology could provide a good starting point toward the collaborative design and promotion of common, standard ontologies on SPM. Results: The workshop led to the OntoSPM Collaborative Action—launched in mid-2016—with the objective to develop, maintain and promote the use of common ontologies of SPM relevant to the whole domain of SDS. The fundamental concept, the architecture, the management and curation of the common ontology have been established, making it ready for wider public use. Conclusion: The OntoSPM Collaborative Action has been in operation for 24 months, with a growing dedicated membership. Its main result is a modular ontology, undergoing constant updates and extensions, based on the experts’ suggestions. It remains an open collaborative action, which always welcomes new contributors and applications. © 2018, CARS.},
	publication_stage = {Final}
}

@ARTICLE{Fedorov20205953,
	author = {Fedorov, Andrey and Hancock, Matthew and Clunie, David and Brochhausen, Mathias and Bona, Jonathan and Kirby, Justin and Freymann, John and Pieper, Steve and J. W. L. Aerts, Hugo and Kikinis, Ron and Prior, Fred},
	title = {DICOM re-encoding of volumetrically annotated Lung Imaging Database Consortium (LIDC) nodules},
	year = {2020},
	journal = {Medical Physics},
	volume = {47},
	number = {11},
	pages = {5953 – 5965},
	doi = {10.1002/mp.14445},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090195731&doi=10.1002%2fmp.14445&partnerID=40&md5=b7a6c92e176bcc1e6d675d0a55e25e7f},
	abstract = {Purpose: The dataset contains annotations for lung nodules collected by the Lung Imaging Data Consortium and Image Database Resource Initiative (LIDC) stored as standard DICOM objects. The annotations accompany a collection of computed tomography (CT) scans for over 1000 subjects annotated by multiple expert readers, and correspond to “nodules ≥ 3 mm”, defined as any lesion considered to be a nodule with greatest in-plane dimension in the range 3–30 mm regardless of presumed histology. The present dataset aims to simplify reuse of the data with the readily available tools, and is targeted towards researchers interested in the analysis of lung CT images. Acquisition and validation methods: Open source tools were utilized to parse the project-specific XML representation of LIDC-IDRI annotations and save the result as standard DICOM objects. Validation procedures focused on establishing compliance of the resulting objects with the standard, consistency of the data between the DICOM and project-specific representation, and evaluating interoperability with the existing tools. Data format and usage notes: The dataset utilizes DICOM Segmentation objects for storing annotations of the lung nodules, and DICOM Structured Reporting objects for communicating qualitative evaluations (nine attributes) and quantitative measurements (three attributes) associated with the nodules. The total of 875 subjects contain 6859 nodule annotations. Clustering of the neighboring annotations resulted in 2651 distinct nodules. The data are available in TCIA at https://doi.org/10.7937/TCIA.2018.h7umfurq. Potential applications: The standardized dataset maintains the content of the original contribution of the LIDC-IDRI consortium, and should be helpful in developing automated tools for characterization of lung lesions and image phenotyping. In addition to those properties, the representation of the present dataset makes it more FAIR (Findable, Accessible, Interoperable, Reusable) for the research community, and enables its integration with other standardized data collections. © 2020 The Authors. Medical Physics published by Wiley Periodicals LLC on behalf of American Association of Physicists in Medicine.},
	publication_stage = {Final}
}

@BOOK{Matthews20171,
	author = {Matthews, Joseph R.},
	title = {The evaluation and measurement of library services},
	year = {2017},
	journal = {The Evaluation and Measurement of Library Services},
	pages = {1 – 447},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200530105&partnerID=40&md5=3da50a92a571b00d1ed0b52c57232247},
	abstract = {This guide provides library directors, managers, and administrators in all types of libraries with complete and up-to-date instructions on how to evaluate library services in order to improve them. It's a fact: today's libraries must evaluate their services in order to find ways to better serve patrons and prove their value to their communities. In this greatly updated and expanded edition of Matthews' seminal text, you'll discover a breadth of tools that can be used to evaluate any library service, including newer tools designed to measure customer and patron outcomes. The book offers practical advice backed by solid research on virtually every aspect of evaluation, including quantitative and qualitative tools, data analysis, and specific recommendations for measuring individual services, such as technical services and reference and interlibrary loan. New chapters give readers effective ways to evaluate critical aspects of their libraries such as automated systems, physical space, staff, performance management frameworks, eBooks, social media, and information literacy. The author explains how broader and more robust adoption of evaluation techniques will help library managers combine traditional internal measurements, such as circulation and reference transactions, with more customer-centric metrics that reflect how well patrons feel they are served and how satisfied they are with the library. By applying this comprehensive strategy, readers will gain the ability to form a truer picture of their library's value to its stakeholders and patrons. © 2018 by Joseph R. Matthews. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Nguyen2020,
	author = {Nguyen, Anthony and O'Dwyer, John and Vu, Thanh and Webb, Penelope M and Johnatty, Sharon E and Spurdle, Amanda B},
	title = {Generating high-quality data abstractions from scanned clinical records: Text-mining-assisted extraction of endometrial carcinoma pathology features as proof of principle},
	year = {2020},
	journal = {BMJ Open},
	volume = {10},
	number = {6},
	doi = {10.1136/bmjopen-2020-037740},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086605237&doi=10.1136%2fbmjopen-2020-037740&partnerID=40&md5=0de1705f70f33178ade70c8570a689c7},
	abstract = {Objective Medical research studies often rely on the manual collection of data from scanned typewritten clinical records, which can be laborious, time consuming and error prone because of the need to review individual clinical records. We aimed to use text mining to assist with the extraction of clinical features from complex text-based scanned pathology records for medical research studies. Design Text mining performance was measured by extracting and annotating three distinct pathological features from scanned photocopies of endometrial carcinoma clinical pathology reports, and comparing results to manually abstracted terms. Inclusion and exclusion keyword trigger terms to capture leiomyomas, endometriosis and adenomyosis were provided based on expert knowledge. Terms were expanded with character variations based on common optical character recognition (OCR) error patterns as well as negation phrases found in sample reports. The approach was evaluated on an unseen test set of 1293 scanned pathology reports originating from laboratories across Australia. Setting Scanned typewritten pathology reports for women aged 18-79 years with newly diagnosed endometrial cancer (2005-2007) in Australia. Results High concordance with final abstracted codes was observed for identifying the presence of three pathology features (94%-98% F-measure). The approach was more consistent and reliable than manual abstractions, identifying 3%-14% additional feature instances. Conclusion Keyword trigger-based automation with OCR error correction and negation handling proved not only to be rapid and convenient, but also providing consistent and reliable data abstractions from scanned clinical records. In conjunction with manual review, it can assist in the generation of high-quality data abstractions for medical research studies. © Author(s) (or their employer(s)) 2020. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.},
	publication_stage = {Final}
}

@ARTICLE{Seroussi2018190,
	author = {Seroussi, Brigitte and Lamy, Jean-Baptiste and Muro, Naiara and Larburu, Nekane and Sekar, Booma Devi and Guézennec, Gilles and Bouaud, Jacques},
	title = {Implementing Guideline-Based, Experience-Based, and Case-Based Approaches to Enrich Decision Support for the Management of Breast Cancer Patients in the DESIREE Project},
	year = {2018},
	journal = {Studies in Health Technology and Informatics},
	volume = {255},
	pages = {190 – 194},
	doi = {10.3233/978-1-61499-921-8-190},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054780324&doi=10.3233%2f978-1-61499-921-8-190&partnerID=40&md5=a8107a818d665af5b42a8b54f7cdf742},
	abstract = {DESIREE is a European-funded project to improve the management of primary breast cancer. We have developed three decision support systems (DSSs), a guideline-based, an experience-based, and a case-based DSSs, resp. GL-DSS, EXP-DSS, and CB-DSS, that operate simultaneously to offer an enriched multi-modal decision support to clinicians. A breast cancer knowledge model has been built to describe within a common ontology the data model and the termino-ontological knowledge used for representing breast cancer patient cases. It allows for rule-based and subsumption-based reasoning in the GL-DSS to provide best patient-centered reconciled care plans. It also allows for using semantic similarity in the retrieval algorithm implemented in the CB-DSS. Rainbow boxes are used to display patient cases similar to a given query patient. This innovative visualization technique translates the question of deciding the most appropriate treatment into a question of deciding the colour dominance among boxes. © 2018 The authors and IOS Press.},
	publication_stage = {Final}
}

@ARTICLE{Shankaran2020,
	author = {Shankaran, Veena and Unger, Joseph M. and Darke, Amy K. and Hershman, Dawn L. and Ramsey, Scott D.},
	title = {Design, data linkage, and implementation considerations in the first cooperative group led study assessing financial outcomes in cancer patients and their informal caregivers},
	year = {2020},
	journal = {Contemporary Clinical Trials},
	volume = {95},
	doi = {10.1016/j.cct.2020.106037},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086604024&doi=10.1016%2fj.cct.2020.106037&partnerID=40&md5=c4a3821299c17760446fd8cf228f0cf7},
	abstract = {Background: Few studies have assessed the financial impact of cancer diagnosis on patients and caregivers in diverse clinical settings. S1417CD, led by the SWOG Cancer Research Network, is the first prospective longitudinal cohort study assessing financial outcomes conducted in the NCI Community Oncology Research Program (NCORP). We report our experience navigating design and implementation barriers. Methods: Patients age ≥ 18 within 120 days of metastatic colorectal cancer diagnosis were considered eligible and invited to identify a caregiver to participate in an optional substudy. Measures include 1) patient and caregiver surveys assessing financial status, caregiver burden, and quality of life and 2) patient credit reports obtained from the credit agency TransUnion through a linkage requiring social security numbers and secure data transfer processes. The primary endpoint is incidence of treatment-related financial hardship, defined as one or more of the following: debt accrual, selling or refinancing home, ≥20% income decline, or borrowing money. Accrual goal was n = 374 patients in 3 years. Results: S1417CD activated on Apr 1, 2016 and closed on Feb 1, 2019 after reaching its accrual goal sooner than anticipated. A total of 380 patients (median age 59.7 years) and 155 caregivers enrolled across 548 clinical sites. Credit data were not obtainable for 76 (20%) patients due to early death, lack of credit, or inability to match records. Conclusions: Robust accrual to S1417CD demonstrates patients' and caregivers' willingness to improve understanding of financial toxicity despite perceived barriers such as embarrassment and fears that disclosing financial status could influence treatment recommendations. © 2020 Elsevier Inc.},
	publication_stage = {Final}
}

@BOOK{Choi2020406,
	author = {Choi, Kiri and Karr, Jonathan R. and Sauro, Herbert M.},
	title = {Status and Challenges of Reproducibility in Computational Systems and Synthetic Biology},
	year = {2020},
	journal = {Systems Medicine: Integrative, Qualitative and Computational Approaches: Volume 1-3},
	volume = {1-3},
	pages = {406 – 412},
	doi = {10.1016/B978-0-12-801238-3.11525-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151178326&doi=10.1016%2fB978-0-12-801238-3.11525-9&partnerID=40&md5=24d621f7a514c7b8e832233db81d3595},
	abstract = {Scientific research is reproducible when the findings can be independently verified. Reproducibility is crucial for the integrity of science. Unfortunately, scientific studies, including computational studies, are often not reproducible. We believe there are two primary causes for the frequent lack of reproducibility of computational systems and synthetic biology studies. First, the information needed to reproduce a result is often not communicated clearly. This issue can be addressed by improving and expanding the existing standards, the support for the standards, and the communication of the standards. Second, the computational environment needed to reproduce a result is often not shared. This issue can be partly addressed with virtual machines. Here, we outline the status of the reproducibility of computational systems and synthetic biology by reviewing the existing standards and software tools. As part of this review, we highlight some of the most common standards and software tools. Additionally, we discuss the shortcomings of the current standards and software tools, highlighting several gaps which continue to make computational systems and synthetic biology studies challenging to reproduce. In particular, we highlight the need for expanded standards for describing the provenance and verification of computational systems biology models. © 2021 Elsevier Inc. All rights reserved},
	publication_stage = {Final}
}

@ARTICLE{Rangel2019457,
	author = {Rangel, José Carlos and Pinzón, Cristian},
	title = {Multiagent system for semantic categorization of places mean the use of distributed surveillance cameras},
	year = {2019},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {801},
	pages = {457 – 464},
	doi = {10.1007/978-3-319-99608-0_64},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061723594&doi=10.1007%2f978-3-319-99608-0_64&partnerID=40&md5=68f84bdf700cf0ddb8a4e52a51cc02e9},
	abstract = {Surveillance systems are quite common in almost every building. The current dimension of these systems is huge and involves a great deal of hardware and human resources for achieving these objectives. This paper proposes the use of an agent-based architecture for helping in the categorization of the places where these are deployed. Proposal uses a deep learning model for evaluating the images captured by the cameras and then label the zone where the camera is located. © Springer Nature Switzerland AG 2019.},
	publication_stage = {Final}
}

@ARTICLE{Clarkson2016,
	author = {Clarkson, Melissa D.},
	title = {Representation of anatomy in online atlases and databases: A survey and collection of patterns for interface design},
	year = {2016},
	journal = {BMC Developmental Biology},
	volume = {16},
	number = {1},
	doi = {10.1186/s12861-016-0116-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969565329&doi=10.1186%2fs12861-016-0116-y&partnerID=40&md5=6e4d16b6e0f6d070d8cf073e73274c0a},
	abstract = {Background: A large number of online atlases and databases have been developed to mange the rapidly growing amount of data describing embryogenesis. As these community resources continue to evolve, it is important to understand how representations of anatomy can facilitate the sharing and integration of data. In addition, attention to the design of the interfaces is critical to make online resources useful and usable. Results: I first present a survey of online atlases and gene expression resources for model organisms, with a focus on methods of semantic and spatial representation of anatomy. A total of 14 anatomical atlases and 21 gene expression resources are included. This survey demonstrates how choices in semantic representation, in the form of ontologies, can enhance interface search functions and provide links between relevant information. This survey also reviews methods for spatially representing anatomy in online resources. I then provide a collection of patterns for interface design based on the atlases and databases surveyed. These patterns include methods for displaying graphics, integrating semantic and spatial representations, organizing information, and querying databases to find genes expressed in anatomical structures. Conclusions: This collection of patterns for interface design will assist biologists and software developers in planning the interfaces of new atlases and databases or enhancing existing ones. They also show the benefits of standardizing semantic and spatial representations of anatomy by demonstrating how interfaces can use standardization to provide enhanced functionality. © 2016 Clarkson.},
	publication_stage = {Final}
}

@BOOK{Berman20181,
	author = {Berman, Jules J.},
	title = {Principles and Practice of Big Data: Preparing, Sharing, and Analyzing Complex Information, Second Edition},
	year = {2018},
	journal = {Principles and Practice of Big Data: Preparing, Sharing, and Analyzing Complex Information, Second Edition},
	pages = {1 – 452},
	doi = {10.1016/C2017-0-03409-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150106106&doi=10.1016%2fC2017-0-03409-2&partnerID=40&md5=427fea1aae58da1126b2630e953109a1},
	abstract = {Principles and Practice of Big Data: Preparing, Sharing, and Analyzing Complex Information, Second Edition updates and expands on the first edition, bringing a set of techniques and algorithms that are tailored to Big Data projects. The book stresses the point that most data analyses conducted on large, complex data sets can be achieved without the use of specialized suites of software (e.g., Hadoop), and without expensive hardware (e.g., supercomputers). The core of every algorithm described in the book can be implemented in a few lines of code using just about any popular programming language (Python snippets are provided). Through the use of new multiple examples, this edition demonstrates that if we understand our data, and if we know how to ask the right questions, we can learn a great deal from large and complex data collections. The book will assist students and professionals from all scientific backgrounds who are interested in stepping outside the traditional boundaries of their chosen academic disciplines. © 2018 Elsevier Inc. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Raciti2018,
	author = {Raciti, Daniela and Yook, Karen and Harris, Todd W and Schedl, Tim and Sternberg, Paul W},
	title = {Micropublication: Incentivizing community curation and placing unpublished data into the public domain},
	year = {2018},
	journal = {Database},
	volume = {2018},
	number = {2018},
	doi = {10.1093/database/bay013},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054774139&doi=10.1093%2fdatabase%2fbay013&partnerID=40&md5=0d43c058834e5cc102a4b8c6d57f1602},
	abstract = {Large volumes of data generated by research laboratories coupled with the required effort and cost of curation present a significant barrier to inclusion of these data in authoritative community databases. Further, many publicly funded experimental observations remain invisible to curation simply because they are never published: results often do not fit within the scope of a standard publication; trainee-generated data are forgotten when the experimenter (e.g. student, post-doc) leaves the lab; results are omitted from science narratives due to publication bias where certain results are considered irrelevant for the publication. While authors are in the best position to curate their own data, they face a steep learning curve to ensure that appropriate referential tags, metadata, and ontologies are applied correctly to their observations, a task sometimes considered beyond the scope of their research and other numerous responsibilities. Getting researchers to adopt a new system of data reporting and curation requires a fundamental change in behavior among all members of the research community. To solve these challenges, we have created a novel scholarly communication platform that captures data from researchers and directly delivers them to information resources via Micropublication. This platform incentivizes authors to publish their unpublished observations along with associated metadata by providing a deliberately fast and lightweight but still peer-reviewed process that results in a citable publication. Our long-term goal is to develop a data ecosystem that improves reproducibility and accountability of publicly funded research and in turn accelerates both basic and translational discovery. © The Author(s) 2018. Published by Oxford University Press.},
	publication_stage = {Final}
}

@CONFERENCE{Gurbuz20201720,
	author = {Gurbuz, Ozge and Sun, Miao and Lawless, Nathan},
	title = {A Methodology to Develop Knowledge Graphs for Indication Expansion: An Exploratory Study},
	year = {2020},
	journal = {Proceedings - 2020 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020},
	pages = {1720 – 1727},
	doi = {10.1109/BIBM49941.2020.9313179},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100355921&doi=10.1109%2fBIBM49941.2020.9313179&partnerID=40&md5=bc13e850c325f827902ccbdabf6de8d5},
	abstract = {The rapid increase in data sources for computational drug discovery have resulted in the subject of ontology research becoming a key point for not only data integration, but they also have enriched the semantics of biological networks. Drug repurposing using populated ontologies, in other words semantic knowledge graphs, have received much attention in the past years. Firstly, researchers constructed knowledge graphs from biological data sources to predict disease-drug relations. After considering the reliability of the data, these studies are followed by the construction of knowledge graphs from published unstructured data via text mining tools. Only two recent examples are available utilizing PubMed abstracts to build knowledge graphs for drug discovery. However, these studies highlight their limitations due to dependency to the text mining tools and missing information in the abstracts. In light of this previous work, we conducted an exploratory study to develop a novel method for constructing knowledge graphs for indication expansion studies in which the aim is to find an alternative indication for the main target. The implication of the study is that the knowledge graph consists of both biological data sources which have publication references and human curated text mining results from the full texts. Consequently, the prediction results of the methodology includes one or more publication references. This paper presents the methodology together with the application on two selected cases. Moreover, we share the results, lessons learned and future work. © 2020 IEEE.},
	publication_stage = {Final}
}

@CONFERENCE{Duncan2017,
	author = {Duncan, William D. and Gaudioso, Carmelo and Diehl, Alexander D.},
	title = {Towards an Ontology for Representing Malignant Neoplasms},
	year = {2017},
	journal = {CEUR Workshop Proceedings},
	volume = {2137},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050466132&partnerID=40&md5=70a4bbfacd589d26e9eb3c60860b82ac},
	abstract = {Oncology research produces data about a wide variety of entities such as tumor types, locations, pathology, and staging, patient treatments and outcomes, and experimental systems such as mouse models and cell lines. In order to conduct effective cancer research, terminologies, classification systems, and ontologies are needed that can integrate these various datasets and provide standards for consistently representing entities. In this paper, we discuss our ongoing efforts to address these difficulties by developing a realism-based ontology for representing instances of malignant neoplasms, disease progression, treatments, and outcomes. This ontology is being built using the principles of the OBO Foundry, and makes use of other OBO Foundry ontologies, such as the Ontology for General Medical Sciences, Uberon, and the Cell Ontology. As a result of our efforts, we have made worthwhile progress towards developing a robust ontological framework for representing malignant neoplasms. © 2018 CEUR-WS. All rights reserved.},
	publication_stage = {Final}
}

@CONFERENCE{Zehra201931,
	author = {Zehra, Durre and Jha, Alokkumar and Khan, Yasar and Hasnain, Ali and d’Aquin, Mathieu and Sahay, Ratnesh},
	title = {A cancer genomics data space within the linked open data (LOD) cloud},
	year = {2019},
	journal = {CEUR Workshop Proceedings},
	volume = {2477},
	pages = {31 – 39},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074582783&partnerID=40&md5=a7a0a2956d66b4c329efac1f07f43d7e},
	abstract = {The ongoing cancer research requires finding patterns and associations among genetic, cellular and molecular features residing in isolated and disparate repositories. The discovery of complex biological associations from these independent repositories will help advanced analysis and hypothesis generation over a network of coherent datasets. In this paper we provide a short overview of three types of cancer genomics datasets that are transformed from raw formats (csv, tsv, relational, etc.) into a set of linked datasets within the Linked Open Data Cloud. The three genomics datasets (Copy Number Variation (CNV), Methylation, & Gene Expression) are related to ovarian cancer studies and originally archived in three different repositories (The Cancer Genome Atlas (TCGA), Catalogue of Somatic Mutations in Cancer (COSMIC), and Copy Number Variation in Disease (CNVD)). Our key motivation is to create a network of coherent cancer genomic linked datasets within the widely accessible LOD cloud. We provide these three genomics datasets as a set – called Linked Open Data for Cancer Genomics (LOD4CG) – of five interlinked publicly accessible SPARQL endpoints that will help researchers and practitioners to exploring these datasets and links across them. Copyright ©2019 for this paper by its authors.},
	publication_stage = {Final}
}

@ARTICLE{Zheng2020,
	author = {Zheng, Ling and Chen, Yan and Min, Hua and Hildebrand, P. Lloyd and Liu, Hao and Halper, Michael and Geller, James and de Coronado, Sherri and Perl, Yehoshua},
	title = {Missing lateral relationships in top-level concepts of an ontology},
	year = {2020},
	journal = {BMC Medical Informatics and Decision Making},
	volume = {20},
	doi = {10.1186/s12911-020-01319-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097565098&doi=10.1186%2fs12911-020-01319-3&partnerID=40&md5=fab3082ae481770069cf4ee693986ae9},
	abstract = {Background: Ontologies house various kinds of domain knowledge in formal structures, primarily in the form of concepts and the associative relationships between them. Ontologies have become integral components of many health information processing environments. Hence, quality assurance of the conceptual content of any ontology is critical. Relationships are foundational to the definition of concepts. Missing relationship errors (i.e., unintended omissions of important definitional relationships) can have a deleterious effect on the quality of an ontology. An abstraction network is a structure that overlays an ontology and provides an alternate, summarization view of its contents. One kind of abstraction network is called an area taxonomy, and a variation of it is called a subtaxonomy. A methodology based on these taxonomies for more readily finding missing relationship errors is explored. Methods: The area taxonomy and the subtaxonomy are deployed to help reveal concepts that have a high likelihood of exhibiting missing relationship errors. A specific top-level grouping unit found within the area taxonomy and subtaxonomy, when deemed to be anomalous, is used as an indicator that missing relationship errors are likely to be found among certain concepts. Two hypotheses pertaining to the effectiveness of our Quality Assurance approach are studied. Results: Our Quality Assurance methodology was applied to the Biological Process hierarchy of the National Cancer Institute thesaurus (NCIt) and SNOMED CT’s Eye/vision finding subhierarchy within its Clinical finding hierarchy. Many missing relationship errors were discovered and confirmed in our analysis. For both test-bed hierarchies, our Quality Assurance methodology yielded a statistically significantly higher number of concepts with missing relationship errors in comparison to a control sample of concepts. Two hypotheses are confirmed by these findings. Conclusions: Quality assurance is a critical part of an ontology’s lifecycle, and automated or semi-automated tools for supporting this process are invaluable. We introduced a Quality Assurance methodology targeted at missing relationship errors. Its successful application to the NCIt’s Biological Process hierarchy and SNOMED CT’s Eye/vision finding subhierarchy indicates that it can be a useful addition to the arsenal of tools available to ontology maintenance personnel. © 2020, The Author(s).},
	publication_stage = {Final}
}

@CONFERENCE{Amdouni201863,
	author = {Amdouni, Emna and Gibaud, Bernard},
	title = {Semantic representation of neuroimaging observations: Proof of concept based on the VASARI terminology},
	year = {2018},
	journal = {IC3K 2018 - Proceedings of the 10th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management},
	volume = {2},
	pages = {63 – 74},
	doi = {10.5220/0006931100630074},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059006132&doi=10.5220%2f0006931100630074&partnerID=40&md5=a56a2a38c59be3472a8d6461ba97cbcc},
	abstract = {The main objective of this work is to facilitate the identification, sharing and reasoning about cerebral tumors observations via the formalization of their semantic meanings in order to facilitate their exploitation in both the clinical practice and research. We have focused our analysis on the VASARI terminology as a proof of concept, but we are convinced that our work can be useful in other biomedical imaging contexts. In this paper, we propose (1) a methodology, a domain ontology and an annotation tool for providing unambiguous formal definitions of neuroimaging data, (2) an experimental work on the REMBRANDT dataset to demonstrate the added value of our work over existing methods, namely DICOM SR and the AIM model. Copyright 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved},
	publication_stage = {Final}
}

@CONFERENCE{Pinto2016,
	author = {Pinto, Luis and Melgar, Andres},
	title = {A classification model for Portuguese documents in the juridical domain},
	year = {2016},
	journal = {Iberian Conference on Information Systems and Technologies, CISTI},
	volume = {2016-July},
	doi = {10.1109/CISTI.2016.7521594},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982151614&doi=10.1109%2fCISTI.2016.7521594&partnerID=40&md5=7d5b7207d723569f99f9cdd05636a8f0},
	abstract = {The attorney's office in Brazil, receive daily a lot of notifications. These notifications must be manually analyzed by procurators to determine what kind of document should they prepare to respond. This situation causes in many cases notifications are not answered in time causing these prescribed. All this has motivated the development of this work whose main objective is the development of a computational model to understand the meaning of each notification and indicate what kind of response should be prepared for every situation. For the construction of this model, machine-learning algorithms are used. The problem is modeled as one of classification using free text documents. The texts were extracted from notification documents, which were written in Portuguese. The method to assess the performance of the algorithms was the area under the curve. During the experiment, four algorithms were evaluated, including k-Nearest Neighbor, Support Vector Machine, Naive Bayes and Complement Naive Bayes. The algorithms were trained using a collection of Portuguese documents in the juridical domain, which includes 5471 documents divided into 8 categories. A 25-fold cross validation method was used to measure the unbiased estimate of these prediction models. This paper is a comparative study of machine learning algorithms for the problem of categorization of notifications. As a result of this study, an algorithm model was constructed in order to classify the documents in the corresponding class. The area under the curve value of Support Vector Machine, k-Nearest Neighbor, Naive Bayes and Complement Naive Bayes was 0.846, 0.831, 0.815 and 0.712 respectively. Our study shows that out of these four classification models Support Vector Machine predicts with highest area under the curve value. © 2016 AISTI.},
	publication_stage = {Final}
}

@ARTICLE{Mogaka20204853,
	author = {Mogaka, John J.O. and James, San E. and Chimbari, Moses J.},
	title = {Leveraging implementation science to improve implementation outcomes in precision medicine},
	year = {2020},
	journal = {American Journal of Translational Research},
	volume = {12},
	number = {9},
	pages = {4853 – 4872},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092462002&partnerID=40&md5=cfd1e1ded863b38f10a4ee813d1b3111},
	abstract = {Background and Purpose: Introduction of omics technologies in clinical practice means increased use of validated biomarkers, through precision medicine (PM). Although implementation science (IS) affords an array of theoretical approaches that can potentially explain PM intervention uptake, their relevance and applicability in PM implementation has not been empirically tested. This article identifies and examines existing implementation frameworks for their applicability in PM, demonstrating how different IS theories can be used to generate testable implementation hypotheses in PM. Methods: A three-step methodology was employed to search and select implementation models: A scoping search in Google Scholar produced 15 commonly used models in healthcare; a systematic search in PUBMED and Web of Science using the names of each model as keywords in search strings produced 290 publications for screening and abstraction; finally, a citation frequency search in the 3 databases produced most cited models that were included in the narrative synthesis. Results: Main concepts and constructs associated with each of the 15 models were identified. Four most cited frameworks in healthcare were: REAIM, CFIR, PRISM and PARiHS. Corresponding constructs were mapped and examined for potential congruence to PM. A generalized PM implementation conceptual framework was developed showing how omics biomarker uptake relates to their evidence base, patient and provider engagement and Big data capabilities of involved organizations. Conclusion: We demonstrated how implementation complexities in PM can be addressed by explicit use of implementation theories. The work here may provide a reference for further research of empirically testing and refining the identified implementation constructs. © 2020 E-Century Publishing Corporation. All rights reserved.},
	publication_stage = {Final}
}

@BOOK{Kaushik20181083,
	author = {Kaushik, Sandeep and Baloni, Priyanka and Midha, Charu K.},
	title = {Text mining resources for bioinformatics},
	year = {2018},
	journal = {Encyclopedia of Bioinformatics and Computational Biology: ABC of Bioinformatics},
	volume = {1-3},
	pages = {1083 – 1092},
	doi = {10.1016/B978-0-12-809633-8.20499-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059641970&doi=10.1016%2fB978-0-12-809633-8.20499-8&partnerID=40&md5=0cb42c4076f550f5d304bf3d334afe04},
	abstract = {Text mining is an evolving field that strives in deriving meaningful information from text. There is abundant information that can be extracted from literature, but without appropriate resources and tools, it’s a major challenge for researchers to utilize this information. The application of this field ranges from simple text search for biomedical research to drug discovery to survey analysis in science and industry. With the advancement in new algorithms, there is an immense opportunity for further development in this field. This article highlights the importance of text mining, describes available resources and case studies that help understanding bioinformatics ways for mining useful information. © 2019 Elsevier Inc. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Zhang202079,
	author = {Zhang, Guo-Qiang and Tao, Shiqiang and Zeng, Ningzhou and Cui, Licong},
	title = {Ontologies as nested facet systems for human-data interaction},
	year = {2020},
	journal = {Semantic Web},
	volume = {11},
	number = {1},
	pages = {79 – 86},
	doi = {10.3233/SW-190378},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079065121&doi=10.3233%2fSW-190378&partnerID=40&md5=f5238a592f2d2f6966cf4a05e4de0a0a},
	abstract = {Irrespective of data size and complexity, query and exploration tools for accessing data resources remain a central linkage for human-data interaction. A fundamental barrier in making query interfaces easier to use, ultimately as easy as online shopping, is the lack of faceted, interactive capabilities. We propose to repurpose existing ontologies by transforming them into nested facet systems (NFS) to support human-data interaction. Two basic issues need to be addressed for this to happen: one is that the structure and quality of ontologies need to be examined and elevated for the purpose of NFS; the second is that mappings from data-source specific metadata to a corresponding NFS need to be developed to support this new generation of NFS-enabled web-interfaces. The purpose of this paper is to introduce the concept of NFS and outline opportunities involved in using ontologies as NFS for querying and exploring data, especially in the biomedical domain. © 2020-IOS Press and the authors. All rights reserved.},
	publication_stage = {Final}
}

@CONFERENCE{Abeysinghe20191982,
	author = {Abeysinghe, Rashmie and Zheng, Fengbo and Hinderer, Eugene W. and Moseley, Hunter N. B. and Cui, Licong},
	title = {A Lexical Approach to Identifying Subtype Inconsistencies in Biomedical Terminologies},
	year = {2019},
	journal = {Proceedings - 2018 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2018},
	pages = {1982 – 1989},
	doi = {10.1109/BIBM.2018.8621511},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062553803&doi=10.1109%2fBIBM.2018.8621511&partnerID=40&md5=d8cb175f6130f3c902b102928d7c386c},
	abstract = {We introduce a lexical-based inference approach for identifying subtype (or is{-}a relation) inconsistencies in biomedical terminologies. Given a terminology, we first represent the name of each concept in the terminology as a sequence of words. We then generate hierarchically-linked and-unlinked pairs of concepts, such that the two concepts in a pair have the same number of words, and contain at least one word in common and a fixed number n of different words (n = 1,2,3,4,5). The linked and unlinked concept-pairs further infer corresponding linked and unlinked term-pairs, respectively. If a linked concept-pair and an unlinked concept-pair infer the same term-pair, we consider this as a potential subtype inconsistency, which may indicate a missing subtype relation or an incorrect subtype relation. We applied this approach to Gene Ontology (GO), National Cancer Institute thesaurus (NCIt) and SNOMED CT. A total of 4,841 potential subtype inconsistencies were found in GO, 2,677 in NCIt, and 53,782 in SNOMED CT. Domain experts evaluated a random sample of 211 potential inconsistencies in GO, and verified that 124 of them are valid (mathrm {i}.mathrm {e}., a precision of 58.77% for detecting subtype inconsistencies in GO). We also performed a preliminary study on the extent to which external knowledge in the Unified Medical Language System (UMLS) can provide supporting evidence for validating the detected potential inconsistencies: 0.54% (=26/4841) for GO, 11.43% (=306/2677) for NCIt, and 3.61% (=1940/53782) for SNOMED CT. Results indicate that our lexical-based inference approach is a promising way to identify subtype inconsistencies and facilitates the quality improvement of biomedical terminologies. © 2018 IEEE.},
	publication_stage = {Final}
}

@ARTICLE{de Campos Souza2020,
	author = {de Campos Souza, Paulo Vitor},
	title = {Fuzzy neural networks and neuro-fuzzy networks: A review the main techniques and applications used in the literature},
	year = {2020},
	journal = {Applied Soft Computing Journal},
	volume = {92},
	doi = {10.1016/j.asoc.2020.106275},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084185252&doi=10.1016%2fj.asoc.2020.106275&partnerID=40&md5=8aa0a4f6f19de74a8416155554c4bc5c},
	abstract = {This paper presents a review of the central theories involved in hybrid models based on fuzzy systems and artificial neural networks, mainly focused on supervised methods for training hybrid models. The basic concepts regarding the history of hybrid models, from the first proposed model to the current advances, the composition and the functionalities in their architecture, the data treatment and the training methods of these intelligent models are presented to the reader so that the evolution of this category of intelligent systems can be evidenced. Finally, the features of the leading models and their applications are presented to the reader. We conclude that the fuzzy neural network models and their derivations are efficient in constructing a system with a high degree of accuracy and an appropriate level of interpretability working in a wide range of areas of economics and science. © 2020 Elsevier B.V.},
	publication_stage = {Final}
}

@ARTICLE{Goyal201821,
	author = {Goyal, Archana and Gupta, Vishal and Kumar, Manish},
	title = {Recent Named Entity Recognition and Classification techniques: A systematic review},
	year = {2018},
	journal = {Computer Science Review},
	volume = {29},
	pages = {21 – 43},
	doi = {10.1016/j.cosrev.2018.06.001},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052754065&doi=10.1016%2fj.cosrev.2018.06.001&partnerID=40&md5=6c7c7906148377be4d9252f07a2ad103},
	abstract = {Textual information is becoming available in abundance on the web, arising the requirement of techniques and tools to extract the meaningful information. One of such an important information extraction task is Named Entity Recognition and Classification. It is the problem of finding the members of various predetermined classes, such as person, organization, location, date/time, quantities, numbers etc. The concept of named entity extraction was first proposed in Sixth Message Understanding Conference in 1996. Since then, a number of techniques have been developed by many researchers for extracting diversity of entities from different languages and genres of text. Still, there is a growing interest among research community to develop more new approaches to extract diverse named entities which are helpful in various natural language applications. Here we present a survey of developments and progresses made in Named Entity Recognition and Classification research. © 2018 Elsevier Inc.},
	publication_stage = {Final}
}

@ARTICLE{Zheng2018135,
	author = {Zheng, Ling and Chen, Yan and Elhanan, Gai and Perl, Yehoshua and Geller, James and Ochs, Christopher},
	title = {Complex overlapping concepts: An effective auditing methodology for families of similarly structured BioPortal ontologies},
	year = {2018},
	journal = {Journal of Biomedical Informatics},
	volume = {83},
	pages = {135 – 149},
	doi = {10.1016/j.jbi.2018.05.015},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048256040&doi=10.1016%2fj.jbi.2018.05.015&partnerID=40&md5=95b0add74a4afcbd5196d4ff0112f6cc},
	abstract = {In previous research, we have demonstrated for a number of ontologies that structurally complex concepts (for different definitions of “complex”) in an ontology are more likely to exhibit errors than other concepts. Thus, such complex concepts often become fertile ground for quality assurance (QA) in ontologies. They should be audited first. One example of complex concepts is given by “overlapping concepts” (to be defined below.) Historically, a different auditing methodology had to be developed for every single ontology. For better scalability and efficiency, it is desirable to identify family-wide QA methodologies. Each such methodology would be applicable to a whole family of similar ontologies. In past research, we had divided the 685 ontologies of BioPortal into families of structurally similar ontologies. We showed for four ontologies of the same large family in BioPortal that “overlapping concepts” are indeed statistically significantly more likely to exhibit errors. In order to make an authoritative statement concerning the success of “overlapping concepts” as a methodology for a whole family of similar ontologies (or of large subhierarchies of ontologies), it is necessary to show that “overlapping concepts” have a higher likelihood of errors for six out of six ontologies of the family. In this paper, we are demonstrating for two more ontologies that “overlapping concepts” can successfully predict groups of concepts with a higher error rate than concepts from a control group. The fifth ontology is the Neoplasm subhierarchy of the National Cancer Institute thesaurus (NCIt). The sixth ontology is the Infectious Disease subhierarchy of SNOMED CT. We demonstrate quality assurance results for both of them. Furthermore, in this paper we observe two novel, important, and useful phenomena during quality assurance of “overlapping concepts.” First, an erroneous “overlapping concept” can help with discovering other erroneous “non-overlapping concepts” in its vicinity. Secondly, correcting erroneous “overlapping concepts” may turn them into “non-overlapping concepts.” We demonstrate that this may reduce the complexity of parts of the ontology, which in turn makes the ontology more comprehensible, simplifying maintenance and use of the ontology. © 2018 Elsevier Inc.},
	publication_stage = {Final}
}

@ARTICLE{Kamdar2019,
	author = {Kamdar, Maulik R. and Fernández, Javier D. and Polleres, Axel and Tudorache, Tania and Musen, Mark A.},
	title = {Enabling Web-scale data integration in biomedicine through Linked Open Data},
	year = {2019},
	journal = {npj Digital Medicine},
	volume = {2},
	number = {1},
	doi = {10.1038/s41746-019-0162-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089606287&doi=10.1038%2fs41746-019-0162-5&partnerID=40&md5=03de82757be86c2190c4541bc8c8035b},
	abstract = {The biomedical data landscape is fragmented with several isolated, heterogeneous data and knowledge sources, which use varying formats, syntaxes, schemas, and entity notations, existing on the Web. Biomedical researchers face severe logistical and technical challenges to query, integrate, analyze, and visualize data from multiple diverse sources in the context of available biomedical knowledge. Semantic Web technologies and Linked Data principles may aid toward Web-scale semantic processing and data integration in biomedicine. The biomedical research community has been one of the earliest adopters of these technologies and principles to publish data and knowledge on the Web as linked graphs and ontologies, hence creating the Life Sciences Linked Open Data (LSLOD) cloud. In this paper, we provide our perspective on some opportunities proffered by the use of LSLOD to integrate biomedical data and knowledge in three domains: (1) pharmacology, (2) cancer research, and (3) infectious diseases. We will discuss some of the major challenges that hinder the wide-spread use and consumption of LSLOD by the biomedical research community. Finally, we provide a few technical solutions and insights that can address these challenges. Eventually, LSLOD can enable the development of scalable, intelligent infrastructures that support artificial intelligence methods for augmenting human intelligence to achieve better clinical outcomes for patients, to enhance the quality of biomedical research, and to improve our understanding of living systems. © 2019, The Author(s).},
	publication_stage = {Final}
}

@ARTICLE{Beel2016305,
	author = {Beel, Joeran and Gipp, Bela and Langer, Stefan and Breitinger, Corinna},
	title = {Research-paper recommender systems: a literature survey},
	year = {2016},
	journal = {International Journal on Digital Libraries},
	volume = {17},
	number = {4},
	pages = {305 – 338},
	doi = {10.1007/s00799-015-0156-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937865069&doi=10.1007%2fs00799-015-0156-0&partnerID=40&md5=7e99bb8676cb4c486bffda17ce91fd8b},
	abstract = {In the last 16 years, more than 200 research articles were published about research-paper recommender systems. We reviewed these articles and present some descriptive statistics in this paper, as well as a discussion about the major advancements and shortcomings and an overview of the most common recommendation concepts and approaches. We found that more than half of the recommendation approaches applied content-based filtering (55 %). Collaborative filtering was applied by only 18 % of the reviewed approaches, and graph-based recommendations by 16 %. Other recommendation concepts included stereotyping, item-centric recommendations, and hybrid recommendations. The content-based filtering approaches mainly utilized papers that the users had authored, tagged, browsed, or downloaded. TF-IDF was the most frequently applied weighting scheme. In addition to simple terms, n-grams, topics, and citations were utilized to model users’ information needs. Our review revealed some shortcomings of the current research. First, it remains unclear which recommendation concepts and approaches are the most promising. For instance, researchers reported different results on the performance of content-based and collaborative filtering. Sometimes content-based filtering performed better than collaborative filtering and sometimes it performed worse. We identified three potential reasons for the ambiguity of the results. (A) Several evaluations had limitations. They were based on strongly pruned datasets, few participants in user studies, or did not use appropriate baselines. (B) Some authors provided little information about their algorithms, which makes it difficult to re-implement the approaches. Consequently, researchers use different implementations of the same recommendations approaches, which might lead to variations in the results. (C) We speculated that minor variations in datasets, algorithms, or user populations inevitably lead to strong variations in the performance of the approaches. Hence, finding the most promising approaches is a challenge. As a second limitation, we noted that many authors neglected to take into account factors other than accuracy, for example overall user satisfaction. In addition, most approaches (81 %) neglected the user-modeling process and did not infer information automatically but let users provide keywords, text snippets, or a single paper as input. Information on runtime was provided for 10 % of the approaches. Finally, few research papers had an impact on research-paper recommender systems in practice. We also identified a lack of authority and long-term research interest in the field: 73 % of the authors published no more than one paper on research-paper recommender systems, and there was little cooperation among different co-author groups. We concluded that several actions could improve the research landscape: developing a common evaluation framework, agreement on the information to include in research papers, a stronger focus on non-accuracy aspects and user modeling, a platform for researchers to exchange information, and an open-source framework that bundles the available recommendation approaches. © 2015, Springer-Verlag Berlin Heidelberg.},
	publication_stage = {Final}
}

@ARTICLE{Elhefny2017162,
	author = {Elhefny, Mohammad Abdelrahman and Elmogy, Mohammed and Elfetouh, Ahmed A. and Badria, Farid A.},
	title = {Developing a fuzzy OWL ontology for obesity related cancer domain},
	year = {2017},
	journal = {International Journal of Medical Engineering and Informatics},
	volume = {9},
	number = {2},
	pages = {162 – 187},
	doi = {10.1504/IJMEI.2017.083092},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016067397&doi=10.1504%2fIJMEI.2017.083092&partnerID=40&md5=c0067e67fbd606fe3780847a5b474a02},
	abstract = {Obesity is associated with various diseases, particularly cardiovascular diseases, diabetes type 2, obstructive sleep apnea, certain types of cancer, osteoarthritis, and asthma. The knowledge of the obesity related cancer (ORC) domain is highly required to be represented in a structured and formalised shape. In this paper, we develop an ontology to represent ORC domain knowledge with its diseases, symptoms, diagnosis, and treatments. The proposed ontology is based on the Web Ontology Language (OWL 2) integrated with the fuzzy logic. The fuzzy developed ontology handles the overlapping concepts, ingesting more concepts, and copes with the linguistic domain variables, which were not possible using the regular ontologies. It allows the users to query the fuzzy Dl reasoner for element and answer them with the fuzzy ontology. By developing the fuzzy ORC ontology, it is expected to be a good practice for the ontologists and knowledge engineers. © 2017 Inderscience Enterprises Ltd.},
	publication_stage = {Final}
}

@ARTICLE{Candanedo2019409,
	author = {Candanedo, Inés Sittón},
	title = {A self-organized multiagent system for industry 4.0},
	year = {2019},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {801},
	pages = {409 – 413},
	doi = {10.1007/978-3-319-99608-0_55},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061736773&doi=10.1007%2f978-3-319-99608-0_55&partnerID=40&md5=4c88bd6c5976c94338917dcec0d802c1},
	abstract = {Industry 4.0 has revolutionized the recent years because the requirements in all domains of manufacturing, production or sales are dynamics and uncertainty and with them the challenges such as emerging technologies, great volumes of data and to make decisions in real time. This paper describes the advantage of a self-organized multiagent system to addresses the problem of data and how process them in Industry 4.0 environment. © Springer Nature Switzerland AG 2019.},
	publication_stage = {Final}
}

@ARTICLE{Burger2016949,
	author = {Burger, Gerard and Abu-Hanna, Ameen and De Keizer, Nicolette and Cornet, Ronald},
	title = {Natural language processing in pathology: A scoping review},
	year = {2016},
	journal = {Journal of Clinical Pathology},
	volume = {69},
	number = {11},
	pages = {949 – 955},
	doi = {10.1136/jclinpath-2016-203872},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979084834&doi=10.1136%2fjclinpath-2016-203872&partnerID=40&md5=18b0f6f0079d29459276e7f3935412a0},
	abstract = {Background Encoded pathology data are key for medical registries and analyses, but pathology information is often expressed as free text. Objective We reviewed and assessed the use of NLP (natural language processing) for encoding pathology documents. Materials and methods Papers addressing NLP in pathology were retrieved from PubMed, Association for Computing Machinery (ACM) Digital Library and Association for Computational Linguistics (ACL) Anthology. We reviewed and summarised the study objectives; NLP methods used and their validation; software implementations; the performance on the dataset used and any reported use in practice. Results The main objectives of the 38 included papers were encoding and extraction of clinically relevant information from pathology reports. Common approaches were word/phrase matching, probabilistic machine learning and rule-based systems. Five papers (13%) compared different methods on the same dataset. Four papers did not specify the method(s) used. 18 of the 26 studies that reported F-measure, recall or precision reported values of over 0.9. Proprietary software was the most frequently mentioned category (14 studies); General Architecture for Text Engineering (GATE) was the most applied architecture overall. Practical system use was reported in four papers. Most papers used expert annotation validation. Conclusions Different methods are used in NLP research in pathology, and good performances, that is, high precision and recall, high retrieval/removal rates, are reported for all of these. Lack of validation and of shared datasets precludes performance comparison. More comparative analysis and validation are needed to provide better insight into the performance and merits of these methods. © Published by the BMJ Publishing Group Limited.},
	publication_stage = {Final}
}

@ARTICLE{Khanam2019,
	author = {Khanam, Shirin Akther and Liu, Fei and Chen, Yi-Ping Phoebe},
	title = {Comprehensive structured knowledge base system construction with natural language presentation},
	year = {2019},
	journal = {Human-centric Computing and Information Sciences},
	volume = {9},
	number = {1},
	doi = {10.1186/s13673-019-0184-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067012018&doi=10.1186%2fs13673-019-0184-7&partnerID=40&md5=739c04fcf68cd7d0b2761642ce590393},
	abstract = {Constructing an ontology-based machine-readable knowledge base system from different sources with minimum human intervention, also known as ontology-based machine-readable knowledge base construction (OMRKBC), has been a long-term outstanding problem. One of the issues is how to build a large-scale OMRKBC process with appropriate structural information. To address this issue, we propose Natural Language Independent Knowledge Representation (NLIKR), a method which regards each word as a concept which should be defined by its relations with other concepts. Using NLIKR, we propose a framework for the OMRKBC process to automatically develop a comprehensive ontology-based machine-readable knowledge base system (OMRKBS) using well-built structural information. Firstly, as part of this framework, we propose formulas to discover concepts and their relations in the OMRKBS. Secondly, the challenges in obtaining rich structured information are resolved through the development of algorithms and rules. Finally, rich structured information is built in the OMRKBS. OMRKBC allows the efficient search of words and supports word queries with a specific attribute. We conduct experiments and analyze the results of relational information extraction, with the results showing that OMRKBS had an accuracy of 84% which was higher than the other knowledge base systems, namely ConceptNet, DBpedia and WordNet. © 2019, The Author(s).},
	publication_stage = {Final}
}

@ARTICLE{Celebi20201,
	author = {Celebi, Remzi and Moreira, Joao Rebelo and Hassan, Ahmed A. and Ayyar, Sandeep and Ridder, Lars and Kuhn, Tobias and Dumontier, Michel},
	title = {Towards FAIR protocols and workflows: The OpenPREDICT use case},
	year = {2020},
	journal = {PeerJ Computer Science},
	volume = {6},
	pages = {1 – 29},
	doi = {10.7717/PEERJ-CS.281},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092652339&doi=10.7717%2fPEERJ-CS.281&partnerID=40&md5=45668a81fab1d033d00c3455cc8c258e},
	abstract = {It is essential for the advancement of science that researchers share, reuse and reproduce each other's workflows and protocols. The FAIR principles are a set of guidelines that aim to maximize the value and usefulness of research data, and emphasize the importance of making digital objects findable and reusable by others. The question of how to apply these principles not just to data but also to the workflows and protocols that consume and produce them is still under debate and poses a number of challenges. In this paper we describe a two-fold approach of simultaneously applying the FAIR principles to scientific workflows as well as the involved data. We apply and evaluate our approach on the case of the PREDICT workflow, a highly cited drug repurposing workflow. This includes FAIRification of the involved datasets, as well as applying semantic technologies to represent and store data about the detailed versions of the general protocol, of the concrete workflow instructions, and of their execution traces. We propose a semantic model to address these specific requirements and was evaluated by answering competency questions. This semantic model consists of classes and relations from a number of existing ontologies, including Workflow4ever, PROV, EDAM, and BPMN. This allowed us then to formulate and answer new kinds of competency questions. Our evaluation shows the high degree to which our FAIRified OpenPREDICT workflow now adheres to the FAIR principles and the practicality and usefulness of being able to answer our new competency questions. © 2020 authors.},
	publication_stage = {Final}
}

@CONFERENCE{Ahmad2018152,
	author = {Ahmad, Mahmood and Odeh, Mohammed and Green, Stewart},
	title = {Derivation of a Semantic Cancer Care Information Architecture from Riva-based Business Process Architecture using the BPAOntoEIA Framework},
	year = {2018},
	journal = {Proceedings - 2018 1st International Conference on Cancer Care Informatics, CCI 2018},
	pages = {152 – 164},
	doi = {10.1109/CANCERCARE.2018.8618193},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062410055&doi=10.1109%2fCANCERCARE.2018.8618193&partnerID=40&md5=629f1f8677a35a3793a700a95ce7677c},
	abstract = {Contemporary Enterprise Information Architecture (EIA) design methods lack the knowledge of business process in an enterprise. The object-based business process architecture (BPA) design methods generate useful information of business entities and process that can assist in deriving a business process-aware EIA in a semi-automatic manner. This paper reports on the impact of applying the BPAOntoEIA framework to a Cancer Care & Registration actual case study in order to derive a semantic EIA from its associated Riva-based BPA semantic model. The benefit of this framework manifests itself in the knowledge of business processes, their orchestration and models of the cancer care organisation while inter-connecting the business processes with information categories, and thus enhancing the business-IT alignment of the given cancer care & Registration process architecture. An automation of 80% of the semantic EIA derivation is reported and EIA quality metrics were developed that have informed the evaluation of the automated derivation of a corresponding Cancer Care and Registration EIA. © 2018 IEEE.},
	publication_stage = {Final}
}

@ARTICLE{Khor201953,
	author = {Khor, Richard C. and Nguyen, Anthony and O'Dwyer, John and Kothari, Gargi and Sia, Joseph and Chang, David and Ng, Sweet Ping and Duchesne, Gillian M. and Foroudi, Farshad},
	title = {Extracting tumour prognostic factors from a diverse electronic record dataset in genito-urinary oncology},
	year = {2019},
	journal = {International Journal of Medical Informatics},
	volume = {121},
	pages = {53 – 57},
	doi = {10.1016/j.ijmedinf.2018.10.008},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056861700&doi=10.1016%2fj.ijmedinf.2018.10.008&partnerID=40&md5=bf60959ef9c1139b6f3e18686c2dded2},
	abstract = {Objectives: To implement a system for unsupervised extraction of tumor stage and prognostic data in patients with genitourinary cancers using clinicopathological and radiology text. Methods: A corpus of 1054 electronic notes (clinician notes, radiology reports and pathology reports) was annotated for tumor stage, prostate specific antigen (PSA) and Gleason grade. Annotations from five clinicians were reconciled to form a gold standard dataset. A training dataset of 386 documents was sequestered. The Medtex algorithm was adapted using the training dataset. Results: Adapted Medtex equaled or exceeded human performance in most annotations, except for implicit M stage (F-measure of 0.69 vs 0.84) and PSA (0.92 vs 0.96). Overall Medtex performed with an F-measure of 0.86 compared to human annotations of 0.92. There was significant inter-observer variability when comparing human annotators to the gold standard. Conclusions: The Medtex algorithm performed similarly to human annotators for extracting stage and prognostic data from varied clinical texts. © 2018 Elsevier B.V.},
	publication_stage = {Final}
}

@ARTICLE{Zeng2019139,
	author = {Zeng, Zexian and Deng, Yu and Li, Xiaoyu and Naumann, Tristan and Luo, Yuan},
	title = {Natural Language Processing for EHR-Based Computational Phenotyping},
	year = {2019},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	volume = {16},
	number = {1},
	pages = {139 – 153},
	doi = {10.1109/TCBB.2018.2849968},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049088419&doi=10.1109%2fTCBB.2018.2849968&partnerID=40&md5=4f3dcef5d14a691657d0a077a80cd927},
	abstract = {This article reviews recent advances in applying natural language processing (NLP) to Electronic Health Records (EHRs) for computational phenotyping. NLP-based computational phenotyping has numerous applications including diagnosis categorization, novel phenotype discovery, clinical trial screening, pharmacogenomics, drug-drug interaction (DDI), and adverse drug event (ADE) detection, as well as genome-wide and phenome-wide association studies. Significant progress has been made in algorithm development and resource construction for computational phenotyping. Among the surveyed methods, well-designed keyword search and rule-based systems often achieve good performance. However, the construction of keyword and rule lists requires significant manual effort, which is difficult to scale. Supervised machine learning models have been favored because they are capable of acquiring both classification patterns and structures from data. Recently, deep learning and unsupervised learning have received growing attention, with the former favored for its performance and the latter for its ability to find novel phenotypes. Integrating heterogeneous data sources have become increasingly important and have shown promise in improving model performance. Often, better performance is achieved by combining multiple modalities of information. Despite these many advances, challenges and opportunities remain for NLP-based computational phenotyping, including better model interpretability and generalizability, and proper characterization of feature relations in clinical narratives. © 2004-2012 IEEE.},
	publication_stage = {Final}
}

@ARTICLE{Alghamdi20211,
	author = {Alghamdi, Norah Saleh},
	title = {Health data warehouses: Reviewing advanced solutions for medical knowledge discovery},
	year = {2021},
	journal = {International Journal of Business Intelligence and Data Mining},
	volume = {19},
	number = {1},
	pages = {1 – 32},
	doi = {10.1504/IJBIDM.2021.115953},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109473528&doi=10.1504%2fIJBIDM.2021.115953&partnerID=40&md5=0979e81a6e552260d075d6312e4ef450},
	abstract = {The implementation of a data warehouse and a decision support system by utilising the capabilities of information retrieval and knowledge discovery tools in the healthcare fields has allowed for the enhancement in the offered healthcare. In this work, we present a review of recent data warehouses and decision support systems in the healthcare domain with their significance, and applications of evidence-based medicine, electronic health records, and nursing. Given the growing trend on their implementation in healthcare services, researches, and education, we present here the most recent publications that employ these tools to produce suitable decisions for patients or health providers. For all the reviewed publications, we have intensively explored their problems, suggested solutions, utilised methods, and their findings. We have also highlighted the strength of the existing approaches and identified potential drawbacks including data correctness, completeness, consistency, and integration to provide proper medical decision-making. Copyright © 2021 Inderscience Enterprises Ltd.},
	publication_stage = {Final}
}

@ARTICLE{Giannaris2020,
	author = {Giannaris, Pericles and Al-Taie, Zainab and Kovalenko, Mikhail and Thanintorn, Nattapon and Kholod, Olha and Innokenteva, Yulia and Coberly, Emily and Frazier, Shellaine and Laziuk, Katsiarina and Popescu, Mihail and Shyu, Chi-Ren and Xu, Dong and Hammer, Richard and Shin, Dmitriy},
	title = {Artificial intelligence-driven structurization of diagnostic information in free-text pathology reports},
	year = {2020},
	journal = {Journal of Pathology Informatics},
	volume = {11},
	number = {1},
	doi = {10.4103/jpi.jpi_30_19},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085389838&doi=10.4103%2fjpi.jpi_30_19&partnerID=40&md5=f70cd4738dfcefe24ba8d244ece72557},
	abstract = {Background: Free-text sections of pathology reports contain the most important information from a diagnostic standpoint. However, this information is largely underutilized for computer-based analytics. The vast majority of NLP-based methods lack a capacity to accurately extract complex diagnostic entities and relationships among them as well as to provide an adequate knowledge representation for downstream data-mining applications. Methods: In this paper, we introduce a novel informatics pipeline that extends open information extraction (openIE) techniques with artificial intelligence (AI) based modeling to extract and transform complex diagnostic entities and relationships among them into Knowledge Graphs (KGs) of relational triples (RTs). Results: Evaluation studies have demonstrated that the pipeline's output significantly differs from a random process. The semantic similarity with original reports is high (Mean Weighted Overlap of 0.83). The precision and recall of extracted RTs based on experts' assessment were 0.925 and 0.841 respectively (P <0.0001). Inter-rater agreement was significant at 93.6% and inter-rated reliability was 81.8%. Conclusion: The results demonstrated important properties of the pipeline such as high accuracy, minimality and adequate knowledge representation. Therefore, we conclude that the pipeline can be used in various downstream data-mining applications to assist diagnostic medicine. © 2019 Wolters Kluwer Medknow Publications. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Sen201977,
	author = {Sen, Arun and Al Kawam, Ahmad and Datta, Aniruddha},
	title = {Emergence of DSS efforts in genomics: Past contributions and challenges},
	year = {2019},
	journal = {Decision Support Systems},
	volume = {116},
	pages = {77 – 90},
	doi = {10.1016/j.dss.2018.10.011},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055990068&doi=10.1016%2fj.dss.2018.10.011&partnerID=40&md5=751208b9e7db92de681418764c4cdb14},
	abstract = {Large amounts of data in biomedical research (from clinical data to gene expression data) are being generated. Use of these data sets and their associated knowledge are essential to understand the biological mechanisms behind diseases. While patients' clinical data from EHR can help researchers accurately and appropriately trace the performance of various kinds of medicines on the patients, the microarray data for the same pool of patients can contain valuable information for discovery of disease-associated gene expression patterns and can help classify the patients. However, research in the area of integrating genomic data with clinical data is still in its infancy and is riddled with many challenges. Even though data and knowledge sets are easily available from genome sequences and protein structural data of organisms, they usually are of many different varieties. Integrating them for a better understanding of biological functions at all levels is complicated. If we want to obtain the full benefit of functional genomics, we need to find a seamless way to integrate large amounts of patient datasets with genomic datasets in the field of biomedicine. Few papers in the decision support systems (DSS) literature provide an overview of Genomic Clinical Decision Support (GCDS) challenges that span data, knowledge, input/output, and architecture/implementation. This paper presents a unique effort dedicated to providing a comprehensive listing and a concise description of the DSS methodological challenges that arise from integrating complex and massive-scale genomic data with Clinical Decision Support (CDS) systems. © 2018 Elsevier B.V.},
	publication_stage = {Final}
}

@ARTICLE{Karystianis201727,
	author = {Karystianis, George and Thayer, Kristina and Wolfe, Mary and Tsafnat, Guy},
	title = {Evaluation of a rule-based method for epidemiological document classification towards the automation of systematic reviews},
	year = {2017},
	journal = {Journal of Biomedical Informatics},
	volume = {70},
	pages = {27 – 34},
	doi = {10.1016/j.jbi.2017.04.004},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018938719&doi=10.1016%2fj.jbi.2017.04.004&partnerID=40&md5=1e26b430669cdbe2427a60f795d76263},
	abstract = {Introduction Most data extraction efforts in epidemiology are focused on obtaining targeted information from clinical trials. In contrast, limited research has been conducted on the identification of information from observational studies, a major source for human evidence in many fields, including environmental health. The recognition of key epidemiological information (e.g., exposures) through text mining techniques can assist in the automation of systematic reviews and other evidence summaries. Method We designed and applied a knowledge-driven, rule-based approach to identify targeted information (study design, participant population, exposure, outcome, confounding factors, and the country where the study was conducted) from abstracts of epidemiological studies included in several systematic reviews of environmental health exposures. The rules were based on common syntactical patterns observed in text and are thus not specific to any systematic review. To validate the general applicability of our approach, we compared the data extracted using our approach versus hand curation for 35 epidemiological study abstracts manually selected for inclusion in two systematic reviews. Results The returned F-score, precision, and recall ranged from 70% to 98%, 81% to 100%, and 54% to 97%, respectively. The highest precision was observed for exposure, outcome and population (100%) while recall was best for exposure and study design with 97% and 89%, respectively. The lowest recall was observed for the population (54%), which also had the lowest F-score (70%). Conclusion The generated performance of our text-mining approach demonstrated encouraging results for the identification of targeted information from observational epidemiological study abstracts related to environmental exposures. We have demonstrated that rules based on generic syntactic patterns in one corpus can be applied to other observational study design by simple interchanging the dictionaries aiming to identify certain characteristics (i.e., outcomes, exposures). At the document level, the recognised information can assist in the selection and categorization of studies included in a systematic review. © 2017 Elsevier Inc.},
	publication_stage = {Final}
}

@ARTICLE{Soysal2017188,
	author = {Soysal, E. and Lee, H.-J. and Zhang, Y. and Huang, L.-C.-C. and Chen, X. and Wei, Q. and Zheng, W. and Chang, J.T. and Cohen, T. and Sun, J. and Xu, H.},
	title = {CATTLE (CAncer treatment treasury with linked evidence): An integrated knowledge base for personalized oncology research and practice},
	year = {2017},
	journal = {CPT: Pharmacometrics and Systems Pharmacology},
	volume = {6},
	number = {3},
	pages = {188 – 196},
	doi = {10.1002/psp4.12174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015240590&doi=10.1002%2fpsp4.12174&partnerID=40&md5=85be9154edcaf22da19a86f808b1e2fb},
	abstract = {Despite the existence of various databases cataloging cancer drugs, there is an emerging need to support the development and application of personalized therapies, where an integrated understanding of the clinical factors and drug mechanism of action and its gene targets is necessary. We have developed CATTLE (CAncer Treatment Treasury with Linked Evidence), a comprehensive cancer drug knowledge base providing information across the complete spectrum of the drug life cycle. The CATTLE system collects relevant data from 22 heterogeneous databases, integrates them into a unified model centralized on drugs, and presents comprehensive drug information via an interactive web portal with a download function. A total of 2,323 unique cancer drugs are currently linked to rich information from these databases in CATTLE. Through two use cases, we demonstrate that CATTLE can be used in supporting both research and practice in personalized oncology. © 2017 The Authors CPT: Pharmacometrics & Systems Pharmacology published by Wiley Periodicals, Inc. on behalf of American Society for Clinical Pharmacology and Therapeutics.},
	publication_stage = {Final}
}

@ARTICLE{Bernasconi202130,
	author = {Bernasconi, Anna and Canakoglu, Arif and Masseroli, Marco and Ceri, Stefano},
	title = {The road towards data integration in human genomics: Players, steps and interactions},
	year = {2021},
	journal = {Briefings in Bioinformatics},
	volume = {22},
	number = {1},
	pages = {30 – 44},
	doi = {10.1093/bib/bbaa080},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100280629&doi=10.1093%2fbib%2fbbaa080&partnerID=40&md5=a4480b75966566f176c7912586a72762},
	abstract = {Thousands of new experimental datasets are becoming available every day; in many cases, they are produced within the scope of large cooperative efforts, involving a variety of laboratories spread all over the world, and typically open for public use. Although the potential collective amount of available information is huge, the effective combination of such public sources is hindered by data heterogeneity, as the datasets exhibit a wide variety of notations and formats, concerning both experimental values and metadata. Thus, data integration is becoming a fundamental activity, to be performed prior to data analysis and biological knowledge discovery, consisting of subsequent steps of data extraction, normalization, matching and enrichment; once applied to heterogeneous data sources, it builds multiple perspectives over the genome, leading to the identification of meaningful relationships that could not be perceived by using incompatible data formats. In this paper, we first describe a technological pipeline from data production to data integration; we then propose a taxonomy of genomic data players (based on the distinction between contributors, repository hosts, consortia, integrators and consumers) and apply the taxonomy to describe about 30 important players in genomic data management. We specifically focus on the integrator players and analyse the issues in solving the genomic data integration challenges, as well as evaluate the computational environments that they provide to follow up data integration by means of visualization and analysis tools.  © 2020 The Author(s).},
	publication_stage = {Final}
}

@ARTICLE{Chan2021,
	author = {Chan, Lauren and Vasilevsky, Nicole and Thessen, Anne and McMurry, Julie and Haendel, Melissa},
	title = {The landscape of nutri-informatics: A review of current resources and challenges for integrative nutrition research},
	year = {2021},
	journal = {Database},
	volume = {2021},
	doi = {10.1093/database/baab003},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100489613&doi=10.1093%2fdatabase%2fbaab003&partnerID=40&md5=7826713ec7b01dd273674e2c6c892316},
	abstract = {Informatics has become an essential component of research in the past few decades, capitalizing on the efficiency and power of computation to improve the knowledge gained from increasing quantities and types of data. While other fields of research such as genomics are well represented in informatics resources, nutrition remains underrepresented. Nutrition is one of the most integral components of human life, and it impacts individuals far beyond just nutrient provisions. For example, nutrition plays a role in cultural practices, interpersonal relationships and body image. Despite this, integrated computational investigations have been limited due to challenges within nutrition informatics (nutri-informatics) and nutrition data. The purpose of this review is to describe the landscape of nutri-informatics resources available for use in computational nutrition research and clinical utilization. In particular, we will focus on the application of biomedical ontologies and their potential to improve the standardization and interoperability of nutrition terminologies and relationships between nutrition and other biomedical disciplines such as disease and phenomics. Additionally, we will highlight challenges currently faced by the nutri-informatics community including experimental design, data aggregation and the roles scientific journals and primary nutrition researchers play in facilitating data reuse and successful computational research. Finally, we will conclude with a call to action to create and follow community standards regarding standardization of language, documentation specifications and requirements for data reuse. With the continued movement toward community standards of this kind, the entire nutrition research community can transition toward greater usage of Findability, Accessibility, Interoperability and Reusability principles and in turn more transparent science.  © 2021 The Author(s) 2021. Published by Oxford University Press.},
	publication_stage = {Final}
}

@ARTICLE{Soysal20191041,
	author = {Soysal, Ergin and Warner, Jeremy L. and Wang, Jingqi and Jiang, Min and Harvey, Krysten and Jain, Sandeep Kumar and Dong, Xiao and Song, Hsing-Yi and Siddhanamatha, Harish and Wang, Liwei and Dai, Qi and Chen, Qingxia and Du, Xianglin and Tao, Cui and Yang, Ping and Denny, Joshua Charles and Liu, Hongfang and Xu, Hua},
	title = {Developing customizable cancer information extraction modules for pathology reports using clamp},
	year = {2019},
	journal = {Studies in Health Technology and Informatics},
	volume = {264},
	pages = {1041 – 1045},
	doi = {10.3233/SHTI190383},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071496267&doi=10.3233%2fSHTI190383&partnerID=40&md5=27a9621c9eaf6d93fa7928bd5f0c000c},
	abstract = {Natural language processing (NLP) technologies have been successfully applied to cancer research by enabling automated phenotypic information extraction from narratives in electronic health records (EHRs) such as pathology reports; however, developing customized NLP solutions requires substantial effort. To facilitate the adoption of NLP in cancer research, we have developed a set of customizable modules for extracting comprehensive types of cancer-related information in pathology reports (e.g., tumor size, tumor stage, and biomarkers), by leveraging the existing CLAMP system, which provides user-friendly interfaces for building customized NLP solutions for individual needs. Evaluation using annotated data at Vanderbilt University Medical Center showed that CLAMP-Cancer could extract diverse types of cancer information with good F-measures (0.80-0.98). We then applied CLAMP-Cancer to an information extraction task at Mayo Clinic and showed that we can quickly build a customized NLP system with comparable performance with an existing system at Mayo Clinic. CLAMP-Cancer is freely available for academic use. © 2019 International Medical Informatics Association (IMIA) and IOS Press.},
	publication_stage = {Final}
}

@ARTICLE{Bouaud2020,
	author = {Bouaud, Jacques and Pelayo, Sylvia and Lamy, Jean-Baptiste and Prebet, Coralie and Ngo, Charlotte and Teixeira, Luis and Guézennec, Gilles and Séroussi, Brigitte},
	title = {Implementation of an ontological reasoning to support the guideline-based management of primary breast cancer patients in the DESIREE project},
	year = {2020},
	journal = {Artificial Intelligence in Medicine},
	volume = {108},
	doi = {10.1016/j.artmed.2020.101922},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088389882&doi=10.1016%2fj.artmed.2020.101922&partnerID=40&md5=505feefa35a32281db39e03b11771f77},
	abstract = {The DESIREE project has developed a platform offering several complementary therapeutic decision support modules to improve the quality of care for breast cancer patients. All modules are operating consistently with a common breast cancer knowledge model (BCKM) following the generic entity-attribute-value model. The BCKM is formalized as an ontology including both the data model to represent clinical patient information and the termino-ontological model to represent the application domain concepts. This ontological model is used to describe data semantics and to allow for reasoning at different levels of abstraction. We present the guideline-based decision support module (GL-DSS). Three breast cancer clinical practice guidelines have been formalized as decision rules including evidence levels, conformance levels, and two types of dependency, “refinement” and “complement”, used to build complete care plans from the reconciliation of atomic recommendations. The system has been assessed on 138 decisions previously made without the system and re-played with the system after a washout period on simulated tumor boards (TBs) in three pilot sites. When TB clinicians changed their decision after using the GL-DSS, it was for a better decision than the decision made without the system in 75 % of the cases. © 2020 The Authors},
	publication_stage = {Final}
}

@ARTICLE{Esteban-Gil2017,
	author = {Esteban-Gil, Angel and Fernández-Breis, Jesualdo Tomás and Boeker, Martin},
	title = {Analysis and visualization of disease courses in a semantically-enabled cancer registry},
	year = {2017},
	journal = {Journal of Biomedical Semantics},
	volume = {8},
	number = {1},
	doi = {10.1186/s13326-017-0154-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030215358&doi=10.1186%2fs13326-017-0154-9&partnerID=40&md5=ecee835bf9842c514a5ac0129d54a33a},
	abstract = {Background: Regional and epidemiological cancer registries are important for cancer research and the quality management of cancer treatment. Many technological solutions are available to collect and analyse data for cancer registries nowadays. However, the lack of a well-defined common semantic model is a problem when user-defined analyses and data linking to external resources are required. The objectives of this study are: (1) design of a semantic model for local cancer registries; (2) development of a semantically-enabled cancer registry based on this model; and (3) semantic exploitation of the cancer registry for analysing and visualising disease courses. Results: Our proposal is based on our previous results and experience working with semantic technologies. Data stored in a cancer registry database were transformed into RDF employing a process driven by OWL ontologies. The semantic representation of the data was then processed to extract semantic patient profiles, which were exploited by means of SPARQL queries to identify groups of similar patients and to analyse the disease timelines of patients. Based on the requirements analysis, we have produced a draft of an ontology that models the semantics of a local cancer registry in a pragmatic extensible way. We have implemented a Semantic Web platform that allows transforming and storing data from cancer registries in RDF. This platform also permits users to formulate incremental user-defined queries through a graphical user interface. The query results can be displayed in several customisable ways. The complex disease timelines of individual patients can be clearly represented. Different events, e.g. different therapies and disease courses, are presented according to their temporal and causal relations. Conclusion: The presented platform is an example of the parallel development of ontologies and applications that take advantage of semantic web technologies in the medical field. The semantic structure of the representation renders it easy to analyse key figures of the patients and their evolution at different granularity levels. © 2017 The Author(s).},
	publication_stage = {Final}
}

@CONFERENCE{Reyes2019120,
	author = {Reyes, Oscar and Luque, Raul M. and Castano, Justo and Ventura, Sebastian},
	title = {A supervised methodology for analyzing dysregulation in splicing machinery: An application in cancer diagnosis},
	year = {2019},
	journal = {Proceedings - IEEE Symposium on Computer-Based Medical Systems},
	volume = {2019-June},
	pages = {120 – 125},
	doi = {10.1109/CBMS.2019.00035},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070981525&doi=10.1109%2fCBMS.2019.00035&partnerID=40&md5=9ab69988d7f1bd5a5aeaaa62facd6c1b},
	abstract = {Deregulated splicing factors have shown to be associated with the development of several types of cancer and, therefore, the determination of such alterations can help the development of tumor-specific molecular targets for early prognosis and therapy. Determining the relevant splicing factors, however, is not a straightforward task mainly due to the heterogeneity of tumors and the variability across samples. In this work, a methodology based on supervised machine learning methods is proposed, allowing the determination of subsets of relevant factors that best discriminate samples. The methodology comprises three main phases: first, a ranking of splicing factors is determined by means of applying feature weighting algorithms; second, the best subset of factors that allows the induction of an accurate classifier is detected; then the confidence over the induced classifier is assessed by means of explaining the individual predictions. Finally, the utility and benefit of the proposed methodology are illustrated by means of analyzing a small dataset of neuroendocrine lung carcinoids, and the results showed that there exist small subsets of deregulated factors which can effectively distinguish between tumor samples and their respective adjacent non-tumor tissues. © 2019 IEEE.},
	publication_stage = {Final}
}

@ARTICLE{Gonzalez-Beltran2020,
	author = {Gonzalez-Beltran, Alejandra N and Masuzzo, Paola and Ampe, Christophe and Bakker, Gert-Jan and Besson, Sébastien and Eibl, Robert H and Friedl, Peter and Gunzer, Matthias and Kittisopikul, Mark and Dévédec, Sylvia E. Le and Leo, Simone and Moore, Josh and Paran, Yael and Prilusky, Jaime and Rocca-Serra, Philippe and Roudot, Philippe and Schuster, Marc and Sergeant, Gwendolien and Strömblad, Staffan and Swedlow, Jason R and Van Erp, Merijn and Van Troys, Marleen and Zaritsky, Assaf and Sansone, Susanna-Assunta and Martens, Lennart},
	title = {Community standards for open cell migration data},
	year = {2020},
	journal = {GigaScience},
	volume = {9},
	number = {5},
	doi = {10.1093/gigascience/giaa041},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084584598&doi=10.1093%2fgigascience%2fgiaa041&partnerID=40&md5=16542def1c81e998f8145fee532a48fc},
	abstract = {Cell migration research has become a high-content field. However, the quantitative information encapsulated in these complex and high-dimensional datasets is not fully exploited owing to the diversity of experimental protocols and non-standardized output formats. In addition, typically the datasets are not open for reuse. Making the data open and Findable, Accessible, Interoperable, and Reusable (FAIR) will enable meta-analysis, data integration, and data mining. Standardized data formats and controlled vocabularies are essential for building a suitable infrastructure for that purpose but are not available in the cell migration domain. We here present standardization efforts by the Cell Migration Standardisation Organisation (CMSO), an open community-driven organization to facilitate the development of standards for cell migration data. This work will foster the development of improved algorithms and tools and enable secondary analysis of public datasets, ultimately unlocking new knowledge of the complex biological process of cell migration. © 2020 The Author(s) 2020. Published by Oxford University Press.},
	publication_stage = {Final}
}

@ARTICLE{Lee2021136,
	author = {Lee, Robert Y. and Brumback, Lyndia C. and Lober, William B. and Sibley, James and Nielsen, Elizabeth L. and Treece, Patsy D. and Kross, Erin K. and Loggers, Elizabeth T. and Fausto, James A. and Lindvall, Charlotta and Engelberg, Ruth A. and Curtis, J. Randall},
	title = {Identifying Goals of Care Conversations in the Electronic Health Record Using Natural Language Processing and Machine Learning},
	year = {2021},
	journal = {Journal of Pain and Symptom Management},
	volume = {61},
	number = {1},
	pages = {136 – 142.e2},
	doi = {10.1016/j.jpainsymman.2020.08.024},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090943250&doi=10.1016%2fj.jpainsymman.2020.08.024&partnerID=40&md5=2f65a433ae6af20d802d2f1862b8df40},
	abstract = {Context: Goals-of-care discussions are an important quality metric in palliative care. However, goals-of-care discussions are often documented as free text in diverse locations. It is difficult to identify these discussions in the electronic health record (EHR) efficiently. Objectives: To develop, train, and test an automated approach to identifying goals-of-care discussions in the EHR, using natural language processing (NLP) and machine learning (ML). Methods: From the electronic health records of an academic health system, we collected a purposive sample of 3183 EHR notes (1435 inpatient notes and 1748 outpatient notes) from 1426 patients with serious illness over 2008–2016, and manually reviewed each note for documentation of goals-of-care discussions. Separately, we developed a program to identify notes containing documentation of goals-of-care discussions using NLP and supervised ML. We estimated the performance characteristics of the NLP/ML program across 100 pairs of randomly partitioned training and test sets. We repeated these methods for inpatient-only and outpatient-only subsets. Results: Of 3183 notes, 689 contained documentation of goals-of-care discussions. The mean sensitivity of the NLP/ML program was 82.3% (SD 3.2%), and the mean specificity was 97.4% (SD 0.7%). NLP/ML results had a median positive likelihood ratio of 32.2 (IQR 27.5–39.2) and a median negative likelihood ratio of 0.18 (IQR 0.16–0.20). Performance was better in inpatient-only samples than outpatient-only samples. Conclusion: Using NLP and ML techniques, we developed a novel approach to identifying goals-of-care discussions in the EHR. NLP and ML represent a potential approach toward measuring goals-of-care discussions as a research outcome and quality metric. © 2020 American Academy of Hospice and Palliative Medicine},
	publication_stage = {Final}
}

@ARTICLE{Zheng2017113,
	author = {Zheng, Ling and Min, Hua and Chen, Yan and Xu, Julia and Geller, James and Perl, Yehoshua},
	title = {Auditing National Cancer Institute thesaurus neoplasm concepts in groups of high error concentration},
	year = {2017},
	journal = {Applied Ontology},
	volume = {12},
	number = {2},
	pages = {113 – 130},
	doi = {10.3233/AO-170179},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022187752&doi=10.3233%2fAO-170179&partnerID=40&md5=a9e49ee73b71600a41fad6ab70dde47f},
	abstract = {The National Cancer Institute thesaurus is an important knowledge resource that should ideally be error-free. We investigated the occurrence of errors in the Neoplasm subhierarchy, which is a part of the National Cancer Institute thesaurus Disease, Disorder or Finding hierarchy. There are five key findings in this study. (1) Errors in the Neoplasm subhierarchy are not uniformly distributed. (2) A partial-area taxonomy, which is a compact network for summarizing the structure and content of an ontology, helped uncover groups of concepts, called "small partial-areas," in the Neoplasm subhierarchy. (3) The rate of errors in "small partial-areas" is twice as large as in "large partial-areas" (44% versus 22%), satisfying statistical significance. Thus, we conclude that higher error concentrations exist in small partial-areas. (4) Group-based auditing can be used successfully to identify additional suspicious concepts in a small group, once a few members of the group are already known as erroneous. (5) Error correction propagation can be used successfully and with minimal effort to correct additional errors in the Neoplasm subhierarchy that occur outside of an initial small group of erroneous concepts. We present examples of errors and examples of how corrections transform and simplify the partial-area taxonomy. © 2017 - IOS Press and the authors. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Martina20203085,
	author = {Martina, Stefano and Ventura, Leonardo and Frasconi, Paolo},
	title = {Classification of cancer pathology reports: A large-scale comparative study},
	year = {2020},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	volume = {24},
	number = {11},
	pages = {3085 – 3094},
	doi = {10.1109/JBHI.2020.3005016},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095800129&doi=10.1109%2fJBHI.2020.3005016&partnerID=40&md5=fdc0be32dfba40e3e2a7c3266eb3eeb6},
	abstract = {We report about the application of state-of-the-art deep learning techniques to the automatic and interpretable assignment of ICD-O3 topography and morphology codes to free-text cancer reports. We present results on a large dataset (more than 80 000 labeled and 1 500 000 unlabeled anonymized reports written in Italian and collected from hospitals in Tuscany over more than a decade) and with a large number of classes (134 morphological classes and 61 topographical classes). We compare alternative architectures in terms of prediction accuracy and interpretability and show that our best model achieves a multiclass accuracy of 90.3% on topography site assignment and 84.8% on morphology type assignment. We found that in this context hierarchical models are not better than flat models and that an element-wise maximum aggregator is slightly better than attentive models on site classification. Moreover, the maximum aggregator offers a way to interpret the classification process. © 2013 IEEE.},
	publication_stage = {Final}
}

@BOOK{Thrusfield20171,
	author = {Thrusfield, Michael and Christley, Robert and Brown, Helen and Diggle, Peter J. and French, Nigel and Howe, Keith and Kelly, Louise and O'Connor, Annette and Sargeant, Jan and Wood, Hannah},
	title = {Veterinary Epidemiology: Fourth Edition},
	year = {2017},
	journal = {Veterinary Epidemiology: Fourth Edition},
	pages = {1 – 861},
	doi = {10.1002/9781118280249},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052921331&doi=10.1002%2f9781118280249&partnerID=40&md5=503f6bae4b073c1e7fddcb88d6c0f6d8},
	abstract = {This fully revised and expanded edition of Veterinary Epidemiology introduces readers to the field of veterinary epidemiology. The new edition also adds new chapters on the design of observational studies, validity in epidemiological studies, systematic reviews, and statistical modelling, to deliver more advanced material. This updated edition begins by offering an historical perspective on the development of veterinary medicine. It then addresses the full scope of epidemiology, with chapters covering causality, disease occurrence, determinants, disease patterns, disease ecology, and much more. Veterinary Epidemiology, Fourth Edition: Features updates of all chapters to provide a current resource on the subject of veterinary epidemiology Presents new chapters essential to the continued advancement of the field Includes examples from companion animal, livestock, and avian medicine, as well as aquatic animal diseases Focuses on the principles and concepts of epidemiology, surveillance, and diagnostic-test validation and performance Includes access to a companion website providing multiple choice questions Veterinary Epidemiology is an invaluable reference for veterinary general practitioners, government veterinarians, agricultural economists, and members of other disciplines interested in animal disease. It is also essential reading for epidemiology students at both the undergraduate and postgraduate levels. © 2018 by John Wiley & Sons Ltd. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Kilicoglu20171400,
	author = {Kilicoglu, Halil},
	title = {Biomedical text mining for research rigor and integrity: Tasks, challenges, directions},
	year = {2017},
	journal = {Briefings in Bioinformatics},
	volume = {19},
	number = {6},
	pages = {1400 – 1414},
	doi = {10.1093/bib/bbx057},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057237672&doi=10.1093%2fbib%2fbbx057&partnerID=40&md5=53599efe2c36ca8677d272ec4102115b},
	abstract = {An estimated quarter of a trillion US dollars is invested in the biomedical research enterprise annually. There is growing alarm that a significant portion of this investment is wasted because of problems in reproducibility of research findings and in the rigor and integrity of research conduct and reporting. Recent years have seen a flurry of activities focusing on standardization and guideline development to enhance the reproducibility and rigor of biomedical research. Research activity is primarily communicated via textual artifacts, ranging from grant applications to journal publications. These artifacts can be both the source and the manifestation of practices leading to research waste. For example, an article may describe a poorly designed experiment, or the authors may reach conclusions not supported by the evidence presented. In this article, we pose the question of whether biomedical text mining techniques can assist the stakeholders in the biomedical research enterprise in doing their part toward enhancing research integrity and rigor. In particular, we identify four key areas in which text mining techniques can make a significant contribution: plagiarism/fraud detection, ensuring adherence to reporting guidelines, managing information overload and accurate citation/enhanced bibliometrics. We review the existing methods and tools for specific tasks, if they exist, or discuss relevant research that can provide guidance for future work. With the exponential increase in biomedical research output and the ability of text mining approaches to perform automatic tasks at large scale, we propose that such approaches can support tools that promote responsible research practices, providing significant benefits for the biomedical research enterprise. © The Author 2017. Published by Oxford University Press. All rights reserved.},
	publication_stage = {Final}
}

@ARTICLE{Zhou2021191,
	author = {Zhou, Fei and Ren, Jie and Lu, Xi and Ma, Shuangge and Wu, Cen},
	title = {Gene–Environment Interaction: A Variable Selection Perspective},
	year = {2021},
	journal = {Methods in Molecular Biology},
	volume = {2212},
	pages = {191 – 223},
	doi = {10.1007/978-1-0716-0947-7_13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102854735&doi=10.1007%2f978-1-0716-0947-7_13&partnerID=40&md5=cd950ab1d910bdfaa25fe23541b57553},
	abstract = {Gene–environment interactions have important implications for elucidating the genetic basis of complex diseases beyond the joint function of multiple genetic factors and their interactions (or epistasis). In the past, G × E interactions have been mainly conducted within the framework of genetic association studies. The high dimensionality of G × E interactions, due to the complicated form of environmental effects and the presence of a large number of genetic factors including gene expressions and SNPs, has motivated the recent development of penalized variable selection methods for dissecting G × E interactions, which has been ignored in the majority of published reviews on genetic interaction studies. In this article, we first survey existing studies on both gene–environment and gene–gene interactions. Then, after a brief introduction to the variable selection methods, we review penalization and relevant variable selection methods in marginal and joint paradigms, respectively, under a variety of conceptual models. Discussions on strengths and limitations, as well as computational aspects of the variable selection methods tailored for G × E studies, have also been provided. © 2021, Springer Science+Business Media, LLC, part of Springer Nature.},
	publication_stage = {Final}
}@CONFERENCE{Fetzer2016,
	author = {Fetzer, Andreas and Metzger, Jasmin and Katic, Darko and März, Keno and Wagner, Martin and Philipp, Patrick and Engelhardt, Sandy and Weller, Tobias and Zelzer, Sascha and Franz, Alfred M. and Schoch, Nicolai and Heuveline, Vincent and Maleshkova, Maria and Rettinger, Achim and Speidel, Stefanie and Wolf, Ivo and Kenngott, Hannes and Mehrabi, Arianeb and Müller-Stich, Beat P. and Maier-Hein, Lena and Meinzer, Hans-Peter and Nolden, Marco},
	title = {Towards an open-source semantic data infrastructure for integrating clinical and scientific data in cognition-guided surgery},
	year = {2016},
	journal = {Progress in Biomedical Optics and Imaging - Proceedings of SPIE},
	volume = {9789},
	doi = {10.1117/12.2217163},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976274877&doi=10.1117%2f12.2217163&partnerID=40&md5=476c916124c0d81e62a0cb49cc2d3972},
	abstract = {In the surgical domain, individual clinical experience, which is derived in large part from past clinical cases, plays an important role in the treatment decision process. Simultaneously the surgeon has to keep track of a large amount of clinical data, emerging from a number of heterogeneous systems during all phases of surgical treatment. This is complemented with the constantly growing knowledge derived from clinical studies and literature. To recall this vast amount of information at the right moment poses a growing challenge that should be supported by adequate technology. While many tools and projects aim at sharing or integrating data from various sources or even provide knowledge-based decision support - to our knowledge - no concept has been proposed that addresses the entire surgical pathway by accessing the entire information in order to provide context-aware cognitive assistance. Therefore a semantic representation and central storage of data and knowledge is a fundamental requirement. We present a semantic data infrastructure for integrating heterogeneous surgical data sources based on a common knowledge representation. A combination of the Extensible Neuroimaging Archive Toolkit (XNAT) with semantic web technologies, standardized interfaces and a common application platform enables applications to access and semantically annotate data, perform semantic reasoning and eventually create individual context-aware surgical assistance. The infrastructure meets the requirements of a cognitive surgical assistant system and has been successfully applied in various use cases. The system is based completely on free technologies and is available to the community as an open-source package. © 2016 SPIE.},
	publication_stage = {Final}
}

@ARTICLE{Sanaeifar2016105,
	author = {Sanaeifar, Ali and Faraahi, Ahmad and Tara, Mahmood},
	title = {SEPHYRES 1: A symptom checker based on semantic pain descriptors and weight spreading},
	year = {2016},
	journal = {Applied Medical Informatics},
	volume = {38},
	number = {3-4},
	pages = {105 – 116},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043559123&partnerID=40&md5=0cb4b57e9a1756737a966ccc01865c19},
	abstract = {Semantic-enabled medical diagnostic systems, which have exploited an ontology in their internal engines, have failed to perfectly describe disease profiles, especially in complex medical terms having a variant generality level or certainty in the medical literature. The main objective of this paper was to present an ontology with a highly matching grade of proeminent medical concepts able to analyze the patient’s descriptive medical condition. Focusing on semantic pain descriptors and weight spreading techniques, we proposed a semantic-pseudo-fuzzy engine entitled SEPHYRES, with which we tried to present an ontology-based solution using not only a generic semantic reasoner but also complementary domain-heuristic reasoning. Having applied the valid evidence-based references along with local experts, we illustrated how the resilient expressive model represents the complex medical term relations. The twenty test cases were extracted from the MEDSCAPE and PubMed databases and the precision and recall were calculated. Finally, the results were compared against the Isabel symptom checker and performed the Wilcoxon signed-rank test. The recall measures indicated that the accuracy was equal to 75%, if the system was adjusted to only ten results as differential diagnoses. Moreover, the Wilcoxon signed-rank test showed that there was significant difference between SEPHYRES and Isabel symptom checker (P= 0.016) so that this method is sufficiently able to improve semantic expressiveness in both professional medical diagnosis and patient decision aid systems. © 2010, Romanian Society for Applied Medical Informatics. All rights reserved.},
	publication_stage = {Final}
}

@BOOK{Berman20161,
	author = {Berman, Jules J.},
	title = {Data Simplification: Taming Information With Open Source Tools},
	year = {2016},
	journal = {Data Simplification: Taming Information With Open Source Tools},
	pages = {1 – 366},
	doi = {10.1016/C2015-0-00783-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967372673&doi=10.1016%2fC2015-0-00783-3&partnerID=40&md5=fb3396bb8cdd6528b41d5d4a834c3f3f},
	abstract = {Data Simplification: Taming Information With Open Source Tools addresses the simple fact that modern data is too big and complex to analyze in its native form. Data simplification is the process whereby large and complex data is rendered usable. Complex data must be simplified before it can be analyzed, but the process of data simplification is anything but simple, requiring a specialized set of skills and tools. This book provides data scientists from every scientific discipline with the methods and tools to simplify their data for immediate analysis or long-term storage in a form that can be readily repurposed or integrated with other data. Drawing upon years of practical experience, and using numerous examples and use cases, Jules Berman discusses the principles, methods, and tools that must be studied and mastered to achieve data simplification, open source tools, free utilities and snippets of code that can be reused and repurposed to simplify data, natural language processing and machine translation as a tool to simplify data, and data summarization and visualization and the role they play in making data useful for the end user. Discusses data simplification principles, methods, and tools that must be studied and mastered Provides open source tools, free utilities, and snippets of code that can be reused and repurposed to simplify data Explains how to best utilize indexes to search, retrieve, and analyze textual data Shows the data scientist how to apply ontologies, classifications, classes, properties, and instances to data using tried and true methods. © 2016 Elsevier Inc. All rights reserved.},
	publication_stage = {Final}
}

@BOOK{Torner20161,
	author = {Torner, Sergi and Bernal, Elisenda},
	title = {Collocations and Other Lexical Combinations in Spanish: Theoretical, Lexicographical and Applied Perspectives},
	year = {2016},
	journal = {Collocations and other lexical combinations in Spanish: Theoretical, lexicographical and applied perspectives},
	pages = {1 – 383},
	doi = {10.4324/9781315455259},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138018920&doi=10.4324%2f9781315455259&partnerID=40&md5=db2348f5c0e23fac5880854a2c2aef9d},
	abstract = {This edited collection presents the state of the art in research related to lexical combinations and their restrictions in Spanish from a variety of theoretical approaches, ranging from Explanatory Combinatorial Lexicology to Distributed Morphology and Generative Lexicon Theory. • Section 1 offers a presentation of the main theoretical and descriptive approaches to collocation. • Section 2 explores collocation from the point of view of its lexicographical representation, while Section 3 offers a pedagogical perspective. • Section 4 surveys current research on collocation in Catalan, Galician and Basque. Collocations and other lexical combinations in Spanish will be of interest to students of Hispanic linguistics. © 2017 Sergi Torner and Elisenda Bernal selection and editorial matter; individual chapters, the contributors.},
	publication_stage = {Final}
}

@ARTICLE{Jeanquartier201649,
	author = {Jeanquartier, Fleur and Jean-Quartier, Claire and Schreck, Tobias and Cemernek, David and Holzinger, Andreas},
	title = {Integrating open data on cancer in support to tumor growth analysis},
	year = {2016},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume = {9832 LNCS},
	pages = {49 – 66},
	doi = {10.1007/978-3-319-43949-5_4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981513401&doi=10.1007%2f978-3-319-43949-5_4&partnerID=40&md5=69f822487e96042bb25df66b2f446c86},
	abstract = {The general disease group of malignant neoplasms depicts one of the leading and increasing causes for death. The underlying complexity of cancer demands for abstractions to disclose an exclusive subset of information related to the disease. Our idea is to create a user interface for linking a simulation on cancer modeling to relevant additional publicly and freely available data. We are not only providing a categorized list of open datasets and queryable databases for the different types of cancer and related information, we also identify a certain subset of temporal and spatial data related to tumor growth. Furthermore, we describe the integration possibilities into a simulation tool on tumor growth that incorporates the tumor’s kinetics. © Springer International Publishing Switzerland 2016.},
	publication_stage = {Final}
}

@ARTICLE{Bandrowski2016,
	author = {Bandrowski, Anita and Brinkman, Ryan and Brochhausen, Mathias and Brush, Matthew H. and Bug, Bill and Chibucos, Marcus C. and Clancy, Kevin and Courtot, Mélanie and Derom, Dirk and Dumontier, Michel and Fan, Liju and Fostel, Jennifer and Fragoso, Gilberto and Gibson, Frank and Gonzalez-Beltran, Alejandra and Haendel, Melissa A. and He, Yongqun and Heiskanen, Mervi and Hernandez-Boussard, Tina and Jensen, Mark and Lin, Yu and Lister, Allyson L. and Lord, Phillip and Malone, James and Manduchi, Elisabetta and McGee, Monnie and Morrison, Norman and Overton, James A. and Parkinson, Helen and Peters, Bjoern and Rocca-Serra, Philippe and Ruttenberg, Alan and Sansone, Susanna-Assunta and Scheuermann, Richard H. and Schober, Daniel and Smith, Barry and Soldatova, Larisa N. and Stoeckert, Christian J. and Taylor, Chris F. and Torniai, Carlo and Turner, Jessica A. and Vita, Randi and Whetzel, Patricia L. and Zheng, Jie},
	title = {The Ontology for Biomedical Investigations},
	year = {2016},
	journal = {PLoS ONE},
	volume = {11},
	number = {4},
	doi = {10.1371/journal.pone.0154556},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967317201&doi=10.1371%2fjournal.pone.0154556&partnerID=40&md5=2183a0d262a219c24743654ca687fe3e},
	abstract = {The Ontology for Biomedical Investigations (OBI) is an ontology that provides terms with precisely defined meanings to describe all aspects of how investigations in the biological and medical domains are conducted. OBI re-uses ontologies that provide a representation of biomedical knowledge from the Open Biological and Biomedical Ontologies (OBO) project and adds the ability to describe how this knowledge was derived. We here describe the state of OBI and several applications that are using it, such as adding semantic expressivity to existing databases, building data entry forms, and enabling interoperability between knowledge resources. OBI covers all phases of the investigation process, such as planning, execution and reporting. It represents information and material entities that participate in these processes, as well as roles and functions. Prior to OBI, it was not possible to use a single internally consistent resource that could be applied to multiple types of experiments for these applications. OBI has made this possible by creating terms for entities involved in biological and medical investigations and by importing parts of other biomedical ontologies such as GO, Chemical Entities of Biological Interest (ChEBI) and Phenotype Attribute and Trait Ontology (PATO) without altering their meaning. OBI is being used in a wide range of projects covering genomics, multi-omics, immunology, and catalogs of services. OBI has also spawned other ontologies (Information Artifact Ontology) and methods for importing parts of ontologies (Minimum information to reference an external ontology term (MIREOT)). The OBI project is an open cross-disciplinary collaborative effort, encompassing multiple research communities from around the globe. To date, OBI has created 2366 classes and 40 relations along with textual and formal definitions. The OBI Consortium maintains a web resource (http://obi-ontology.org) providing details on the people, policies, and issues being addressed in association with OBI. The current release of OBI is available at http://purl. obolibrary.org/obo/obi.owl. © 2016, Public Library of Science. All rights reserved. This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the Creative Commons CC0 public domain dedication.},
	publication_stage = {Final}
}